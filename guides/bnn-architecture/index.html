<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=description content="An Open-Source Library for Training Binarized Neural Networks"><link href=https://larq.dev/guides/bnn-architecture/ rel=canonical><meta name=author content=Plumerai><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.0.4, mkdocs-material-4.4.0"><title>Building BNNs - Larq</title><link rel=stylesheet href=../../assets/stylesheets/application.0284f74d.css><link rel=stylesheet href=../../assets/stylesheets/application-palette.01803549.css><meta name=theme-color content=#2196f3><script src=../../assets/javascripts/modernizr.74668098.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../../assets/fonts/material-icons.css><link rel=stylesheet href=../../custom.css></head> <body dir=ltr data-md-color-primary=blue data-md-color-accent=blue> <svg class=md-svg> <defs> <svg xmlns=http://www.w3.org/2000/svg width=416 height=448 viewbox="0 0 416 448" id=__github><path fill=currentColor d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#binarizing-a-single-layer tabindex=1 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://larq.dev/ title=Larq class="md-header-nav__button md-logo"> <i class=md-icon>developer_board</i> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class=md-flex__ellipsis data-md-component=title> <span class=md-header-nav__topic> <nav class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. title=Learn class="md-tabs__link md-tabs__link--active"> Learn </a> </li> <li class=md-tabs__item> <a href=../../api/layers/ title=API class=md-tabs__link> API </a> </li> <li class=md-tabs__item> <a href=../../models/ title=Models class=md-tabs__link> Models </a> </li> <li class=md-tabs__item> <a href=../../papers/ title=Community class=md-tabs__link> Community </a> </li> <li class=md-tabs__item> <a href=../../about/ title="Why Larq?" class=md-tabs__link> Why Larq? </a> </li> </ul> </nav> </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <div class=md-header-nav__source> <a href=https://github.com/larq/larq title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> larq/larq </div> </a> </div> </div> </div> </nav> </header> <div class=md-container> <!-- Noop component to simplify sidebar appearence --> <nav class="md-tabs md-tabs--active"></nav> <main class=md-main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=https://larq.dev/ title=Larq class="md-nav__button md-logo"> <i class=md-icon>developer_board</i> </a> Larq </label> <div class=md-nav__source> <a href=https://github.com/larq/larq title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> larq/larq </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-1 type=checkbox id=nav-1 checked> <label class=md-nav__link for=nav-1> Learn </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-1> Learn </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. title=Introduction class=md-nav__link> Introduction </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-1-2 type=checkbox id=nav-1-2 checked> <label class=md-nav__link for=nav-1-2> User Guides </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-1-2> User Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../key-concepts/ title="Key Concepts" class=md-nav__link> Key Concepts </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-toggle md-nav__toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Building BNNs </label> <a href=./ title="Building BNNs" class="md-nav__link md-nav__link--active"> Building BNNs </a> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#binarizing-a-single-layer title="Binarizing a Single Layer" class=md-nav__link> Binarizing a Single Layer </a> </li> <li class=md-nav__item> <a href=#first-last-layer title="First & Last Layer" class=md-nav__link> First &amp; Last Layer </a> </li> <li class=md-nav__item> <a href=#batch-normalization title="Batch Normalization" class=md-nav__link> Batch Normalization </a> </li> <li class=md-nav__item> <a href=#high-precision-shortcuts title="High-Precision Shortcuts" class=md-nav__link> High-Precision Shortcuts </a> </li> <li class=md-nav__item> <a href=#pooling title=Pooling class=md-nav__link> Pooling </a> </li> <li class=md-nav__item> <a href=#further-references title="Further References" class=md-nav__link> Further References </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../bnn-optimization/ title="Training BNNs" class=md-nav__link> Training BNNs </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-1-3 type=checkbox id=nav-1-3> <label class=md-nav__link for=nav-1-3> Examples </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-1-3> Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../examples/mnist/ title="Introduction to BNNs with Larq" class=md-nav__link> Introduction to BNNs with Larq </a> </li> <li class=md-nav__item> <a href=../../examples/binarynet_cifar10/ title="BinaryNet on CIFAR10" class=md-nav__link> BinaryNet on CIFAR10 </a> </li> <li class=md-nav__item> <a href=../../examples/binarynet_advanced_cifar10/ title="BinaryNet on CIFAR10 (Advanced)" class=md-nav__link> BinaryNet on CIFAR10 (Advanced) </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> API </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-2> API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../api/layers/ title=Layers class=md-nav__link> Layers </a> </li> <li class=md-nav__item> <a href=../../api/quantizers/ title=Quantizers class=md-nav__link> Quantizers </a> </li> <li class=md-nav__item> <a href=../../api/activations/ title=Activations class=md-nav__link> Activations </a> </li> <li class=md-nav__item> <a href=../../api/constraints/ title=Constraints class=md-nav__link> Constraints </a> </li> <li class=md-nav__item> <a href=../../api/callbacks/ title=Callbacks class=md-nav__link> Callbacks </a> </li> <li class=md-nav__item> <a href=../../api/optimizers/ title=Optimizers class=md-nav__link> Optimizers </a> </li> <li class=md-nav__item> <a href=../../api/math/ title=Math class=md-nav__link> Math </a> </li> <li class=md-nav__item> <a href=../../api/models/ title=Models class=md-nav__link> Models </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Models </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-3> Models </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../models/ title="Larq Zoo" class=md-nav__link> Larq Zoo </a> </li> <li class=md-nav__item> <a href=../../models/examples/ title=Examples class=md-nav__link> Examples </a> </li> <li class=md-nav__item> <a href=../../models/api/ title=API class=md-nav__link> API </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Community </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-4> Community </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../papers/ title="Papers using Larq" class=md-nav__link> Papers using Larq </a> </li> <li class=md-nav__item> <a href=../../contributing/ title="Contributing Guide" class=md-nav__link> Contributing Guide </a> </li> <li class=md-nav__item> <a href=../../code-of-conduct/ title="Code of Conduct" class=md-nav__link> Code of Conduct </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Why Larq? </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-5> Why Larq? </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../about/ title=About class=md-nav__link> About </a> </li> <li class=md-nav__item> <a href=../../faq/ title=FAQ class=md-nav__link> FAQ </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#binarizing-a-single-layer title="Binarizing a Single Layer" class=md-nav__link> Binarizing a Single Layer </a> </li> <li class=md-nav__item> <a href=#first-last-layer title="First & Last Layer" class=md-nav__link> First &amp; Last Layer </a> </li> <li class=md-nav__item> <a href=#batch-normalization title="Batch Normalization" class=md-nav__link> Batch Normalization </a> </li> <li class=md-nav__item> <a href=#high-precision-shortcuts title="High-Precision Shortcuts" class=md-nav__link> High-Precision Shortcuts </a> </li> <li class=md-nav__item> <a href=#pooling title=Pooling class=md-nav__link> Pooling </a> </li> <li class=md-nav__item> <a href=#further-references title="Further References" class=md-nav__link> Further References </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1>Building BNNs</h1> <p>Here you will find a quick overview of the best practices that have evolved in the BNN community over the past few years regarding BNN architecture. After following this guide, you should be able to start designing a BNN for your application of interest that is both efficient and powerful.</p> <h2 id=binarizing-a-single-layer>Binarizing a Single Layer<a class=headerlink href=#binarizing-a-single-layer title="Permanent link">&para;</a></h2> <p>Any layer has two types of inputs: the layer parameters, such as a weight matrix and biases, and incoming activations.</p> <p>We can reduce the memory footprint of the model by binarizing parameters. In Larq, this can be done by passing a <code>kernel_quantizer</code> from <a href=/api/quantizers><code>larq.quantizers</code></a> when instantiating a <a href=/api/layers><code>larq.layer</code></a> object, or by using a custom BNN optimizer such as <a href=/api/optimizers/#bop>Bop</a> (note that by default Bop only targets kernels of layers in <a href=/api/layers><code>larq.layers</code></a>).</p> <p>To get the efficiency of binary computations, the incoming activations need to be binary as well. This can be done by setting a <code>input_quantizer</code>.</p> <p>Note that the output of a binarized layer is <em>not</em> binary. Instead the output is integer, due to the summation that appears in most neural network layers.</p> <p>When viewing binarization as an activation function just like ReLU, one may be inclined to binarize the outgoing activations rather than the incoming activations. However, if the network contains batch normalization layers or residual connections, this may result in unintentional non-binary operations. Therefore we have opted for an <code>input_quantizer</code> rather than an <code>activation_quantizer</code>.</p> <p>A typical binarized layer looks something like:</p> <div class=codehilite><pre><span></span><span class=kn>import</span> <span class=nn>larq</span> <span class=kn>as</span> <span class=nn>lq</span>
<span class=o>...</span>
<span class=n>x_out</span> <span class=o>=</span> <span class=n>lq</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>QuantDense</span><span class=p>(</span>
    <span class=n>units</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span>
    <span class=n>input_quantizer</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>quantizers</span><span class=o>.</span><span class=n>ste_sign</span><span class=p>,</span>
    <span class=n>kernel_quantizer</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>quantizers</span><span class=o>.</span><span class=n>ste_sign</span><span class=p>,</span>
    <span class=n>kernel_constraint</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>constraints</span><span class=o>.</span><span class=n>weight_clip</span><span class=p>,</span>
    <span class=p>)(</span><span class=n>x_in</span><span class=p>)</span>
</pre></div> <h2 id=first-last-layer>First &amp; Last Layer<a class=headerlink href=#first-last-layer title="Permanent link">&para;</a></h2> <p>Binarizing the first and last layers hurts accuracy much more than binarizing other layers in the network. Meanwhile, the number of weights and operations in these layers are relatively small. Therefore it has become standard to leave these layers in higher precision. This applies to the incoming activations as well as the weights. The following shows a network that was trained on CIFAR10 with different precisions for first and last layers.</p> <p> <div id=altair-plot-b644aa43-3916-460c-94b7-15e3796a2e6c> <script>
          // embed when document is loaded, to ensure vega library is available
          // this works on all modern browsers, except IE8 and older
          document.addEventListener("DOMContentLoaded", function(event) {
              var opt = {
                "mode": "vega-lite",
                "renderer": "canvas",
                "actions": false,
              };
              vegaEmbed('#altair-plot-b644aa43-3916-460c-94b7-15e3796a2e6c', '/plots/first_and_last_layers.vg.json', opt).catch(console.err);
          }, {passive: true, once: true});
        </script> </div> </p> <h2 id=batch-normalization>Batch Normalization<a class=headerlink href=#batch-normalization title="Permanent link">&para;</a></h2> <p>Perhaps somewhat surprisingly, batch normalization remains crucial in BNNs. All successful BNNs still contain a batch norm layer after each binarized layer. We recommend using batch normalization with trainable beta and gamma.</p> <p>Note that when no residuals are used, the batch norm operation can be simplified (see e.g. Fig. 2 in <a href=https://arxiv.org/pdf/1904.02823.pdf>this paper</a>).</p> <h2 id=high-precision-shortcuts>High-Precision Shortcuts<a class=headerlink href=#high-precision-shortcuts title="Permanent link">&para;</a></h2> <p>A binarized layer outputs an integer activation matrix that is binarized before the next layer. This means that in a VGG-style network such as <a href=https://arxiv.org/abs/1602.02830>BinaryNet</a> information is lost between every two layers, and one may wonder if this is optimal in terms of efficiency.</p> <p>High-precision shortcuts avoid this loss of information. Examples of networks that include such shortcuts are <a href=https://arxiv.org/abs/1808.00278>Bi-Real net</a> and <a href=https://arxiv.org/abs/1906.08637>Binary Densenets</a>. Note that the argument for introducing these shortcuts is no longer just to improve gradient flow, as it is in real-valued models: in BNNs, high-precision shortcuts really improve the expressivity of the model.</p> <p>Such shortcuts are relatively cheap in terms of memory footprint and computational cost, and they greatly improve accuracy. Beware that they do increase the runtime memory requirements of the model.</p> <p>An issue with ResNet-style shortcuts comes up when there is a dimensionality change. Currently, the most popular solution to this is to use pointwise high-precision convolutions in the residual connections if there is a dimensionality change.</p> <p>We recommend to use as many shortcuts as you can: for example, in ResNet-style architectures it helps to bypass every single convolutional layer, instead of every two convolutional layers.</p> <p>An example of a convolutional block with shortcut in Larq could look something like this:</p> <div class=codehilite><pre><span></span><span class=kn>import</span> <span class=nn>larq</span> <span class=kn>as</span> <span class=nn>lq</span>
<span class=o>...</span>
<span class=k>def</span> <span class=nf>conv_with_shortcut</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=sd>&quot;&quot;&quot;Convolutional block with shortcut connection.</span>

<span class=sd>    Args:</span>
<span class=sd>      x: input tensor with high-precision activations</span>

<span class=sd>    Returns:</span>
<span class=sd>      Tensor with high-precision activations</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=c1># get number of filters</span>
    <span class=n>filters</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>

    <span class=c1># create shortcut that retains the high-precision information</span>
    <span class=n>shortcut</span> <span class=o>=</span> <span class=n>x</span>

    <span class=c1># efficient binarized convolutions (note inputs are also binarized)</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>lq</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>QuantConv2D</span><span class=p>(</span>
        <span class=n>filters</span><span class=o>=</span><span class=n>filters</span><span class=p>,</span>
        <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
        <span class=n>padding</span><span class=o>=</span><span class=s2>&quot;same&quot;</span><span class=p>,</span>
        <span class=n>input_quantizer</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>quantizers</span><span class=o>.</span><span class=n>ste_sign</span><span class=p>,</span>
        <span class=n>kernel_quantizer</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>quantizers</span><span class=o>.</span><span class=n>ste_sign</span><span class=p>,</span>
        <span class=n>kernel_initializer</span><span class=o>=</span><span class=s2>&quot;glorot_normal&quot;</span><span class=p>,</span>
        <span class=n>kernel_constraint</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>constraints</span><span class=o>.</span><span class=n>weight_clip</span><span class=p>,</span>
        <span class=n>use_bias</span><span class=o>=</span><span class=bp>False</span><span class=p>,</span>
    <span class=p>)(</span><span class=n>x</span><span class=p>)</span>

    <span class=c1># normalize the (integer) output of the binary conv and merge</span>
    <span class=c1># with shortcut</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>(</span><span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>out</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>add</span><span class=p>([</span><span class=n>x</span><span class=p>,</span> <span class=n>shortcut</span><span class=p>])</span>

    <span class=k>return</span> <span class=n>out</span>
</pre></div> <h2 id=pooling>Pooling<a class=headerlink href=#pooling title="Permanent link">&para;</a></h2> <p>The <a href=https://arxiv.org/abs/1603.05279>XNOR-net authors</a> found that accuracy improves when applying batch normalization after instead of before max-pooling. In general, max pooling in BNNs can be problematic as it can lead to skewed binarized activations.</p> <p>Thus, in a VGG-style network a layer could look like this:</p> <div class=codehilite><pre><span></span><span class=kn>import</span> <span class=nn>larq</span> <span class=kn>as</span> <span class=nn>lq</span>
<span class=o>...</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>lq</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>QuantConv2D</span><span class=p>(</span>
    <span class=n>filters</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
    <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>padding</span><span class=o>=</span><span class=s2>&quot;same&quot;</span><span class=p>,</span>
    <span class=n>input_quantizer</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>quantizers</span><span class=o>.</span><span class=n>ste_sign</span><span class=p>,</span>
    <span class=n>kernel_quantizer</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>quantizers</span><span class=o>.</span><span class=n>ste_sign</span><span class=p>,</span>
    <span class=n>kernel_constraint</span><span class=o>=</span><span class=n>lq</span><span class=o>.</span><span class=n>constraints</span><span class=o>.</span><span class=n>weight_clip</span><span class=p>,</span>
    <span class=n>use_bias</span><span class=o>=</span><span class=bp>False</span><span class=p>,</span>
    <span class=p>)(</span><span class=n>x</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>MaxPool2D</span><span class=p>(</span><span class=n>pool_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>strides</span><span class=o>=</span><span class=mi>2</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>(</span><span class=n>momentum</span><span class=o>=.</span><span class=mi>9</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
</pre></div> <h2 id=further-references>Further References<a class=headerlink href=#further-references title="Permanent link">&para;</a></h2> <p>If you would like to learn more, we recommend checking out the following papers (starting at the most recent):</p> <ul> <li><a href=https://arxiv.org/abs/1906.08637>Back to Simplicity: How to Train Accurate BNNs from Scratch?</a> - This recent paper introduces Binary Densenets, demonstrating good results on ImageNet. The authors take an information-theoretic perspective on BNN architectures and give a number of recommendations for good architecture design.</li> <li><a href=https://arxiv.org/abs/1811.10413>Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation</a> - A thought-provoking paper that presents Group-Net. The authors question whether architectural features developed for real-valued networks are the most appropriate for BNN. Even more than the presented architecture, we find this line of thinking very interesting and hope Larq will enable people to explore novel ideas more easily.</li> <li><a href=https://arxiv.org/abs/1808.00278>Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm</a> - This ECCV 2018 paper introduces Bi-Real nets, one of the first binarized networks that uses high-precision shortcuts.</li> <li><a href=https://arxiv.org/abs/1602.02830>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</a> - The classic BNN paper, mandatory reading for anyone working in the field. In addition to introducing many of the foundational ideas for BNNs, the paper contains an interesting discussion on batch normalization.</li> </ul> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid"> <a href=../key-concepts/ title="Key Concepts" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i> </div> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Previous </span> Key Concepts </span> </div> </a> <a href=../bnn-optimization/ title="Training BNNs" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel=next> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Next </span> Training BNNs </span> </div> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> powered by <a href=https://www.mkdocs.org>MkDocs</a> and <a href=https://squidfunk.github.io/mkdocs-material/ > Material for MkDocs</a> </div> <div class=md-footer-social> <link rel=stylesheet href=../../assets/fonts/font-awesome.css> <a href=https://github.com/larq class="md-footer-social__link fa fa-github"></a> <a href=https://twitter.com/PlumeraiHQ class="md-footer-social__link fa fa-twitter"></a> <a href=https://www.linkedin.com/company/plumerai/ class="md-footer-social__link fa fa-linkedin"></a> </div> </div> </div> </footer> </div> <script src=../../assets/javascripts/application.245445c6.js></script> <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML"></script> <script src=https://cdn.jsdelivr.net/npm/vega@5></script> <script src=https://cdn.jsdelivr.net/npm/vega-lite@3></script> <script src=https://cdn.jsdelivr.net/npm/vega-embed@4></script> </body> </html>
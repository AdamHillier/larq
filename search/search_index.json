{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Larq \u00b6 Larq is an open-source deep learning library for training neural networks with extremely low precision weights and activations, such as Binarized Neural Networks (BNNs). Existing deep neural networks use 32 bits, 16 bits or 8 bits to encode each weight and activation, making them large, slow and power-hungry. This prohibits many applications in resource-constrained environments. Larq is the first step towards solving this. It is designed to provide an easy to use, composable way to train BNNs (1 bit) and other types of Quantized Neural Networks (QNNs) and is based on the tf.keras interface. Getting Started \u00b6 To build a QNN, Larq introduces the concept of quantized layers and quantizers . A quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each quantized layer requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. You can define a simple binarized fully-connected Keras model using the Straight-Through Estimator the following way: model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) This layer can be used inside a Keras model or with a custom training loop . Examples \u00b6 Check out our examples on how to train a Binarized Neural Network in just a few lines of code: Introduction to BNNs with Larq BinaryNet on CIFAR10 BinaryNet on CIFAR10 (Advanced) Requirements \u00b6 Before installing Larq, please install: Python version 3.6 or 3.7 Tensorflow version 1.13+ or 2.0.0 You can also check out one of our prebuilt docker images . Installation \u00b6 You can install Larq with Python's pip package manager: pip install larq","title":"Introduction"},{"location":"#larq","text":"Larq is an open-source deep learning library for training neural networks with extremely low precision weights and activations, such as Binarized Neural Networks (BNNs). Existing deep neural networks use 32 bits, 16 bits or 8 bits to encode each weight and activation, making them large, slow and power-hungry. This prohibits many applications in resource-constrained environments. Larq is the first step towards solving this. It is designed to provide an easy to use, composable way to train BNNs (1 bit) and other types of Quantized Neural Networks (QNNs) and is based on the tf.keras interface.","title":"Larq"},{"location":"#getting-started","text":"To build a QNN, Larq introduces the concept of quantized layers and quantizers . A quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each quantized layer requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. You can define a simple binarized fully-connected Keras model using the Straight-Through Estimator the following way: model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) This layer can be used inside a Keras model or with a custom training loop .","title":"Getting Started"},{"location":"#examples","text":"Check out our examples on how to train a Binarized Neural Network in just a few lines of code: Introduction to BNNs with Larq BinaryNet on CIFAR10 BinaryNet on CIFAR10 (Advanced)","title":"Examples"},{"location":"#requirements","text":"Before installing Larq, please install: Python version 3.6 or 3.7 Tensorflow version 1.13+ or 2.0.0 You can also check out one of our prebuilt docker images .","title":"Requirements"},{"location":"#installation","text":"You can install Larq with Python's pip package manager: pip install larq","title":"Installation"},{"location":"about/","text":"Larq is an open-source Binarized Neural Network (BNN) training library on top of TensorFlow, developed and maintained by Plumerai . We believe efficient inference of deep learning models will radically change the world by enabling a multitude of new technologies - from delivery drones and self-driving cars to fruit picking robots and autonomous Mars rovers. BNNs are the key that will unlock this future. These networks have a much smaller memory footprint than ordinary neural networks, and their bitwise nature makes it possible to execute the computations extremely efficienciently. Larq has been created to make it easier for researchers and developers to explore the possibilities of BNNs. We strongly believe an open approach to knowledge is the way forward, which is why Larq lives under a permissive open-source software Apache 2.0 licence .","title":"About"},{"location":"code-of-conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at roeland@plumerai.co.uk or lukas@plumerai.co.uk. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at roeland@plumerai.co.uk or lukas@plumerai.co.uk. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contributing to Larq \u00b6 \ud83d\udc4d \ud83c\udf89 First off, thanks for taking the time to contribute! \ud83d\udc4d \ud83c\udf89 Working on your first Pull Request? You can learn how from this free series How to Contribute to an Open Source Project on GitHub . Project setup \u00b6 To send a Pull Request it is required to fork Larq on GitHub. After that clone it to a desired directory: git clone https://github.com/my-username/larq.git Install all required dependencies for local development by running: cd larq # go into the directory you just cloned pip install -e . [ tensorflow ] # Installs Tensorflow for CPU # pip install -e .[tensorflow_gpu] # Installs Tensorflow for GPU pip install -e . [ test ] # Installs all development dependencies Run Unit tests \u00b6 Inside the project directory run: pytest . Build documentation \u00b6 Installs dependencies for building the docs: pip install nbconvert git+https://github.com/lgeiger/pydoc-markdown.git pip install -e . [ docs ] Inside the project directory run: ./larqdocs.sh serve A new version of the documentation will be automatically published once merged into the master branch. Code style \u00b6 We use black to format all of our code. We recommend installing it as a plugin for your favorite code editor . Publish release \u00b6 Install dependencies python -m pip install --upgrade setuptools wheel twine Increment the version number in setup.py Push new tag git tag <version number> git push && git push --tags Build wheels rm -r build/* dist/* python setup.py sdist bdist_wheel Upload release python -m twine upload dist/*","title":"Contributing Guide"},{"location":"contributing/#contributing-to-larq","text":"\ud83d\udc4d \ud83c\udf89 First off, thanks for taking the time to contribute! \ud83d\udc4d \ud83c\udf89 Working on your first Pull Request? You can learn how from this free series How to Contribute to an Open Source Project on GitHub .","title":"Contributing to Larq"},{"location":"contributing/#project-setup","text":"To send a Pull Request it is required to fork Larq on GitHub. After that clone it to a desired directory: git clone https://github.com/my-username/larq.git Install all required dependencies for local development by running: cd larq # go into the directory you just cloned pip install -e . [ tensorflow ] # Installs Tensorflow for CPU # pip install -e .[tensorflow_gpu] # Installs Tensorflow for GPU pip install -e . [ test ] # Installs all development dependencies","title":"Project setup"},{"location":"contributing/#run-unit-tests","text":"Inside the project directory run: pytest .","title":"Run Unit tests"},{"location":"contributing/#build-documentation","text":"Installs dependencies for building the docs: pip install nbconvert git+https://github.com/lgeiger/pydoc-markdown.git pip install -e . [ docs ] Inside the project directory run: ./larqdocs.sh serve A new version of the documentation will be automatically published once merged into the master branch.","title":"Build documentation"},{"location":"contributing/#code-style","text":"We use black to format all of our code. We recommend installing it as a plugin for your favorite code editor .","title":"Code style"},{"location":"contributing/#publish-release","text":"Install dependencies python -m pip install --upgrade setuptools wheel twine Increment the version number in setup.py Push new tag git tag <version number> git push && git push --tags Build wheels rm -r build/* dist/* python setup.py sdist bdist_wheel Upload release python -m twine upload dist/*","title":"Publish release"},{"location":"faq/","text":"Are there any restrictions on using Larq? \u00b6 We believe open research and software is the way forward for the field of Binarized Neural Network (BNN) and deep learning and general: machine learning can be a powerful engine for good, but only if it is transparent, save and widely accessible. That is why we have made Larq open-source and free to use. Larq is licensed under Apache 2.0 licence - you are free to contribute to Larq , fork it or build commercial applications on top of it. If you distribute a modified version of Larq, you should clearly state changes you have made and consider contributing them back. What is a Binarized Neural Network (BNN)? \u00b6 A BNN is a deep neural network in which the bulk of the computations are performed using binary values. For example, in a convolutional layer we can use weights that are either -1 or 1, and similarly binarize the activations. Binarization enables the creation of deep learning models that are extremely efficient: storing the model only requires a single bit per weight, and evaluating the model can be done very efficiently because of the bitwise nature of the operations. Note that in BNN not everything is binary: usually higher-precision computations are still used for things like the first layer, batch-normalization and residual connections. How can I cite Larq? \u00b6 If Larq helps you in your work or research, it would be great if you can cite it as follows: @ misc { larq , author = { Geiger , Lukas and Widdicombe , James and Bakhtiari , Arash and Helwegen , Koen and Heuss , Maria and Nusselder , Roeland }, title = { Larq : An Open - Source Deep Learning Library for Training Binarized Neural Networks }, howpublished = { Web page }, url = { https : // larq . dev }, year = { 2019 } } If your paper is publicly available, feel free to also add it to the list of Papers using Larq . Can I add my own algorithm or model to Larq? \u00b6 Absolutely! If you have developed a new model or training method that you would like to share with the community, create a PR or get in touch with us. Make sure you check out the contribution guide. For entire models with pretrained weights larq-zoo is the correct place, everything else can be added to larq directly. Can I use Larq only for Binarized Neural Networks (BNNs)? \u00b6 No, Larq is not just for BNNs! The real goal of Larq is to make it easy to work with extremely quantized networks. This includes BNNs as well as ternary networks (see for example Ternary Weight Networks or Trained Ternary Quantization ). Although the focus is currently on BNNs, Larq already supports a ternary quantizer and binary and ternary networks have a lot in common. Moreover, modern BNNs are not 'pure' binary networks: they contain higher-precision first and last layers and shortcut connections, for example. We will expand support for ternary and mixed precision networks in the future by implementing layers and optimization tools that are useful in this context, and providing more examples. If there is any particular feature you would like to use, let us know by posting an issue . Why is Larq built on top of tf.keras ? \u00b6 We put a lot of thought into the question of which framework we should build Larq on and decided to go with TensorFlow / Keras over PyTorch or some other framework. There are a number of reasons for this: We really like the Keras API for its simplicity. At the same time, it is still very flexible if you want to build complex architectures or custom training loops. The TensorFlow ecosystem provides a wide range of tools for both researchers and developers. We think integration into that ecosystem will be beneficial for people working with BNNs. We are big fans of tf.datasets . Reproducibility is a key concern to us, and our approach for larq-zoo is heavily inspired by Keras Applications . Will there be a PyTorch version of Larq? \u00b6 No, currently we are not planning on releasing a PyTorch version of Larq. Can I use Larq for deployment of my models? \u00b6 Currently Larq is designed purely for training BNNs and there is no support for deployment of binarized models. This means that at this moment Larq is most useful to researchers, and less so for developers working on applications. Of course, the real goal of BNNs is efficient deployment, and in the future we will offer solutions for smooth deployment of models created and trained with Larq.","title":"FAQ"},{"location":"faq/#are-there-any-restrictions-on-using-larq","text":"We believe open research and software is the way forward for the field of Binarized Neural Network (BNN) and deep learning and general: machine learning can be a powerful engine for good, but only if it is transparent, save and widely accessible. That is why we have made Larq open-source and free to use. Larq is licensed under Apache 2.0 licence - you are free to contribute to Larq , fork it or build commercial applications on top of it. If you distribute a modified version of Larq, you should clearly state changes you have made and consider contributing them back.","title":"Are there any restrictions on using Larq?"},{"location":"faq/#what-is-a-binarized-neural-network-bnn","text":"A BNN is a deep neural network in which the bulk of the computations are performed using binary values. For example, in a convolutional layer we can use weights that are either -1 or 1, and similarly binarize the activations. Binarization enables the creation of deep learning models that are extremely efficient: storing the model only requires a single bit per weight, and evaluating the model can be done very efficiently because of the bitwise nature of the operations. Note that in BNN not everything is binary: usually higher-precision computations are still used for things like the first layer, batch-normalization and residual connections.","title":"What is a Binarized Neural Network (BNN)?"},{"location":"faq/#how-can-i-cite-larq","text":"If Larq helps you in your work or research, it would be great if you can cite it as follows: @ misc { larq , author = { Geiger , Lukas and Widdicombe , James and Bakhtiari , Arash and Helwegen , Koen and Heuss , Maria and Nusselder , Roeland }, title = { Larq : An Open - Source Deep Learning Library for Training Binarized Neural Networks }, howpublished = { Web page }, url = { https : // larq . dev }, year = { 2019 } } If your paper is publicly available, feel free to also add it to the list of Papers using Larq .","title":"How can I cite Larq?"},{"location":"faq/#can-i-add-my-own-algorithm-or-model-to-larq","text":"Absolutely! If you have developed a new model or training method that you would like to share with the community, create a PR or get in touch with us. Make sure you check out the contribution guide. For entire models with pretrained weights larq-zoo is the correct place, everything else can be added to larq directly.","title":"Can I add my own algorithm or model to Larq?"},{"location":"faq/#can-i-use-larq-only-for-binarized-neural-networks-bnns","text":"No, Larq is not just for BNNs! The real goal of Larq is to make it easy to work with extremely quantized networks. This includes BNNs as well as ternary networks (see for example Ternary Weight Networks or Trained Ternary Quantization ). Although the focus is currently on BNNs, Larq already supports a ternary quantizer and binary and ternary networks have a lot in common. Moreover, modern BNNs are not 'pure' binary networks: they contain higher-precision first and last layers and shortcut connections, for example. We will expand support for ternary and mixed precision networks in the future by implementing layers and optimization tools that are useful in this context, and providing more examples. If there is any particular feature you would like to use, let us know by posting an issue .","title":"Can I use Larq only for Binarized Neural Networks (BNNs)?"},{"location":"faq/#why-is-larq-built-on-top-of-tfkeras","text":"We put a lot of thought into the question of which framework we should build Larq on and decided to go with TensorFlow / Keras over PyTorch or some other framework. There are a number of reasons for this: We really like the Keras API for its simplicity. At the same time, it is still very flexible if you want to build complex architectures or custom training loops. The TensorFlow ecosystem provides a wide range of tools for both researchers and developers. We think integration into that ecosystem will be beneficial for people working with BNNs. We are big fans of tf.datasets . Reproducibility is a key concern to us, and our approach for larq-zoo is heavily inspired by Keras Applications .","title":"Why is Larq built on top of tf.keras?"},{"location":"faq/#will-there-be-a-pytorch-version-of-larq","text":"No, currently we are not planning on releasing a PyTorch version of Larq.","title":"Will there be a PyTorch version of Larq?"},{"location":"faq/#can-i-use-larq-for-deployment-of-my-models","text":"Currently Larq is designed purely for training BNNs and there is no support for deployment of binarized models. This means that at this moment Larq is most useful to researchers, and less so for developers working on applications. Of course, the real goal of BNNs is efficient deployment, and in the future we will offer solutions for smooth deployment of models created and trained with Larq.","title":"Can I use Larq for deployment of my models?"},{"location":"papers/","text":"Papers using Larq \u00b6 One of the main focuses of Larq is to accelerate research on neural networks with extremely low precision weights and activations. If you puplish a paper using using Larq please let us know and add it to the list below . Feel free to also add the author names, abstract and links to the paper and source code. code library_books Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, Roeland Nusselder Optimization of Binarized Neural Networks (BNNs) currently relies on real-valued latent weights to accumulate small update steps. In this paper, we argue that these latent weights cannot be treated analogously to weights in real-valued networks. Instead their main role is to provide inertia during training. We interpret current methods in terms of inertia and provide novel insights into the optimization of BNNs. We subsequently introduce the first optimizer specifically designed for BNNs, Binary Optimizer (Bop), and demonstrate its performance on CIFAR-10 and ImageNet. Together, the redefinition of latent weights as inertia and the introduction of Bop enable a better understanding of BNN optimization and open up the way for further improvements in training methodologies for BNNs.","title":"Papers using Larq"},{"location":"papers/#papers-using-larq","text":"One of the main focuses of Larq is to accelerate research on neural networks with extremely low precision weights and activations. If you puplish a paper using using Larq please let us know and add it to the list below . Feel free to also add the author names, abstract and links to the paper and source code.","title":"Papers using Larq"},{"location":"api/activations/","text":"larq.activations \u00b6 Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers: import tensorflow as tf import larq as lq model . add ( lq . layers . QuantDense ( 64 )) model . add ( tf . keras . layers . Activation ( 'hard_tanh' )) This is equivalent to: model . add ( lq . layers . QuantDense ( 64 , activation = 'hard_tanh' )) You can also pass an element-wise TensorFlow function as an activation: model . add ( lq . layers . QuantDense ( 64 , activation = lq . activations . hard_tanh )) hard_tanh \u00b6 hard_tanh ( x ) Hard tanh activation function. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Hard tanh activation. leaky_tanh \u00b6 leaky_tanh ( x , alpha = 0.2 ) Leaky tanh activation function. Similar to hard tanh, but with non-zero slopes as in leaky ReLU. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. alpha : Slope of the activation function outside of [-1, 1]. Returns Leaky tanh activation.","title":"Activations"},{"location":"api/activations/#larqactivations","text":"Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers: import tensorflow as tf import larq as lq model . add ( lq . layers . QuantDense ( 64 )) model . add ( tf . keras . layers . Activation ( 'hard_tanh' )) This is equivalent to: model . add ( lq . layers . QuantDense ( 64 , activation = 'hard_tanh' )) You can also pass an element-wise TensorFlow function as an activation: model . add ( lq . layers . QuantDense ( 64 , activation = lq . activations . hard_tanh ))","title":"larq.activations"},{"location":"api/activations/#hard_tanh","text":"hard_tanh ( x ) Hard tanh activation function. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Hard tanh activation.","title":"hard_tanh"},{"location":"api/activations/#leaky_tanh","text":"leaky_tanh ( x , alpha = 0.2 ) Leaky tanh activation function. Similar to hard tanh, but with non-zero slopes as in leaky ReLU. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. alpha : Slope of the activation function outside of [-1, 1]. Returns Leaky tanh activation.","title":"leaky_tanh"},{"location":"api/callbacks/","text":"larq.callbacks \u00b6 HyperparameterScheduler \u00b6 HyperparameterScheduler ( schedule , hyperparameter , verbose = 0 ) Generic hyperparameter scheduler. Arguments schedule : a function that takes an epoch index as input (integer, indexed from 0) and returns a new hyperparameter as output. hyperparameter : str. the name of the hyperparameter to be scheduled. verbose : int. 0: quiet, 1: update messages.","title":"Callbacks"},{"location":"api/callbacks/#larqcallbacks","text":"","title":"larq.callbacks"},{"location":"api/callbacks/#hyperparameterscheduler","text":"HyperparameterScheduler ( schedule , hyperparameter , verbose = 0 ) Generic hyperparameter scheduler. Arguments schedule : a function that takes an epoch index as input (integer, indexed from 0) and returns a new hyperparameter as output. hyperparameter : str. the name of the hyperparameter to be scheduled. verbose : int. 0: quiet, 1: update messages.","title":"HyperparameterScheduler"},{"location":"api/constraints/","text":"larq.constraints \u00b6 Functions from the constraints module allow setting constraints (eg. weight clipping) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers QuantDense , QuantConv1D , QuantConv2D and QuantConv3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. import larq as lq lq . layers . QuantDense ( 64 , kernel_constraint = \"weight_clip\" ) lq . layers . QuantDense ( 64 , kernel_constraint = lq . constraints . WeightClip ( 2. )) WeightClip \u00b6 WeightClip ( clip_value = 1 ) Weight Clip constraint Constrains the weights incident to each hidden unit to be between [-clip_value, clip_value] . Arguments clip_value : The value to clip incoming weights. weight_clip \u00b6 weight_clip ( clip_value = 1 )","title":"Constraints"},{"location":"api/constraints/#larqconstraints","text":"Functions from the constraints module allow setting constraints (eg. weight clipping) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers QuantDense , QuantConv1D , QuantConv2D and QuantConv3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. import larq as lq lq . layers . QuantDense ( 64 , kernel_constraint = \"weight_clip\" ) lq . layers . QuantDense ( 64 , kernel_constraint = lq . constraints . WeightClip ( 2. ))","title":"larq.constraints"},{"location":"api/constraints/#weightclip","text":"WeightClip ( clip_value = 1 ) Weight Clip constraint Constrains the weights incident to each hidden unit to be between [-clip_value, clip_value] . Arguments clip_value : The value to clip incoming weights.","title":"WeightClip"},{"location":"api/constraints/#weight_clip","text":"weight_clip ( clip_value = 1 )","title":"weight_clip"},{"location":"api/layers/","text":"larq.layers \u00b6 Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. QuantDense \u00b6 QuantDense ( units , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Just your regular densely-connected quantized NN layer. QuantDense implements the operation: output = activation(dot(input_quantizer(input), kernel_quantizer(kernel)) + bias) , where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Dense . If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Sequential () model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , input_shape = ( 16 ,), ) ) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) ) Arguments units : Positive integer, dimensionality of the output space. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel weights matrix. bias_constraint : Constraint function applied to the bias vector. Input shape N-D tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape N-D tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) . QuantConv1D \u00b6 QuantConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = 'channels_last' , dilation_rate = 1 , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 1D quantized convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv1D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for variable-length sequences of 128-dimensional vectors. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1 :]. Useful when modeling temporal data where the model should not violate the temporal order. See [WaveNet : A Generative Model for Raw Audio, section 2.1](https ://arxiv.org/abs/1609.03499). data_format : A string, one of channels_last (default) or channels_first . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) . steps value might have changed due to padding or strides. QuantConv2D \u00b6 QuantConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 2D quantized convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. QuantConv3D \u00b6 QuantConv3D ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format='channels_last'. new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding. QuantDepthwiseConv2D \u00b6 QuantDepthwiseConv2D ( kernel_size , strides = ( 1 , 1 ), padding = 'valid' , depth_multiplier = 1 , data_format = None , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , bias_constraint = None , ** kwargs ) \"Quantized depthwise separable 2D convolution. Depthwise Separable convolutions consists in performing just the first step in a depthwise spatial convolution (which acts on each input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Arguments kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of 'valid' or 'same' (case-insensitive). depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be 'channels_last'. activation : Activation function to use. If you don't specify anything, no activation is applied (ie. a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise_kernel weights matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its 'activation'). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: [batch, channels, rows, cols] if data_format='channels_first' or 4D tensor with shape: [batch, rows, cols, channels] if data_format='channels_last'. Output shape 4D tensor with shape: [batch, filters, new_rows, new_cols] if data_format='channels_first' or 4D tensor with shape: [batch, new_rows, new_cols, filters] if data_format='channels_last'. rows and cols values might have changed due to padding. QuantSeparableConv1D \u00b6 QuantSeparableConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , dilation_rate = 1 , depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 1D quantized convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of filters in the convolution). kernel_size : A single integer specifying the spatial dimensions of the filters. strides : A single integer specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"same\" , or \"causal\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . dilation_rate : A single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier . activation : Activation function. Set it to None to maintain a linear activation. use_bias : Boolean, whether the layer uses a bias. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel. pointwise_quantizer : Quantization function applied to the pointwise kernel. depthwise_initializer : An initializer for the depthwise convolution kernel. pointwise_initializer : An initializer for the pointwise convolution kernel. bias_initializer : An initializer for the bias vector. If None, the default initializer will be used. depthwise_regularizer : Optional regularizer for the depthwise convolution kernel. pointwise_regularizer : Optional regularizer for the pointwise convolution kernel. bias_regularizer : Optional regularizer for the bias vector. activity_regularizer : Optional regularizer function for the output. depthwise_constraint : Optional projection function to be applied to the depthwise kernel after being updated by an Optimizer (e.g. used for norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. pointwise_constraint : Optional projection function to be applied to the pointwise kernel after being updated by an Optimizer . bias_constraint : Optional projection function to be applied to the bias after being updated by an Optimizer . trainable : Boolean, if True the weights of this layer will be marked as trainable (and listed in layer.trainable_weights ). name : A string, the name of the layer. QuantSeparableConv2D \u00b6 QuantSeparableConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 2D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel matrix. pointwise_quantizer : Quantization function applied to the pointwise kernel matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. pointwise_initializer : Initializer for the pointwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. pointwise_constraint : Constraint function applied to the pointwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. QuantConv2DTranspose \u00b6 QuantConv2DTranspose ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks QuantConv3DTranspose \u00b6 QuantConv3DTranspose ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format='channels_first' or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format='channels_first' or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format='channels_last'. depth and rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks QuantLocallyConnected1D \u00b6 QuantLocallyConnected1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 1D inputs. The QuantLocallyConnected1D layer works similarly to the QuantConv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected1D . Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Sequential () model . add ( QuantLocallyConnected1D ( 64 , 3 , input_shape = ( 10 , 32 ))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model . add ( QuantLocallyConnected1D ( 32 , 3 )) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides. QuantLocallyConnected2D \u00b6 QuantLocallyConnected2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 2D inputs. The QuantLocallyConnected2D layer works similarly to the QuantConv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected2D . Example # apply a 3x3 unshared weights convolution with 64 output filters on a 32 x32 image # with `data_format=\"channels_last\"`: model = Sequential () model . add ( QuantLocallyConnected2D ( 64 , ( 3 , 3 ), input_shape = ( 32 , 32 , 3 ))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model . add ( QuantLocallyConnected2D ( 32 , ( 3 , 3 ))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"Layers"},{"location":"api/layers/#larqlayers","text":"Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer.","title":"larq.layers"},{"location":"api/layers/#quantdense","text":"QuantDense ( units , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Just your regular densely-connected quantized NN layer. QuantDense implements the operation: output = activation(dot(input_quantizer(input), kernel_quantizer(kernel)) + bias) , where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Dense . If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Sequential () model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , input_shape = ( 16 ,), ) ) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) ) Arguments units : Positive integer, dimensionality of the output space. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel weights matrix. bias_constraint : Constraint function applied to the bias vector. Input shape N-D tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape N-D tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) .","title":"QuantDense"},{"location":"api/layers/#quantconv1d","text":"QuantConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = 'channels_last' , dilation_rate = 1 , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 1D quantized convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv1D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for variable-length sequences of 128-dimensional vectors. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1 :]. Useful when modeling temporal data where the model should not violate the temporal order. See [WaveNet : A Generative Model for Raw Audio, section 2.1](https ://arxiv.org/abs/1609.03499). data_format : A string, one of channels_last (default) or channels_first . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) . steps value might have changed due to padding or strides.","title":"QuantConv1D"},{"location":"api/layers/#quantconv2d","text":"QuantConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 2D quantized convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantConv2D"},{"location":"api/layers/#quantconv3d","text":"QuantConv3D ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format='channels_last'. new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding.","title":"QuantConv3D"},{"location":"api/layers/#quantdepthwiseconv2d","text":"QuantDepthwiseConv2D ( kernel_size , strides = ( 1 , 1 ), padding = 'valid' , depth_multiplier = 1 , data_format = None , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , bias_constraint = None , ** kwargs ) \"Quantized depthwise separable 2D convolution. Depthwise Separable convolutions consists in performing just the first step in a depthwise spatial convolution (which acts on each input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Arguments kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of 'valid' or 'same' (case-insensitive). depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be 'channels_last'. activation : Activation function to use. If you don't specify anything, no activation is applied (ie. a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise_kernel weights matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its 'activation'). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: [batch, channels, rows, cols] if data_format='channels_first' or 4D tensor with shape: [batch, rows, cols, channels] if data_format='channels_last'. Output shape 4D tensor with shape: [batch, filters, new_rows, new_cols] if data_format='channels_first' or 4D tensor with shape: [batch, new_rows, new_cols, filters] if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantDepthwiseConv2D"},{"location":"api/layers/#quantseparableconv1d","text":"QuantSeparableConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , dilation_rate = 1 , depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 1D quantized convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of filters in the convolution). kernel_size : A single integer specifying the spatial dimensions of the filters. strides : A single integer specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"same\" , or \"causal\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . dilation_rate : A single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier . activation : Activation function. Set it to None to maintain a linear activation. use_bias : Boolean, whether the layer uses a bias. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel. pointwise_quantizer : Quantization function applied to the pointwise kernel. depthwise_initializer : An initializer for the depthwise convolution kernel. pointwise_initializer : An initializer for the pointwise convolution kernel. bias_initializer : An initializer for the bias vector. If None, the default initializer will be used. depthwise_regularizer : Optional regularizer for the depthwise convolution kernel. pointwise_regularizer : Optional regularizer for the pointwise convolution kernel. bias_regularizer : Optional regularizer for the bias vector. activity_regularizer : Optional regularizer function for the output. depthwise_constraint : Optional projection function to be applied to the depthwise kernel after being updated by an Optimizer (e.g. used for norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. pointwise_constraint : Optional projection function to be applied to the pointwise kernel after being updated by an Optimizer . bias_constraint : Optional projection function to be applied to the bias after being updated by an Optimizer . trainable : Boolean, if True the weights of this layer will be marked as trainable (and listed in layer.trainable_weights ). name : A string, the name of the layer.","title":"QuantSeparableConv1D"},{"location":"api/layers/#quantseparableconv2d","text":"QuantSeparableConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 2D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel matrix. pointwise_quantizer : Quantization function applied to the pointwise kernel matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. pointwise_initializer : Initializer for the pointwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. pointwise_constraint : Constraint function applied to the pointwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantSeparableConv2D"},{"location":"api/layers/#quantconv2dtranspose","text":"QuantConv2DTranspose ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks","title":"QuantConv2DTranspose"},{"location":"api/layers/#quantconv3dtranspose","text":"QuantConv3DTranspose ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format='channels_first' or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format='channels_first' or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format='channels_last'. depth and rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks","title":"QuantConv3DTranspose"},{"location":"api/layers/#quantlocallyconnected1d","text":"QuantLocallyConnected1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 1D inputs. The QuantLocallyConnected1D layer works similarly to the QuantConv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected1D . Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Sequential () model . add ( QuantLocallyConnected1D ( 64 , 3 , input_shape = ( 10 , 32 ))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model . add ( QuantLocallyConnected1D ( 32 , 3 )) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides.","title":"QuantLocallyConnected1D"},{"location":"api/layers/#quantlocallyconnected2d","text":"QuantLocallyConnected2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 2D inputs. The QuantLocallyConnected2D layer works similarly to the QuantConv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected2D . Example # apply a 3x3 unshared weights convolution with 64 output filters on a 32 x32 image # with `data_format=\"channels_last\"`: model = Sequential () model . add ( QuantLocallyConnected2D ( 64 , ( 3 , 3 ), input_shape = ( 32 , 32 , 3 ))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model . add ( QuantLocallyConnected2D ( 32 , ( 3 , 3 ))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantLocallyConnected2D"},{"location":"api/math/","text":"larq.math \u00b6 Math operations that are specific to extremely quantized networks. sign \u00b6 sign ( x ) A sign function that will never be zero \\[ f(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] Arguments x : Input Tensor Returns A Tensor with same type as x .","title":"Math"},{"location":"api/math/#larqmath","text":"Math operations that are specific to extremely quantized networks.","title":"larq.math"},{"location":"api/math/#sign","text":"sign ( x ) A sign function that will never be zero \\[ f(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] Arguments x : Input Tensor Returns A Tensor with same type as x .","title":"sign"},{"location":"api/models/","text":"larq.models \u00b6 summary \u00b6 summary ( model , print_fn = None ) Prints a string summary of the network. Arguments model : tf.keras model instance. print_fn : Print function to use. Defaults to print . You can set it to a custom function in order to capture the string summary. Raises ValueError : if called before the model is built.","title":"Models"},{"location":"api/models/#larqmodels","text":"","title":"larq.models"},{"location":"api/models/#summary","text":"summary ( model , print_fn = None ) Prints a string summary of the network. Arguments model : tf.keras model instance. print_fn : Print function to use. Defaults to print . You can set it to a custom function in order to capture the string summary. Raises ValueError : if called before the model is built.","title":"summary"},{"location":"api/optimizers/","text":"larq.optimizers \u00b6 Bop \u00b6 Bop ( fp_optimizer , threshold = 1e-05 , gamma = 0.01 , name = 'Bop' , ** kwargs ) Binary optimizer (Bop). Bop is a latent-free optimizer for Binarized Neural Networks (BNNs). Example optimizer = lq . optimizers . Bop ( fp_optimizer = tf . keras . optimizers . Adam ( 0.01 )) Arguments fp_optimizer : a tf.keras.optimizers.Optimizer . threshold : determines to whether to flip each weight. gamma : the adaptivity rate. name : name of the optimizer. References Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization XavierLearningRateScaling \u00b6 XavierLearningRateScaling ( optimizer , model ) Optimizer wrapper for Xavier Learning Rate Scaling Scale the weights learning rates respectively with the weights initialization This is a wrapper and does not implement any optimization algorithm. Example optimizer = lq . optimizers . XavierLearningRateScaling ( tf . keras . optimizers . Adam ( 0.01 ), model ) Arguments optimizer : A tf.keras.optimizers.Optimizer model : A tf.keras.Model References BinaryConnect: Training Deep Neural Networks with binary weights during propagations","title":"Optimizers"},{"location":"api/optimizers/#larqoptimizers","text":"","title":"larq.optimizers"},{"location":"api/optimizers/#bop","text":"Bop ( fp_optimizer , threshold = 1e-05 , gamma = 0.01 , name = 'Bop' , ** kwargs ) Binary optimizer (Bop). Bop is a latent-free optimizer for Binarized Neural Networks (BNNs). Example optimizer = lq . optimizers . Bop ( fp_optimizer = tf . keras . optimizers . Adam ( 0.01 )) Arguments fp_optimizer : a tf.keras.optimizers.Optimizer . threshold : determines to whether to flip each weight. gamma : the adaptivity rate. name : name of the optimizer. References Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization","title":"Bop"},{"location":"api/optimizers/#xavierlearningratescaling","text":"XavierLearningRateScaling ( optimizer , model ) Optimizer wrapper for Xavier Learning Rate Scaling Scale the weights learning rates respectively with the weights initialization This is a wrapper and does not implement any optimization algorithm. Example optimizer = lq . optimizers . XavierLearningRateScaling ( tf . keras . optimizers . Adam ( 0.01 ), model ) Arguments optimizer : A tf.keras.optimizers.Optimizer model : A tf.keras.Model References BinaryConnect: Training Deep Neural Networks with binary weights during propagations","title":"XavierLearningRateScaling"},{"location":"api/quantizers/","text":"larq.quantizers \u00b6 A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Quantizers can either be used through quantizer arguments that are supported for Larq layers, such as input_quantizer and kernel_quantizer ; or they can be used similar to activations, i.e. either through an Activation layer, or through the activation argument supported by all forward layer: import tensorflow as tf import larq as lq ... x = lq . layers . QuantDense ( 64 , activation = None )( x ) x = lq . layers . QuantDense ( 64 , input_quantizer = \"ste_sign\" )( x ) is equivalent to: x = lq . layers . QuantDense ( 64 )( x ) x = tf . keras . layers . Activation ( \"ste_sign\" )( x ) x = lq . layers . QuantDense ( 64 )( x ) as well as: x = lq . layers . QuantDense ( 64 , activation = \"ste_sign\" )( x ) x = lq . layers . QuantDense ( 64 )( x ) We highly recommend using the first of these formulations: for the other two formulations, intermediate layers - like batch normalization or average pooling - and shortcut connections may result in non-binary input to the convolutions. ste_sign \u00b6 ste_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the Straight-Through Estimator (essentially the binarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 approx_sign \u00b6 approx_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the ApproxSign method. \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} (2 - 2 \\left|x\\right|) & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} \\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.5 1.0 1.5 2.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm magnitude_aware_sign \u00b6 magnitude_aware_sign ( x ) Magnitude-aware sign for Bi-Real Net. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22122 \u22121 0 1 2 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor Returns Scaled binarized tensor (with values in \\{-a, a\\} \\{-a, a\\} , where a a is a float). References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm SteTern \u00b6 SteTern ( threshold_value = 0.05 , ternary_weight_networks = False ) Ternarization function. \\[ q(x) = \\begin{cases} +1 & x > \\Delta \\\\ 0 & |x| < \\Delta \\\\ -1 & x < - \\Delta \\end{cases} \\] where \\Delta \\Delta is defined as the threshold and can be passed as an argument, or can be calculated as per the Ternary Weight Networks original paper, such that \\[ \\Delta = \\frac{0.7}{n} \\sum_{i=1}^{n} |W_i| \\] where we assume that W_i W_i is generated from a normal distribution. The gradient is estimated using the Straight-Through Estimator (essentially the Ternarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. threshold_value : The value for the threshold, \\Delta \\Delta . ternary_weight_networks : Boolean of whether to use the Ternary Weight Networks threshold calculation. Returns Ternarized tensor. References Ternary Weight Networks","title":"Quantizers"},{"location":"api/quantizers/#larqquantizers","text":"A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Quantizers can either be used through quantizer arguments that are supported for Larq layers, such as input_quantizer and kernel_quantizer ; or they can be used similar to activations, i.e. either through an Activation layer, or through the activation argument supported by all forward layer: import tensorflow as tf import larq as lq ... x = lq . layers . QuantDense ( 64 , activation = None )( x ) x = lq . layers . QuantDense ( 64 , input_quantizer = \"ste_sign\" )( x ) is equivalent to: x = lq . layers . QuantDense ( 64 )( x ) x = tf . keras . layers . Activation ( \"ste_sign\" )( x ) x = lq . layers . QuantDense ( 64 )( x ) as well as: x = lq . layers . QuantDense ( 64 , activation = \"ste_sign\" )( x ) x = lq . layers . QuantDense ( 64 )( x ) We highly recommend using the first of these formulations: for the other two formulations, intermediate layers - like batch normalization or average pooling - and shortcut connections may result in non-binary input to the convolutions.","title":"larq.quantizers"},{"location":"api/quantizers/#ste_sign","text":"ste_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the Straight-Through Estimator (essentially the binarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1","title":"ste_sign"},{"location":"api/quantizers/#approx_sign","text":"approx_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the ApproxSign method. \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} (2 - 2 \\left|x\\right|) & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} \\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.5 1.0 1.5 2.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"approx_sign"},{"location":"api/quantizers/#magnitude_aware_sign","text":"magnitude_aware_sign ( x ) Magnitude-aware sign for Bi-Real Net. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22122 \u22121 0 1 2 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor Returns Scaled binarized tensor (with values in \\{-a, a\\} \\{-a, a\\} , where a a is a float). References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"magnitude_aware_sign"},{"location":"api/quantizers/#stetern","text":"SteTern ( threshold_value = 0.05 , ternary_weight_networks = False ) Ternarization function. \\[ q(x) = \\begin{cases} +1 & x > \\Delta \\\\ 0 & |x| < \\Delta \\\\ -1 & x < - \\Delta \\end{cases} \\] where \\Delta \\Delta is defined as the threshold and can be passed as an argument, or can be calculated as per the Ternary Weight Networks original paper, such that \\[ \\Delta = \\frac{0.7}{n} \\sum_{i=1}^{n} |W_i| \\] where we assume that W_i W_i is generated from a normal distribution. The gradient is estimated using the Straight-Through Estimator (essentially the Ternarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. threshold_value : The value for the threshold, \\Delta \\Delta . ternary_weight_networks : Boolean of whether to use the Ternary Weight Networks threshold calculation. Returns Ternarized tensor. References Ternary Weight Networks","title":"SteTern"},{"location":"examples/binarynet_advanced_cifar10/","text":"BinaryNet on CIFAR10 (Advanced) \u00b6 Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy of around 90% using a heavy lifting GPU like a Nvidia V100. On a Nvidia V100 it takes approximately 250 minutes to train. Compared to the original papers, BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not implement image whitening, but we use image augmentation, and a stepped learning rate scheduler. import tensorflow as tf import larq as lq import numpy as np Import CIFAR10 Dataset \u00b6 Here we download the CIFAR10 dataset: train_data , test_data = tf . keras . datasets . cifar10 . load_data () Next, we define our image augmentation technqiues, and create the dataset: def resize_and_flip ( image , labels , training ): image = tf . cast ( image , tf . float32 ) / ( 255. / 2. ) - 1. if training : image = tf . image . resize_image_with_crop_or_pad ( image , 40 , 40 ) image = tf . random_crop ( image , [ 32 , 32 , 3 ]) image = tf . image . random_flip_left_right ( image ) return image , labels def create_dataset ( data , batch_size , training ): images , labels = data labels = tf . one_hot ( np . squeeze ( labels ), 10 ) dataset = tf . data . Dataset . from_tensor_slices (( images , labels )) dataset = dataset . repeat () if training : dataset = dataset . shuffle ( 1000 ) dataset = dataset . map ( lambda x , y : resize_and_flip ( x , y , training )) dataset = dataset . batch ( batch_size ) return dataset batch_size = 50 train_dataset = create_dataset ( train_data , batch_size , True ) test_dataset = create_dataset ( test_data , batch_size , False ) Build BinaryNet \u00b6 Here we build the binarynet model layer by layer using a keras sequential model: # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) Larq allows you to print a summary of the model that includes bit-precision information: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+ Model Training \u00b6 We compile and train the model as you are used to in Keras: initial_lr = 1e-3 var_decay = 1e-5 optimizer = tf . keras . optimizers . Adam ( lr = initial_lr , decay = var_decay ) model . compile ( optimizer = lq . optimizers . XavierLearningRateScaling ( optimizer , model ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) def lr_schedule ( epoch ): return initial_lr * 0.1 ** ( epoch // 100 ) trained_model = model . fit ( train_dataset , epochs = 500 , steps_per_epoch = train_data [ 1 ] . shape [ 0 ] // batch_size , validation_data = test_dataset , validation_steps = test_data [ 1 ] . shape [ 0 ] // batch_size , verbose = 1 , callbacks = [ tf . keras . callbacks . LearningRateScheduler ( lr_schedule )] )","title":"BinaryNet on CIFAR10 (Advanced)"},{"location":"examples/binarynet_advanced_cifar10/#binarynet-on-cifar10-advanced","text":"Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy of around 90% using a heavy lifting GPU like a Nvidia V100. On a Nvidia V100 it takes approximately 250 minutes to train. Compared to the original papers, BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not implement image whitening, but we use image augmentation, and a stepped learning rate scheduler. import tensorflow as tf import larq as lq import numpy as np","title":"BinaryNet on CIFAR10 (Advanced)"},{"location":"examples/binarynet_advanced_cifar10/#import-cifar10-dataset","text":"Here we download the CIFAR10 dataset: train_data , test_data = tf . keras . datasets . cifar10 . load_data () Next, we define our image augmentation technqiues, and create the dataset: def resize_and_flip ( image , labels , training ): image = tf . cast ( image , tf . float32 ) / ( 255. / 2. ) - 1. if training : image = tf . image . resize_image_with_crop_or_pad ( image , 40 , 40 ) image = tf . random_crop ( image , [ 32 , 32 , 3 ]) image = tf . image . random_flip_left_right ( image ) return image , labels def create_dataset ( data , batch_size , training ): images , labels = data labels = tf . one_hot ( np . squeeze ( labels ), 10 ) dataset = tf . data . Dataset . from_tensor_slices (( images , labels )) dataset = dataset . repeat () if training : dataset = dataset . shuffle ( 1000 ) dataset = dataset . map ( lambda x , y : resize_and_flip ( x , y , training )) dataset = dataset . batch ( batch_size ) return dataset batch_size = 50 train_dataset = create_dataset ( train_data , batch_size , True ) test_dataset = create_dataset ( test_data , batch_size , False )","title":"Import CIFAR10 Dataset"},{"location":"examples/binarynet_advanced_cifar10/#build-binarynet","text":"Here we build the binarynet model layer by layer using a keras sequential model: # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) Larq allows you to print a summary of the model that includes bit-precision information: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+","title":"Build BinaryNet"},{"location":"examples/binarynet_advanced_cifar10/#model-training","text":"We compile and train the model as you are used to in Keras: initial_lr = 1e-3 var_decay = 1e-5 optimizer = tf . keras . optimizers . Adam ( lr = initial_lr , decay = var_decay ) model . compile ( optimizer = lq . optimizers . XavierLearningRateScaling ( optimizer , model ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) def lr_schedule ( epoch ): return initial_lr * 0.1 ** ( epoch // 100 ) trained_model = model . fit ( train_dataset , epochs = 500 , steps_per_epoch = train_data [ 1 ] . shape [ 0 ] // batch_size , validation_data = test_dataset , validation_steps = test_data [ 1 ] . shape [ 0 ] // batch_size , verbose = 1 , callbacks = [ tf . keras . callbacks . LearningRateScheduler ( lr_schedule )] )","title":"Model Training"},{"location":"examples/binarynet_cifar10/","text":"BinaryNet on CIFAR10 \u00b6 Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy approximately 83% on laptop hardware. On a Nvidia GTX 1050 Ti Max-Q it takes approximately 200 minutes to train. For simplicity, compared to the original papers BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not impliment learning rate scaling, or image whitening. import tensorflow as tf import larq as lq import numpy as np import matplotlib.pyplot as plt Import CIFAR10 Dataset \u00b6 We download and normalize the CIFAR10 dataset. num_classes = 10 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . cifar10 . load_data () train_images = train_images . reshape (( 50000 , 32 , 32 , 3 )) . astype ( \"float32\" ) test_images = test_images . reshape (( 10000 , 32 , 32 , 3 )) . astype ( \"float32\" ) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1 train_labels = tf . keras . utils . to_categorical ( train_labels , num_classes ) test_labels = tf . keras . utils . to_categorical ( test_labels , num_classes ) Build BinaryNet \u00b6 Here we build the BinaryNet model layer by layer using the Keras Sequential API . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) One can output a summary of the model: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+ Model Training \u00b6 Compile the model and train the model model . compile ( tf . keras . optimizers . Adam ( lr = 0.01 , decay = 0.0001 ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) trained_model = model . fit ( train_images , train_labels , batch_size = 50 , epochs = 100 , validation_data = ( test_images , test_labels ), shuffle = True ) Train on 50000 samples , validate on 10000 samples Epoch 1 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 1 . 5733 - acc : 0 . 4533 - val_loss : 1 . 6368 - val_acc : 0 . 4244 Epoch 2 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 1 . 1485 - acc : 0 . 6387 - val_loss : 1 . 8497 - val_acc : 0 . 3764 Epoch 3 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 9641 - acc : 0 . 7207 - val_loss : 1 . 5696 - val_acc : 0 . 4794 Epoch 4 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 8452 - acc : 0 . 7728 - val_loss : 1 . 5765 - val_acc : 0 . 4669 Epoch 5 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 7553 - acc : 0 . 8114 - val_loss : 1 . 0653 - val_acc : 0 . 6928 Epoch 6 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 6841 - acc : 0 . 8447 - val_loss : 1 . 0944 - val_acc : 0 . 6880 Epoch 7 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 6356 - acc : 0 . 8685 - val_loss : 0 . 9909 - val_acc : 0 . 7317 Epoch 8 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5907 - acc : 0 . 8910 - val_loss : 0 . 9453 - val_acc : 0 . 7446 Epoch 9 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5610 - acc : 0 . 9043 - val_loss : 0 . 9441 - val_acc : 0 . 7460 Epoch 10 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 5295 - acc : 0 . 9201 - val_loss : 0 . 8892 - val_acc : 0 . 7679 Epoch 11 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 5100 - acc : 0 . 9309 - val_loss : 0 . 8808 - val_acc : 0 . 7818 Epoch 12 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4926 - acc : 0 . 9397 - val_loss : 0 . 8404 - val_acc : 0 . 7894 Epoch 13 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4807 - acc : 0 . 9470 - val_loss : 0 . 8600 - val_acc : 0 . 7928 Epoch 14 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4661 - acc : 0 . 9529 - val_loss : 0 . 9046 - val_acc : 0 . 7732 Epoch 15 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 4588 - acc : 0 . 9571 - val_loss : 0 . 8505 - val_acc : 0 . 7965 Epoch 16 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4558 - acc : 0 . 9593 - val_loss : 0 . 8748 - val_acc : 0 . 7859 Epoch 17 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4434 - acc : 0 . 9649 - val_loss : 0 . 9109 - val_acc : 0 . 7656 Epoch 18 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4449 - acc : 0 . 9643 - val_loss : 0 . 8532 - val_acc : 0 . 7971 Epoch 19 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4349 - acc : 0 . 9701 - val_loss : 0 . 8677 - val_acc : 0 . 7951 Epoch 20 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4351 - acc : 0 . 9698 - val_loss : 0 . 9145 - val_acc : 0 . 7740 Epoch 21 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4268 - acc : 0 . 9740 - val_loss : 0 . 8308 - val_acc : 0 . 8065 Epoch 22 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4243 - acc : 0 . 9741 - val_loss : 0 . 8229 - val_acc : 0 . 8075 Epoch 23 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4201 - acc : 0 . 9764 - val_loss : 0 . 8411 - val_acc : 0 . 8062 Epoch 24 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4190 - acc : 0 . 9769 - val_loss : 0 . 8649 - val_acc : 0 . 7951 Epoch 25 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4139 - acc : 0 . 9787 - val_loss : 0 . 8257 - val_acc : 0 . 8071 Epoch 26 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4154 - acc : 0 . 9779 - val_loss : 0 . 8041 - val_acc : 0 . 8205 Epoch 27 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4128 - acc : 0 . 9798 - val_loss : 0 . 8296 - val_acc : 0 . 8115 Epoch 28 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4121 - acc : 0 . 9798 - val_loss : 0 . 8241 - val_acc : 0 . 8074 Epoch 29 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4093 - acc : 0 . 9807 - val_loss : 0 . 8575 - val_acc : 0 . 7913 Epoch 30 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4048 - acc : 0 . 9826 - val_loss : 0 . 8118 - val_acc : 0 . 8166 Epoch 31 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4041 - acc : 0 . 9837 - val_loss : 0 . 8375 - val_acc : 0 . 8082 Epoch 32 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4045 - acc : 0 . 9831 - val_loss : 0 . 8604 - val_acc : 0 . 8091 Epoch 33 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4047 - acc : 0 . 9823 - val_loss : 0 . 8797 - val_acc : 0 . 7931 Epoch 34 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4023 - acc : 0 . 9842 - val_loss : 0 . 8694 - val_acc : 0 . 8020 Epoch 35 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3995 - acc : 0 . 9858 - val_loss : 0 . 8161 - val_acc : 0 . 8186 Epoch 36 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3976 - acc : 0 . 9859 - val_loss : 0 . 8495 - val_acc : 0 . 7988 Epoch 37 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4021 - acc : 0 . 9847 - val_loss : 0 . 8542 - val_acc : 0 . 8062 Epoch 38 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3939 - acc : 0 . 9869 - val_loss : 0 . 8347 - val_acc : 0 . 8122 Epoch 39 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3955 - acc : 0 . 9856 - val_loss : 0 . 8521 - val_acc : 0 . 7993 Epoch 40 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3907 - acc : 0 . 9885 - val_loss : 0 . 9023 - val_acc : 0 . 7992 Epoch 41 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3911 - acc : 0 . 9873 - val_loss : 0 . 8597 - val_acc : 0 . 8010 Epoch 42 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3917 - acc : 0 . 9885 - val_loss : 0 . 8968 - val_acc : 0 . 7936 Epoch 43 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3931 - acc : 0 . 9874 - val_loss : 0 . 8318 - val_acc : 0 . 8169 Epoch 44 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3897 - acc : 0 . 9893 - val_loss : 0 . 8811 - val_acc : 0 . 7988 Epoch 45 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9888 - val_loss : 0 . 8453 - val_acc : 0 . 8094 Epoch 46 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9889 - val_loss : 0 . 8195 - val_acc : 0 . 8179 Epoch 47 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3891 - acc : 0 . 9890 - val_loss : 0 . 8373 - val_acc : 0 . 8137 Epoch 48 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3902 - acc : 0 . 9888 - val_loss : 0 . 8457 - val_acc : 0 . 8120 Epoch 49 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3864 - acc : 0 . 9903 - val_loss : 0 . 9012 - val_acc : 0 . 7907 Epoch 50 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3859 - acc : 0 . 9903 - val_loss : 0 . 8291 - val_acc : 0 . 8053 Epoch 51 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3830 - acc : 0 . 9915 - val_loss : 0 . 8494 - val_acc : 0 . 8139 Epoch 52 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3828 - acc : 0 . 9907 - val_loss : 0 . 8447 - val_acc : 0 . 8135 Epoch 53 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9910 - val_loss : 0 . 8539 - val_acc : 0 . 8120 Epoch 54 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3832 - acc : 0 . 9905 - val_loss : 0 . 8592 - val_acc : 0 . 8098 Epoch 55 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9908 - val_loss : 0 . 8585 - val_acc : 0 . 8087 Epoch 56 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3817 - acc : 0 . 9911 - val_loss : 0 . 8840 - val_acc : 0 . 7889 Epoch 57 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3827 - acc : 0 . 9914 - val_loss : 0 . 8205 - val_acc : 0 . 8250 Epoch 58 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3818 - acc : 0 . 9912 - val_loss : 0 . 8571 - val_acc : 0 . 8051 Epoch 59 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3811 - acc : 0 . 9919 - val_loss : 0 . 8155 - val_acc : 0 . 8254 Epoch 60 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3803 - acc : 0 . 9919 - val_loss : 0 . 8617 - val_acc : 0 . 8040 Epoch 61 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3793 - acc : 0 . 9926 - val_loss : 0 . 8212 - val_acc : 0 . 8192 Epoch 62 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3825 - acc : 0 . 9912 - val_loss : 0 . 8139 - val_acc : 0 . 8277 Epoch 63 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9923 - val_loss : 0 . 8304 - val_acc : 0 . 8121 Epoch 64 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3809 - acc : 0 . 9918 - val_loss : 0 . 7961 - val_acc : 0 . 8289 Epoch 65 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3750 - acc : 0 . 9930 - val_loss : 0 . 8676 - val_acc : 0 . 8110 Epoch 66 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3789 - acc : 0 . 9928 - val_loss : 0 . 8308 - val_acc : 0 . 8148 Epoch 67 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3783 - acc : 0 . 9929 - val_loss : 0 . 8595 - val_acc : 0 . 8097 Epoch 68 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9935 - val_loss : 0 . 8359 - val_acc : 0 . 8065 Epoch 69 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9927 - val_loss : 0 . 8189 - val_acc : 0 . 8255 Epoch 70 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3786 - acc : 0 . 9924 - val_loss : 0 . 8754 - val_acc : 0 . 8001 Epoch 71 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9936 - val_loss : 0 . 8188 - val_acc : 0 . 8262 Epoch 72 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9932 - val_loss : 0 . 8540 - val_acc : 0 . 8169 Epoch 73 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3740 - acc : 0 . 9934 - val_loss : 0 . 8127 - val_acc : 0 . 8258 Epoch 74 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9932 - val_loss : 0 . 8662 - val_acc : 0 . 8018 Epoch 75 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3721 - acc : 0 . 9941 - val_loss : 0 . 8359 - val_acc : 0 . 8213 Epoch 76 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3746 - acc : 0 . 9937 - val_loss : 0 . 8462 - val_acc : 0 . 8178 Epoch 77 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3741 - acc : 0 . 9936 - val_loss : 0 . 8983 - val_acc : 0 . 7972 Epoch 78 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3751 - acc : 0 . 9933 - val_loss : 0 . 8525 - val_acc : 0 . 8173 Epoch 79 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3762 - acc : 0 . 9931 - val_loss : 0 . 8190 - val_acc : 0 . 8201 Epoch 80 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3737 - acc : 0 . 9940 - val_loss : 0 . 8441 - val_acc : 0 . 8196 Epoch 81 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3729 - acc : 0 . 9935 - val_loss : 0 . 8151 - val_acc : 0 . 8267 Epoch 82 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3735 - acc : 0 . 9938 - val_loss : 0 . 8405 - val_acc : 0 . 8163 Epoch 83 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3723 - acc : 0 . 9939 - val_loss : 0 . 8225 - val_acc : 0 . 8243 Epoch 84 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3738 - acc : 0 . 9938 - val_loss : 0 . 8413 - val_acc : 0 . 8115 Epoch 85 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3714 - acc : 0 . 9947 - val_loss : 0 . 9080 - val_acc : 0 . 7932 Epoch 86 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3744 - acc : 0 . 9942 - val_loss : 0 . 8467 - val_acc : 0 . 8135 Epoch 87 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3705 - acc : 0 . 9948 - val_loss : 0 . 8491 - val_acc : 0 . 8163 Epoch 88 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3733 - acc : 0 . 9944 - val_loss : 0 . 8005 - val_acc : 0 . 8214 Epoch 89 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3693 - acc : 0 . 9949 - val_loss : 0 . 7791 - val_acc : 0 . 8321 Epoch 90 / 100 50000 / 50000 [ ============================== ] - 135 s 3 ms / step - loss : 0 . 3724 - acc : 0 . 9942 - val_loss : 0 . 8458 - val_acc : 0 . 8124 Epoch 91 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3732 - acc : 0 . 9947 - val_loss : 0 . 8315 - val_acc : 0 . 8164 Epoch 92 / 100 50000 / 50000 [ ============================== ] - 127 s 3 ms / step - loss : 0 . 3699 - acc : 0 . 9950 - val_loss : 0 . 8140 - val_acc : 0 . 8226 Epoch 93 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 0 . 3694 - acc : 0 . 9950 - val_loss : 0 . 8342 - val_acc : 0 . 8210 Epoch 94 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8938 - val_acc : 0 . 8019 Epoch 95 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8771 - val_acc : 0 . 8066 Epoch 96 / 100 50000 / 50000 [ ============================== ] - 164 s 3 ms / step - loss : 0 . 3712 - acc : 0 . 9946 - val_loss : 0 . 8396 - val_acc : 0 . 8211 Epoch 97 / 100 50000 / 50000 [ ============================== ] - 155 s 3 ms / step - loss : 0 . 3689 - acc : 0 . 9949 - val_loss : 0 . 8728 - val_acc : 0 . 8112 Epoch 98 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3663 - acc : 0 . 9953 - val_loss : 0 . 9615 - val_acc : 0 . 7902 Epoch 99 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3714 - acc : 0 . 9944 - val_loss : 0 . 8414 - val_acc : 0 . 8188 Epoch 100 / 100 50000 / 50000 [ ============================== ] - 138 s 3 ms / step - loss : 0 . 3682 - acc : 0 . 9956 - val_loss : 0 . 8055 - val_acc : 0 . 8266 Model Output \u00b6 We can now plot the final validation accuracy and loss: plt . plot ( trained_model . history [ 'acc' ]) plt . plot ( trained_model . history [ 'val_acc' ]) plt . title ( 'model accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . max ( trained_model . history [ 'acc' ])) print ( np . max ( trained_model . history [ 'val_acc' ])) 0 . 9956000019311905 0 . 8320999944210052 plt . plot ( trained_model . history [ 'loss' ]) plt . plot ( trained_model . history [ 'val_loss' ]) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . min ( trained_model . history [ 'loss' ])) print ( np . min ( trained_model . history [ 'val_loss' ])) 0 . 3663262344896793 0 . 7790719392895699","title":"BinaryNet on CIFAR10"},{"location":"examples/binarynet_cifar10/#binarynet-on-cifar10","text":"Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy approximately 83% on laptop hardware. On a Nvidia GTX 1050 Ti Max-Q it takes approximately 200 minutes to train. For simplicity, compared to the original papers BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not impliment learning rate scaling, or image whitening. import tensorflow as tf import larq as lq import numpy as np import matplotlib.pyplot as plt","title":"BinaryNet on CIFAR10"},{"location":"examples/binarynet_cifar10/#import-cifar10-dataset","text":"We download and normalize the CIFAR10 dataset. num_classes = 10 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . cifar10 . load_data () train_images = train_images . reshape (( 50000 , 32 , 32 , 3 )) . astype ( \"float32\" ) test_images = test_images . reshape (( 10000 , 32 , 32 , 3 )) . astype ( \"float32\" ) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1 train_labels = tf . keras . utils . to_categorical ( train_labels , num_classes ) test_labels = tf . keras . utils . to_categorical ( test_labels , num_classes )","title":"Import CIFAR10 Dataset"},{"location":"examples/binarynet_cifar10/#build-binarynet","text":"Here we build the BinaryNet model layer by layer using the Keras Sequential API . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) One can output a summary of the model: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+","title":"Build BinaryNet"},{"location":"examples/binarynet_cifar10/#model-training","text":"Compile the model and train the model model . compile ( tf . keras . optimizers . Adam ( lr = 0.01 , decay = 0.0001 ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) trained_model = model . fit ( train_images , train_labels , batch_size = 50 , epochs = 100 , validation_data = ( test_images , test_labels ), shuffle = True ) Train on 50000 samples , validate on 10000 samples Epoch 1 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 1 . 5733 - acc : 0 . 4533 - val_loss : 1 . 6368 - val_acc : 0 . 4244 Epoch 2 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 1 . 1485 - acc : 0 . 6387 - val_loss : 1 . 8497 - val_acc : 0 . 3764 Epoch 3 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 9641 - acc : 0 . 7207 - val_loss : 1 . 5696 - val_acc : 0 . 4794 Epoch 4 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 8452 - acc : 0 . 7728 - val_loss : 1 . 5765 - val_acc : 0 . 4669 Epoch 5 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 7553 - acc : 0 . 8114 - val_loss : 1 . 0653 - val_acc : 0 . 6928 Epoch 6 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 6841 - acc : 0 . 8447 - val_loss : 1 . 0944 - val_acc : 0 . 6880 Epoch 7 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 6356 - acc : 0 . 8685 - val_loss : 0 . 9909 - val_acc : 0 . 7317 Epoch 8 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5907 - acc : 0 . 8910 - val_loss : 0 . 9453 - val_acc : 0 . 7446 Epoch 9 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5610 - acc : 0 . 9043 - val_loss : 0 . 9441 - val_acc : 0 . 7460 Epoch 10 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 5295 - acc : 0 . 9201 - val_loss : 0 . 8892 - val_acc : 0 . 7679 Epoch 11 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 5100 - acc : 0 . 9309 - val_loss : 0 . 8808 - val_acc : 0 . 7818 Epoch 12 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4926 - acc : 0 . 9397 - val_loss : 0 . 8404 - val_acc : 0 . 7894 Epoch 13 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4807 - acc : 0 . 9470 - val_loss : 0 . 8600 - val_acc : 0 . 7928 Epoch 14 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4661 - acc : 0 . 9529 - val_loss : 0 . 9046 - val_acc : 0 . 7732 Epoch 15 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 4588 - acc : 0 . 9571 - val_loss : 0 . 8505 - val_acc : 0 . 7965 Epoch 16 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4558 - acc : 0 . 9593 - val_loss : 0 . 8748 - val_acc : 0 . 7859 Epoch 17 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4434 - acc : 0 . 9649 - val_loss : 0 . 9109 - val_acc : 0 . 7656 Epoch 18 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4449 - acc : 0 . 9643 - val_loss : 0 . 8532 - val_acc : 0 . 7971 Epoch 19 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4349 - acc : 0 . 9701 - val_loss : 0 . 8677 - val_acc : 0 . 7951 Epoch 20 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4351 - acc : 0 . 9698 - val_loss : 0 . 9145 - val_acc : 0 . 7740 Epoch 21 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4268 - acc : 0 . 9740 - val_loss : 0 . 8308 - val_acc : 0 . 8065 Epoch 22 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4243 - acc : 0 . 9741 - val_loss : 0 . 8229 - val_acc : 0 . 8075 Epoch 23 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4201 - acc : 0 . 9764 - val_loss : 0 . 8411 - val_acc : 0 . 8062 Epoch 24 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4190 - acc : 0 . 9769 - val_loss : 0 . 8649 - val_acc : 0 . 7951 Epoch 25 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4139 - acc : 0 . 9787 - val_loss : 0 . 8257 - val_acc : 0 . 8071 Epoch 26 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4154 - acc : 0 . 9779 - val_loss : 0 . 8041 - val_acc : 0 . 8205 Epoch 27 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4128 - acc : 0 . 9798 - val_loss : 0 . 8296 - val_acc : 0 . 8115 Epoch 28 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4121 - acc : 0 . 9798 - val_loss : 0 . 8241 - val_acc : 0 . 8074 Epoch 29 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4093 - acc : 0 . 9807 - val_loss : 0 . 8575 - val_acc : 0 . 7913 Epoch 30 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4048 - acc : 0 . 9826 - val_loss : 0 . 8118 - val_acc : 0 . 8166 Epoch 31 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4041 - acc : 0 . 9837 - val_loss : 0 . 8375 - val_acc : 0 . 8082 Epoch 32 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4045 - acc : 0 . 9831 - val_loss : 0 . 8604 - val_acc : 0 . 8091 Epoch 33 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4047 - acc : 0 . 9823 - val_loss : 0 . 8797 - val_acc : 0 . 7931 Epoch 34 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4023 - acc : 0 . 9842 - val_loss : 0 . 8694 - val_acc : 0 . 8020 Epoch 35 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3995 - acc : 0 . 9858 - val_loss : 0 . 8161 - val_acc : 0 . 8186 Epoch 36 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3976 - acc : 0 . 9859 - val_loss : 0 . 8495 - val_acc : 0 . 7988 Epoch 37 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4021 - acc : 0 . 9847 - val_loss : 0 . 8542 - val_acc : 0 . 8062 Epoch 38 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3939 - acc : 0 . 9869 - val_loss : 0 . 8347 - val_acc : 0 . 8122 Epoch 39 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3955 - acc : 0 . 9856 - val_loss : 0 . 8521 - val_acc : 0 . 7993 Epoch 40 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3907 - acc : 0 . 9885 - val_loss : 0 . 9023 - val_acc : 0 . 7992 Epoch 41 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3911 - acc : 0 . 9873 - val_loss : 0 . 8597 - val_acc : 0 . 8010 Epoch 42 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3917 - acc : 0 . 9885 - val_loss : 0 . 8968 - val_acc : 0 . 7936 Epoch 43 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3931 - acc : 0 . 9874 - val_loss : 0 . 8318 - val_acc : 0 . 8169 Epoch 44 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3897 - acc : 0 . 9893 - val_loss : 0 . 8811 - val_acc : 0 . 7988 Epoch 45 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9888 - val_loss : 0 . 8453 - val_acc : 0 . 8094 Epoch 46 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9889 - val_loss : 0 . 8195 - val_acc : 0 . 8179 Epoch 47 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3891 - acc : 0 . 9890 - val_loss : 0 . 8373 - val_acc : 0 . 8137 Epoch 48 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3902 - acc : 0 . 9888 - val_loss : 0 . 8457 - val_acc : 0 . 8120 Epoch 49 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3864 - acc : 0 . 9903 - val_loss : 0 . 9012 - val_acc : 0 . 7907 Epoch 50 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3859 - acc : 0 . 9903 - val_loss : 0 . 8291 - val_acc : 0 . 8053 Epoch 51 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3830 - acc : 0 . 9915 - val_loss : 0 . 8494 - val_acc : 0 . 8139 Epoch 52 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3828 - acc : 0 . 9907 - val_loss : 0 . 8447 - val_acc : 0 . 8135 Epoch 53 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9910 - val_loss : 0 . 8539 - val_acc : 0 . 8120 Epoch 54 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3832 - acc : 0 . 9905 - val_loss : 0 . 8592 - val_acc : 0 . 8098 Epoch 55 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9908 - val_loss : 0 . 8585 - val_acc : 0 . 8087 Epoch 56 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3817 - acc : 0 . 9911 - val_loss : 0 . 8840 - val_acc : 0 . 7889 Epoch 57 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3827 - acc : 0 . 9914 - val_loss : 0 . 8205 - val_acc : 0 . 8250 Epoch 58 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3818 - acc : 0 . 9912 - val_loss : 0 . 8571 - val_acc : 0 . 8051 Epoch 59 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3811 - acc : 0 . 9919 - val_loss : 0 . 8155 - val_acc : 0 . 8254 Epoch 60 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3803 - acc : 0 . 9919 - val_loss : 0 . 8617 - val_acc : 0 . 8040 Epoch 61 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3793 - acc : 0 . 9926 - val_loss : 0 . 8212 - val_acc : 0 . 8192 Epoch 62 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3825 - acc : 0 . 9912 - val_loss : 0 . 8139 - val_acc : 0 . 8277 Epoch 63 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9923 - val_loss : 0 . 8304 - val_acc : 0 . 8121 Epoch 64 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3809 - acc : 0 . 9918 - val_loss : 0 . 7961 - val_acc : 0 . 8289 Epoch 65 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3750 - acc : 0 . 9930 - val_loss : 0 . 8676 - val_acc : 0 . 8110 Epoch 66 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3789 - acc : 0 . 9928 - val_loss : 0 . 8308 - val_acc : 0 . 8148 Epoch 67 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3783 - acc : 0 . 9929 - val_loss : 0 . 8595 - val_acc : 0 . 8097 Epoch 68 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9935 - val_loss : 0 . 8359 - val_acc : 0 . 8065 Epoch 69 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9927 - val_loss : 0 . 8189 - val_acc : 0 . 8255 Epoch 70 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3786 - acc : 0 . 9924 - val_loss : 0 . 8754 - val_acc : 0 . 8001 Epoch 71 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9936 - val_loss : 0 . 8188 - val_acc : 0 . 8262 Epoch 72 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9932 - val_loss : 0 . 8540 - val_acc : 0 . 8169 Epoch 73 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3740 - acc : 0 . 9934 - val_loss : 0 . 8127 - val_acc : 0 . 8258 Epoch 74 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9932 - val_loss : 0 . 8662 - val_acc : 0 . 8018 Epoch 75 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3721 - acc : 0 . 9941 - val_loss : 0 . 8359 - val_acc : 0 . 8213 Epoch 76 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3746 - acc : 0 . 9937 - val_loss : 0 . 8462 - val_acc : 0 . 8178 Epoch 77 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3741 - acc : 0 . 9936 - val_loss : 0 . 8983 - val_acc : 0 . 7972 Epoch 78 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3751 - acc : 0 . 9933 - val_loss : 0 . 8525 - val_acc : 0 . 8173 Epoch 79 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3762 - acc : 0 . 9931 - val_loss : 0 . 8190 - val_acc : 0 . 8201 Epoch 80 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3737 - acc : 0 . 9940 - val_loss : 0 . 8441 - val_acc : 0 . 8196 Epoch 81 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3729 - acc : 0 . 9935 - val_loss : 0 . 8151 - val_acc : 0 . 8267 Epoch 82 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3735 - acc : 0 . 9938 - val_loss : 0 . 8405 - val_acc : 0 . 8163 Epoch 83 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3723 - acc : 0 . 9939 - val_loss : 0 . 8225 - val_acc : 0 . 8243 Epoch 84 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3738 - acc : 0 . 9938 - val_loss : 0 . 8413 - val_acc : 0 . 8115 Epoch 85 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3714 - acc : 0 . 9947 - val_loss : 0 . 9080 - val_acc : 0 . 7932 Epoch 86 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3744 - acc : 0 . 9942 - val_loss : 0 . 8467 - val_acc : 0 . 8135 Epoch 87 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3705 - acc : 0 . 9948 - val_loss : 0 . 8491 - val_acc : 0 . 8163 Epoch 88 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3733 - acc : 0 . 9944 - val_loss : 0 . 8005 - val_acc : 0 . 8214 Epoch 89 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3693 - acc : 0 . 9949 - val_loss : 0 . 7791 - val_acc : 0 . 8321 Epoch 90 / 100 50000 / 50000 [ ============================== ] - 135 s 3 ms / step - loss : 0 . 3724 - acc : 0 . 9942 - val_loss : 0 . 8458 - val_acc : 0 . 8124 Epoch 91 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3732 - acc : 0 . 9947 - val_loss : 0 . 8315 - val_acc : 0 . 8164 Epoch 92 / 100 50000 / 50000 [ ============================== ] - 127 s 3 ms / step - loss : 0 . 3699 - acc : 0 . 9950 - val_loss : 0 . 8140 - val_acc : 0 . 8226 Epoch 93 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 0 . 3694 - acc : 0 . 9950 - val_loss : 0 . 8342 - val_acc : 0 . 8210 Epoch 94 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8938 - val_acc : 0 . 8019 Epoch 95 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8771 - val_acc : 0 . 8066 Epoch 96 / 100 50000 / 50000 [ ============================== ] - 164 s 3 ms / step - loss : 0 . 3712 - acc : 0 . 9946 - val_loss : 0 . 8396 - val_acc : 0 . 8211 Epoch 97 / 100 50000 / 50000 [ ============================== ] - 155 s 3 ms / step - loss : 0 . 3689 - acc : 0 . 9949 - val_loss : 0 . 8728 - val_acc : 0 . 8112 Epoch 98 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3663 - acc : 0 . 9953 - val_loss : 0 . 9615 - val_acc : 0 . 7902 Epoch 99 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3714 - acc : 0 . 9944 - val_loss : 0 . 8414 - val_acc : 0 . 8188 Epoch 100 / 100 50000 / 50000 [ ============================== ] - 138 s 3 ms / step - loss : 0 . 3682 - acc : 0 . 9956 - val_loss : 0 . 8055 - val_acc : 0 . 8266","title":"Model Training"},{"location":"examples/binarynet_cifar10/#model-output","text":"We can now plot the final validation accuracy and loss: plt . plot ( trained_model . history [ 'acc' ]) plt . plot ( trained_model . history [ 'val_acc' ]) plt . title ( 'model accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . max ( trained_model . history [ 'acc' ])) print ( np . max ( trained_model . history [ 'val_acc' ])) 0 . 9956000019311905 0 . 8320999944210052 plt . plot ( trained_model . history [ 'loss' ]) plt . plot ( trained_model . history [ 'val_loss' ]) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . min ( trained_model . history [ 'loss' ])) print ( np . min ( trained_model . history [ 'val_loss' ])) 0 . 3663262344896793 0 . 7790719392895699","title":"Model Output"},{"location":"examples/mnist/","text":"Introduction to BNNs with Larq \u00b6 Run on Binder View on GitHub This tutorial demonstrates how to train a simple binarized Convolutional Neural Network (CNN) to classify MNIST digits. This simple network will achieve approximately 98% accuracy on the MNIST test set. This tutorial uses Larq and the Keras Sequential API , so creating and training our model will require only a few lines of code. import tensorflow as tf import larq as lq Download and prepare the MNIST dataset \u00b6 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . mnist . load_data () train_images = train_images . reshape (( 60000 , 28 , 28 , 1 )) test_images = test_images . reshape (( 10000 , 28 , 28 , 1 )) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1 Create the model \u00b6 The following will create a simple binarized CNN. The quantization function $$ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} $$ is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning. To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE) (the binarization is essentially replaced by a clipped identity on the backward pass): $$ \\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} $$ In Larq this can be done by using input_quantizer=\"ste_sign\" and kernel_quantizer=\"ste_sign\" . Additionally, the latent full precision weights are clipped to -1 and 1 using kernel_constraint=\"weight_clip\" . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) model = tf . keras . models . Sequential () # In the first layer we only quantize the weights and not the input model . add ( lq . layers . QuantConv2D ( 32 , ( 3 , 3 ), kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 28 , 28 , 1 ))) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Flatten ()) model . add ( lq . layers . QuantDense ( 64 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantDense ( 10 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Activation ( \"softmax\" )) Almost all parameters in the network are binarized, so either -1 or 1. This makes the network extremely fast if it would be deployed on custom BNN hardware. Here is the complete architecture of our model: lq . models . summary ( model ) + sequential_1 stats ---------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + ---------------------------------------------------------------------------------+ | quant_conv2d_3 - ( - 1 , 26 , 26 , 32 ) 288 0 0 . 04 | | max_pooling2d_2 - ( - 1 , 13 , 13 , 32 ) 0 0 0 . 00 | | batch_normalization_5 - ( - 1 , 13 , 13 , 32 ) 0 96 0 . 38 | | quant_conv2d_4 1 ( - 1 , 11 , 11 , 64 ) 18432 0 2 . 25 | | max_pooling2d_3 - ( - 1 , 5 , 5 , 64 ) 0 0 0 . 00 | | batch_normalization_6 - ( - 1 , 5 , 5 , 64 ) 0 192 0 . 75 | | quant_conv2d_5 1 ( - 1 , 3 , 3 , 64 ) 36864 0 4 . 50 | | batch_normalization_7 - ( - 1 , 3 , 3 , 64 ) 0 192 0 . 75 | | flatten_1 - ( - 1 , 576 ) 0 0 0 . 00 | | quant_dense_2 1 ( - 1 , 64 ) 36864 0 4 . 50 | | batch_normalization_8 - ( - 1 , 64 ) 0 192 0 . 75 | | quant_dense_3 1 ( - 1 , 10 ) 640 0 0 . 08 | | batch_normalization_9 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + ---------------------------------------------------------------------------------+ | Total 93088 702 14 . 11 | + ---------------------------------------------------------------------------------+ + sequential_1 summary ------------+ | Total params 93790 | | Trainable params 93322 | | Non - trainable params 468 | | Float - 32 Equivalent 0 . 36 MB | | Compression of Memory 25 . 97 | + --------------------------------+ Compile and train the model \u00b6 Note: This may take a few minutes depending on your system. model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( train_images , train_labels , batch_size = 64 , epochs = 6 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) Epoch 1 / 6 60000 / 60000 [ ============================== ] - 72 s 1 ms / sample - loss : 0 . 6494 - acc : 0 . 9070 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 2 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4760 - acc : 0 . 9606 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 3 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4480 - acc : 0 . 9691 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 4 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4365 - acc : 0 . 9718 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 5 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4329 - acc : 0 . 9739 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 6 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4287 - acc : 0 . 9758 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10000 / 10000 [ ============================== ] - 6 s 576 us / sample - loss : 0 . 4283 - acc : 0 . 9751 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Evaluate the model \u00b6 print ( f \"Test accuracy {test_acc * 100:.2f} %\" ) Test accuracy 97 . 51 % As you can see, our simple binarized CNN has achieved a test accuracy of over 97.5 %. Not bad for a few lines of code!","title":"Introduction to BNNs with Larq"},{"location":"examples/mnist/#introduction-to-bnns-with-larq","text":"Run on Binder View on GitHub This tutorial demonstrates how to train a simple binarized Convolutional Neural Network (CNN) to classify MNIST digits. This simple network will achieve approximately 98% accuracy on the MNIST test set. This tutorial uses Larq and the Keras Sequential API , so creating and training our model will require only a few lines of code. import tensorflow as tf import larq as lq","title":"Introduction to BNNs with Larq"},{"location":"examples/mnist/#download-and-prepare-the-mnist-dataset","text":"( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . mnist . load_data () train_images = train_images . reshape (( 60000 , 28 , 28 , 1 )) test_images = test_images . reshape (( 10000 , 28 , 28 , 1 )) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1","title":"Download and prepare the MNIST dataset"},{"location":"examples/mnist/#create-the-model","text":"The following will create a simple binarized CNN. The quantization function $$ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} $$ is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning. To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE) (the binarization is essentially replaced by a clipped identity on the backward pass): $$ \\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} $$ In Larq this can be done by using input_quantizer=\"ste_sign\" and kernel_quantizer=\"ste_sign\" . Additionally, the latent full precision weights are clipped to -1 and 1 using kernel_constraint=\"weight_clip\" . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) model = tf . keras . models . Sequential () # In the first layer we only quantize the weights and not the input model . add ( lq . layers . QuantConv2D ( 32 , ( 3 , 3 ), kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 28 , 28 , 1 ))) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Flatten ()) model . add ( lq . layers . QuantDense ( 64 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantDense ( 10 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Activation ( \"softmax\" )) Almost all parameters in the network are binarized, so either -1 or 1. This makes the network extremely fast if it would be deployed on custom BNN hardware. Here is the complete architecture of our model: lq . models . summary ( model ) + sequential_1 stats ---------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + ---------------------------------------------------------------------------------+ | quant_conv2d_3 - ( - 1 , 26 , 26 , 32 ) 288 0 0 . 04 | | max_pooling2d_2 - ( - 1 , 13 , 13 , 32 ) 0 0 0 . 00 | | batch_normalization_5 - ( - 1 , 13 , 13 , 32 ) 0 96 0 . 38 | | quant_conv2d_4 1 ( - 1 , 11 , 11 , 64 ) 18432 0 2 . 25 | | max_pooling2d_3 - ( - 1 , 5 , 5 , 64 ) 0 0 0 . 00 | | batch_normalization_6 - ( - 1 , 5 , 5 , 64 ) 0 192 0 . 75 | | quant_conv2d_5 1 ( - 1 , 3 , 3 , 64 ) 36864 0 4 . 50 | | batch_normalization_7 - ( - 1 , 3 , 3 , 64 ) 0 192 0 . 75 | | flatten_1 - ( - 1 , 576 ) 0 0 0 . 00 | | quant_dense_2 1 ( - 1 , 64 ) 36864 0 4 . 50 | | batch_normalization_8 - ( - 1 , 64 ) 0 192 0 . 75 | | quant_dense_3 1 ( - 1 , 10 ) 640 0 0 . 08 | | batch_normalization_9 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + ---------------------------------------------------------------------------------+ | Total 93088 702 14 . 11 | + ---------------------------------------------------------------------------------+ + sequential_1 summary ------------+ | Total params 93790 | | Trainable params 93322 | | Non - trainable params 468 | | Float - 32 Equivalent 0 . 36 MB | | Compression of Memory 25 . 97 | + --------------------------------+","title":"Create the model"},{"location":"examples/mnist/#compile-and-train-the-model","text":"Note: This may take a few minutes depending on your system. model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( train_images , train_labels , batch_size = 64 , epochs = 6 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) Epoch 1 / 6 60000 / 60000 [ ============================== ] - 72 s 1 ms / sample - loss : 0 . 6494 - acc : 0 . 9070 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 2 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4760 - acc : 0 . 9606 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 3 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4480 - acc : 0 . 9691 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 4 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4365 - acc : 0 . 9718 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 5 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4329 - acc : 0 . 9739 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 6 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4287 - acc : 0 . 9758 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10000 / 10000 [ ============================== ] - 6 s 576 us / sample - loss : 0 . 4283 - acc : 0 . 9751 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b","title":"Compile and train the model"},{"location":"examples/mnist/#evaluate-the-model","text":"print ( f \"Test accuracy {test_acc * 100:.2f} %\" ) Test accuracy 97 . 51 % As you can see, our simple binarized CNN has achieved a test accuracy of over 97.5 %. Not bad for a few lines of code!","title":"Evaluate the model"},{"location":"guides/bnn-architecture/","text":"Here you will find a quick overview of the best practices that have evolved in the BNN community over the past few years regarding BNN architecture. After following this guide, you should be able to start designing a BNN for your application of interest that is both efficient and powerful. Binarizing a Single Layer \u00b6 Any layer has two types of inputs: the layer parameters, such as a weight matrix and biases, and incoming activations. We can reduce the memory footprint of the model by binarizing parameters. In Larq, this can be done by passing a kernel_quantizer from larq.quantizers when instantiating a larq.layer object, or by using a custom BNN optimizer such as Bop (note that by default Bop only targets kernels of layers in larq.layers ). To get the efficiency of binary computations, the incoming activations need to be binary as well. This can be done by setting a input_quantizer . Note that the output of a binarized layer is not binary. Instead the output is integer, due to the summation that appears in most neural network layers. When viewing binarization as an activation function just like ReLU, one may be inclined to binarize the outgoing activations rather than the incoming activations. However, if the network contains batch normalization layers or residual connections, this may result in unintentional non-binary operations. Therefore we have opted for an input_quantizer rather than an activation_quantizer . A typical binarized layer looks something like: import larq as lq ... x_out = lq . layers . QuantDense ( units = 1024 , input_quantizer = lq . quantizers . ste_sign , kernel_quantizer = lq . quantizers . ste_sign , kernel_constraint = lq . constraints . weight_clip , )( x_in ) First & Last Layer \u00b6 Binarizing the first and last layers hurts accuracy much more than binarizing other layers in the network. Meanwhile, the number of weights and operations in these layers are relatively small. Therefore it has become standard to leave these layers in higher precision. This applies to the incoming activations as well as the weights. The following shows a network that was trained on CIFAR10 with different precisions for first and last layers. // embed when document is loaded, to ensure vega library is available // this works on all modern browsers, except IE8 and older document.addEventListener(\"DOMContentLoaded\", function(event) { var opt = { \"mode\": \"vega-lite\", \"renderer\": \"canvas\", \"actions\": false, }; vegaEmbed('#altair-plot-e78d7bf9-feff-4d79-b3dd-fc7f7465f25c', '/plots/first_and_last_layers.vg.json', opt).catch(console.err); }, {passive: true, once: true}); Batch Normalization \u00b6 Perhaps somewhat surprisingly, batch normalization remains crucial in BNNs. All successful BNNs still contain a batch norm layer after each binarized layer. We recommend using batch normalization with trainable beta and gamma. Note that when no residuals are used, the batch norm operation can be simplified (see e.g. Fig. 2 in this paper ). High-Precision Shortcuts \u00b6 A binarized layer outputs an integer activation matrix that is binarized before the next layer. This means that in a VGG-style network such as BinaryNet information is lost between every two layers, and one may wonder if this is optimal in terms of efficiency. High-precision shortcuts avoid this loss of information. Examples of networks that include such shortcuts are Bi-Real net and Binary Densenets . Note that the argument for introducing these shortcuts is no longer just to improve gradient flow, as it is in real-valued models: in BNNs, high-precision shortcuts really improve the expressivity of the model. Such shortcuts are relatively cheap in terms of memory footprint and computational cost, and they greatly improve accuracy. Beware that they do increase the runtime memory requirements of the model. An issue with ResNet-style shortcuts comes up when there is a dimensionality change. Currently, the most popular solution to this is to use pointwise high-precision convolutions in the residual connections if there is a dimensionality change. We recommend to use as many shortcuts as you can: for example, in ResNet-style architectures it helps to bypass every single convolutional layer, instead of every two convolutional layers. An example of a convolutional block with shortcut in Larq could look something like this: import larq as lq ... def conv_with_shortcut ( x ): \"\"\"Convolutional block with shortcut connection. Args: x: input tensor with high-precision activations Returns: Tensor with high-precision activations \"\"\" # get number of filters filters = x . get_shape () . as_list ()[ - 1 ] # create shortcut that retains the high-precision information shortcut = x # efficient binarized convolutions (note inputs are also binarized) x = lq . layers . QuantConv2D ( filters = filters , kernel_size = 3 , padding = \"same\" , input_quantizer = lq . quantizers . ste_sign , kernel_quantizer = lq . quantizers . ste_sign , kernel_initializer = \"glorot_normal\" , kernel_constraint = lq . constraints . weight_clip , use_bias = False , )( x ) # normalize the (integer) output of the binary conv and merge # with shortcut x = tf . keras . layers . BatchNormalization ( momentum = 0.9 )( x ) out = tf . keras . layers . add ([ x , shortcut ]) return out Pooling \u00b6 The XNOR-net authors found that accuracy improves when applying batch normalization after instead of before max-pooling. In general, max pooling in BNNs can be problematic as it can lead to skewed binarized activations. Thus, in a VGG-style network a layer could look like this: import larq as lq ... x = lq . layers . QuantConv2D ( filters = 512 , kernel_size = 3 , padding = \"same\" , input_quantizer = lq . quantizers . ste_sign , kernel_quantizer = lq . quantizers . ste_sign , kernel_constraint = lq . constraints . weight_clip , use_bias = False , )( x ) x = tf . keras . layers . MaxPool2D ( pool_size = 3 , strides = 2 )( x ) x = tf . keras . layers . BatchNormalization ( momentum =. 9 )( x ) Further References \u00b6 If you would like to learn more, we recommend checking out the following papers (starting at the most recent): Back to Simplicity: How to Train Accurate BNNs from Scratch? - This recent paper introduces Binary Densenets, demonstrating good results on ImageNet. The authors take an information-theoretic perspective on BNN architectures and give a number of recommendations for good architecture design. Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation - A thought-provoking paper that presents Group-Net. The authors question whether architectural features developed for real-valued networks are the most appropriate for BNN. Even more than the presented architecture, we find this line of thinking very interesting and hope Larq will enable people to explore novel ideas more easily. Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm - This ECCV 2018 paper introduces Bi-Real nets, one of the first binarized networks that uses high-precision shortcuts. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 - The classic BNN paper, mandatory reading for anyone working in the field. In addition to introducing many of the foundational ideas for BNNs, the paper contains an interesting discussion on batch normalization.","title":"Building BNNs"},{"location":"guides/bnn-architecture/#binarizing-a-single-layer","text":"Any layer has two types of inputs: the layer parameters, such as a weight matrix and biases, and incoming activations. We can reduce the memory footprint of the model by binarizing parameters. In Larq, this can be done by passing a kernel_quantizer from larq.quantizers when instantiating a larq.layer object, or by using a custom BNN optimizer such as Bop (note that by default Bop only targets kernels of layers in larq.layers ). To get the efficiency of binary computations, the incoming activations need to be binary as well. This can be done by setting a input_quantizer . Note that the output of a binarized layer is not binary. Instead the output is integer, due to the summation that appears in most neural network layers. When viewing binarization as an activation function just like ReLU, one may be inclined to binarize the outgoing activations rather than the incoming activations. However, if the network contains batch normalization layers or residual connections, this may result in unintentional non-binary operations. Therefore we have opted for an input_quantizer rather than an activation_quantizer . A typical binarized layer looks something like: import larq as lq ... x_out = lq . layers . QuantDense ( units = 1024 , input_quantizer = lq . quantizers . ste_sign , kernel_quantizer = lq . quantizers . ste_sign , kernel_constraint = lq . constraints . weight_clip , )( x_in )","title":"Binarizing a Single Layer"},{"location":"guides/bnn-architecture/#first-last-layer","text":"Binarizing the first and last layers hurts accuracy much more than binarizing other layers in the network. Meanwhile, the number of weights and operations in these layers are relatively small. Therefore it has become standard to leave these layers in higher precision. This applies to the incoming activations as well as the weights. The following shows a network that was trained on CIFAR10 with different precisions for first and last layers. // embed when document is loaded, to ensure vega library is available // this works on all modern browsers, except IE8 and older document.addEventListener(\"DOMContentLoaded\", function(event) { var opt = { \"mode\": \"vega-lite\", \"renderer\": \"canvas\", \"actions\": false, }; vegaEmbed('#altair-plot-e78d7bf9-feff-4d79-b3dd-fc7f7465f25c', '/plots/first_and_last_layers.vg.json', opt).catch(console.err); }, {passive: true, once: true});","title":"First &amp; Last Layer"},{"location":"guides/bnn-architecture/#batch-normalization","text":"Perhaps somewhat surprisingly, batch normalization remains crucial in BNNs. All successful BNNs still contain a batch norm layer after each binarized layer. We recommend using batch normalization with trainable beta and gamma. Note that when no residuals are used, the batch norm operation can be simplified (see e.g. Fig. 2 in this paper ).","title":"Batch Normalization"},{"location":"guides/bnn-architecture/#high-precision-shortcuts","text":"A binarized layer outputs an integer activation matrix that is binarized before the next layer. This means that in a VGG-style network such as BinaryNet information is lost between every two layers, and one may wonder if this is optimal in terms of efficiency. High-precision shortcuts avoid this loss of information. Examples of networks that include such shortcuts are Bi-Real net and Binary Densenets . Note that the argument for introducing these shortcuts is no longer just to improve gradient flow, as it is in real-valued models: in BNNs, high-precision shortcuts really improve the expressivity of the model. Such shortcuts are relatively cheap in terms of memory footprint and computational cost, and they greatly improve accuracy. Beware that they do increase the runtime memory requirements of the model. An issue with ResNet-style shortcuts comes up when there is a dimensionality change. Currently, the most popular solution to this is to use pointwise high-precision convolutions in the residual connections if there is a dimensionality change. We recommend to use as many shortcuts as you can: for example, in ResNet-style architectures it helps to bypass every single convolutional layer, instead of every two convolutional layers. An example of a convolutional block with shortcut in Larq could look something like this: import larq as lq ... def conv_with_shortcut ( x ): \"\"\"Convolutional block with shortcut connection. Args: x: input tensor with high-precision activations Returns: Tensor with high-precision activations \"\"\" # get number of filters filters = x . get_shape () . as_list ()[ - 1 ] # create shortcut that retains the high-precision information shortcut = x # efficient binarized convolutions (note inputs are also binarized) x = lq . layers . QuantConv2D ( filters = filters , kernel_size = 3 , padding = \"same\" , input_quantizer = lq . quantizers . ste_sign , kernel_quantizer = lq . quantizers . ste_sign , kernel_initializer = \"glorot_normal\" , kernel_constraint = lq . constraints . weight_clip , use_bias = False , )( x ) # normalize the (integer) output of the binary conv and merge # with shortcut x = tf . keras . layers . BatchNormalization ( momentum = 0.9 )( x ) out = tf . keras . layers . add ([ x , shortcut ]) return out","title":"High-Precision Shortcuts"},{"location":"guides/bnn-architecture/#pooling","text":"The XNOR-net authors found that accuracy improves when applying batch normalization after instead of before max-pooling. In general, max pooling in BNNs can be problematic as it can lead to skewed binarized activations. Thus, in a VGG-style network a layer could look like this: import larq as lq ... x = lq . layers . QuantConv2D ( filters = 512 , kernel_size = 3 , padding = \"same\" , input_quantizer = lq . quantizers . ste_sign , kernel_quantizer = lq . quantizers . ste_sign , kernel_constraint = lq . constraints . weight_clip , use_bias = False , )( x ) x = tf . keras . layers . MaxPool2D ( pool_size = 3 , strides = 2 )( x ) x = tf . keras . layers . BatchNormalization ( momentum =. 9 )( x )","title":"Pooling"},{"location":"guides/bnn-architecture/#further-references","text":"If you would like to learn more, we recommend checking out the following papers (starting at the most recent): Back to Simplicity: How to Train Accurate BNNs from Scratch? - This recent paper introduces Binary Densenets, demonstrating good results on ImageNet. The authors take an information-theoretic perspective on BNN architectures and give a number of recommendations for good architecture design. Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation - A thought-provoking paper that presents Group-Net. The authors question whether architectural features developed for real-valued networks are the most appropriate for BNN. Even more than the presented architecture, we find this line of thinking very interesting and hope Larq will enable people to explore novel ideas more easily. Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm - This ECCV 2018 paper introduces Bi-Real nets, one of the first binarized networks that uses high-precision shortcuts. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 - The classic BNN paper, mandatory reading for anyone working in the field. In addition to introducing many of the foundational ideas for BNNs, the paper contains an interesting discussion on batch normalization.","title":"Further References"},{"location":"guides/bnn-optimization/","text":"Once you have defined a good architecture for your BNN, you want to train it. Here we give an introduction to common training strategies and tricks that are popular in the field. After reading this guide you should have a good idea of how you can train your BNN. Below we first discuss the fundamental challenges of BNN optimization. We then explain the most commonly used training strategy, of using latent weights, and cover the questions, tips & tricks that tend to come up when training BNNs. The Problem with SGD in BNNs \u00b6 Stochastic Gradient Descent (SGD) is used pretty much everywhere in the field Deep Learning nowadays - either in its vanilla form or as the core part of some more sophisticated algorithm like Adam . However, when turning to BNNs, two fundamental issues arise with SGD: The gradient of the binarization operation is zero almost everywhere, making the gradient \\frac{\\partial L}{\\partial w} \\frac{\\partial L}{\\partial w} utterly uninformative. SGD performs optimization through small update steps that are accumulated over time. Binary weights, meanwhile, cannot absorb small updates: they can only be left alone, or flipped. Another way of putting this is that the loss landscape for BNN is very different than what you are used to for real-valued networks. Gone are the glowing hills you can simply glide down from: the loss is now a discrete function, and many of the intuitions and theories developed for continuous loss landscapes no longer apply. Luckily, there has been significant progress in solving these problems. The issue of zero gradients is resolved by replacing the gradient by some more informative alternative, what we call a 'pseudo-gradient'. The issue of updating can be resolved either by introducing latent weights, or by opting for a custom BNN optimizer. Latent Weights \u00b6 Suppose we take a batch of training samples and evaluate a forward and backward pass. During the backward pass we replace the gradients with a pseudo-gradient, and we get a gradient vector on our weights. We then feed this into an optimizer, and get a vector with updates for our weights. At this point, what do we do? If we directly apply the updates to our weights, they are no longer binary. The standard solution to this problem has been to introduce real-valued latent weights . We apply our update step to this real-valued weight. During the forward pass, we use the binarized version of the latent weight. Beware that latent weights are not really weights at all - after all, changing the latent weights usually doesn't affect the behavior of the network and we throw the latent weights away after we're done training. Instead, they are best thought of as a product between the weight and a positive inertia: the higher this inertia, the stronger the signal required to make the weight flip. One implication of this is that the latent weights should be constrained: as an increase in inertia does not change the behavior of the network, it can otherwise grow indefinitely. In Larq, it is trivial to implement this strategy. An example of a layer optimized with this method would look like: x_out = larq . layers . QuantDense ( 512 , input_qunatizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" )( x_out ) Any optimizer you now apply will update the latent weights; after the update the latent weights are clipped to [-1, 1] [-1, 1] . Alternative: Custom Optimizers \u00b6 Instead of using latent weights, one can opt for a custom BNN optimizer that inherently generates binary weights. An example of such an optimizer is Bop . Choice of Pseudo-Gradient \u00b6 In larq.quantizers you will find a variety of quantizers that have been introduced in different papers. Many of these quantizers behave identically during the forward pass but implement different pseudo-gradients. Studies comparing different pseudo-gradients report little difference between them. Therefore, we recommend using the classical ste_sign() as default. Choice of Optimizer \u00b6 When using a latent weight strategy, you can apply any optimizer you are familiar with from real-valued deep learning. However, due the different nature of BNNs your intuitions may be off. We recommend using Adam: although other optimizers can achieve similar accuracies with a lot of finetuning, we and others have found that Adam is the quickest to converge and the least sensitive to the choice of hyperparamters . Tips & Tricks \u00b6 Here are some general tips and tricks that you may want to keep in mind: BNN training is more noisy due to non-continuous nature of flipping weights; therefore, we recommend setting your batch norm momentum to 0.9. Beware that BNNs tend to require many more epochs than real-valued networks to converge: 200+ epochs when training an AlexNet or ResNet-18 style network on ImageNet is not unusual. Networks tend to train much quicker if they are initialized from a trained real-valued model. Importantly, this requires the overall architecture of the pretrained network to be as similar as possible to the BNN, including placement of the activation operation (which replaces the binarization operation). Note that although convergence is faster, pretraining does not seem to improve final accuracy. Further References \u00b6 If you would like to learn more, we recommend checking out the following papers (starting at the most recent): Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization - This paper investigates optimization of BNNs using latent weights and introduces Bop as the first custom BNN optimizer. An Empirical study of Binary Neural Networks' Optimisation - An empirical comparison of BNN optimization methods, including a detailed discussion on the use of various optimizers and a number of tricks used in the literature.","title":"Training BNNs"},{"location":"guides/bnn-optimization/#the-problem-with-sgd-in-bnns","text":"Stochastic Gradient Descent (SGD) is used pretty much everywhere in the field Deep Learning nowadays - either in its vanilla form or as the core part of some more sophisticated algorithm like Adam . However, when turning to BNNs, two fundamental issues arise with SGD: The gradient of the binarization operation is zero almost everywhere, making the gradient \\frac{\\partial L}{\\partial w} \\frac{\\partial L}{\\partial w} utterly uninformative. SGD performs optimization through small update steps that are accumulated over time. Binary weights, meanwhile, cannot absorb small updates: they can only be left alone, or flipped. Another way of putting this is that the loss landscape for BNN is very different than what you are used to for real-valued networks. Gone are the glowing hills you can simply glide down from: the loss is now a discrete function, and many of the intuitions and theories developed for continuous loss landscapes no longer apply. Luckily, there has been significant progress in solving these problems. The issue of zero gradients is resolved by replacing the gradient by some more informative alternative, what we call a 'pseudo-gradient'. The issue of updating can be resolved either by introducing latent weights, or by opting for a custom BNN optimizer.","title":"The Problem with SGD in BNNs"},{"location":"guides/bnn-optimization/#latent-weights","text":"Suppose we take a batch of training samples and evaluate a forward and backward pass. During the backward pass we replace the gradients with a pseudo-gradient, and we get a gradient vector on our weights. We then feed this into an optimizer, and get a vector with updates for our weights. At this point, what do we do? If we directly apply the updates to our weights, they are no longer binary. The standard solution to this problem has been to introduce real-valued latent weights . We apply our update step to this real-valued weight. During the forward pass, we use the binarized version of the latent weight. Beware that latent weights are not really weights at all - after all, changing the latent weights usually doesn't affect the behavior of the network and we throw the latent weights away after we're done training. Instead, they are best thought of as a product between the weight and a positive inertia: the higher this inertia, the stronger the signal required to make the weight flip. One implication of this is that the latent weights should be constrained: as an increase in inertia does not change the behavior of the network, it can otherwise grow indefinitely. In Larq, it is trivial to implement this strategy. An example of a layer optimized with this method would look like: x_out = larq . layers . QuantDense ( 512 , input_qunatizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" )( x_out ) Any optimizer you now apply will update the latent weights; after the update the latent weights are clipped to [-1, 1] [-1, 1] .","title":"Latent Weights"},{"location":"guides/bnn-optimization/#alternative-custom-optimizers","text":"Instead of using latent weights, one can opt for a custom BNN optimizer that inherently generates binary weights. An example of such an optimizer is Bop .","title":"Alternative: Custom Optimizers"},{"location":"guides/bnn-optimization/#choice-of-pseudo-gradient","text":"In larq.quantizers you will find a variety of quantizers that have been introduced in different papers. Many of these quantizers behave identically during the forward pass but implement different pseudo-gradients. Studies comparing different pseudo-gradients report little difference between them. Therefore, we recommend using the classical ste_sign() as default.","title":"Choice of Pseudo-Gradient"},{"location":"guides/bnn-optimization/#choice-of-optimizer","text":"When using a latent weight strategy, you can apply any optimizer you are familiar with from real-valued deep learning. However, due the different nature of BNNs your intuitions may be off. We recommend using Adam: although other optimizers can achieve similar accuracies with a lot of finetuning, we and others have found that Adam is the quickest to converge and the least sensitive to the choice of hyperparamters .","title":"Choice of Optimizer"},{"location":"guides/bnn-optimization/#tips-tricks","text":"Here are some general tips and tricks that you may want to keep in mind: BNN training is more noisy due to non-continuous nature of flipping weights; therefore, we recommend setting your batch norm momentum to 0.9. Beware that BNNs tend to require many more epochs than real-valued networks to converge: 200+ epochs when training an AlexNet or ResNet-18 style network on ImageNet is not unusual. Networks tend to train much quicker if they are initialized from a trained real-valued model. Importantly, this requires the overall architecture of the pretrained network to be as similar as possible to the BNN, including placement of the activation operation (which replaces the binarization operation). Note that although convergence is faster, pretraining does not seem to improve final accuracy.","title":"Tips &amp; Tricks"},{"location":"guides/bnn-optimization/#further-references","text":"If you would like to learn more, we recommend checking out the following papers (starting at the most recent): Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization - This paper investigates optimization of BNNs using latent weights and introduces Bop as the first custom BNN optimizer. An Empirical study of Binary Neural Networks' Optimisation - An empirical comparison of BNN optimization methods, including a detailed discussion on the use of various optimizers and a number of tricks used in the literature.","title":"Further References"},{"location":"guides/key-concepts/","text":"If you are new to Larq and/or Binarized Neural Networks (BNNs), this is the right place to start. Below we summarize the key concepts you need to understand to work with BNNs. Quantizer \u00b6 A quantizer defines an operation that quantizes a vector, as well as a pseudo-gradient that is used for automatic differentation. This pseudo-gradient is in general not the true gradient. Generally you will find quantizers throughout the network to quantize activations. This is because most layers output integers, even if all inputs are binary, because they sum over multiple binary values. It is also common to apply quantizers to the weights during training. This is necessary when relying on real-valued latent-weights to accumulate non-binary update steps, a common optimization strategy for BNNs. After training is finished, the real-valued weights and associated quantization operations can be discarded. Pseudo-Gradient \u00b6 The true gradient of a quantizer is in general zero almost everywhere and therefore cannot be used for gradient descent. Instead, optimization of BNNs rely on what we call pseudo-gradients, which are used during back-propagation. In the documentation for each quantizer you will find the definition and a graph of the pseudo-gradient. Using Quantizers as Activations \u00b6 Although quantizers are usually passed to specialized arguments (see Quantized Layers ), they can be used just like tf.keras activations: # Use a quantizer als activation y = larq . layers . QuantDense ( 512 , activation = \"ste_sign\" )( x ) Just like activations, quantizers can also be used on their own: # The two lines below are equivalent x_binarized = larq . quantizers . ste_sign ( x ) x_binarized = tf . keras . layers . Activation ( \"ste_sign\" )( x ) Quantized Layers \u00b6 Each quantized layers requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. A quantized layer computes \\[ \\sigma(f(q_{\\, \\mathrm{kernel}}(\\boldsymbol{w}), q_{\\, \\mathrm{input}}(\\boldsymbol{x})) + b) \\] with full precision weights \\boldsymbol{w} \\boldsymbol{w} , arbitrary precision input \\boldsymbol{x} \\boldsymbol{x} , layer operation f f (e.g. f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} for a densely-connected layer), activation \\sigma \\sigma and bias b b . This will result in the following computational graph: kernel bias input output input_quantizer kernel_quantizer layer_operation add activation Larq layers are fully compatible with the Keras API so you can use them with Keras Layers interchangeably: Larq 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , activation = \"relu\" ), larq . layers . QuantDense ( 10 , activation = \"softmax\" ) ]) Keras 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 512 , activation = \"relu\" ), tf . keras . layers . Dense ( 10 , activation = \"softmax\" ) ]) A simple fully-connected Binarized Neural Network (BNN) using the Straight-Through Estimator can be defined in just a few lines of code using either the Keras sequential, functional or model subclassing APIs: Larq 1-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) Larq 1-bit model functional x = tf . keras . Input ( shape = ( 28 , 28 , 1 )) y = tf . keras . layers . Flatten ()( x ) y = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" )( y ) y = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )( y ) model = tf . keras . Model ( inputs = x , outputs = y ) Larq 1-bit model subclassing class MyModel ( tf . keras . Model ): def __init__ ( self ): super () . __init__ () self . flatten = tf . keras . layers . Flatten () self . dense1 = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) self . dense2 = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" ) def call ( self , inputs ): x = self . flatten ( inputs ) x = self . dense1 ( x ) return self . dense2 ( x ) model = MyModel () Using Custom Quantizers \u00b6 Quantizers are functions that transform a full precision input to a quantized output. Since this transformation usually is non-differentiable it is necessary to modify the gradient in order to be able to train the resulting QNN. This can be done with the tf.custom_gradient decorator. In this example we will define a binarization function with an identity gradient: @tf.custom_gradient def identity_sign ( x ): def grad ( dy ): return dy return tf . sign ( x ), grad This function can now be used as an input_quantizer or a kernel_quantizer : larq . layers . QuantDense ( 10 , input_quantizer = identity_sign , kernel_quantizer = identity_sign )","title":"Key Concepts"},{"location":"guides/key-concepts/#quantizer","text":"A quantizer defines an operation that quantizes a vector, as well as a pseudo-gradient that is used for automatic differentation. This pseudo-gradient is in general not the true gradient. Generally you will find quantizers throughout the network to quantize activations. This is because most layers output integers, even if all inputs are binary, because they sum over multiple binary values. It is also common to apply quantizers to the weights during training. This is necessary when relying on real-valued latent-weights to accumulate non-binary update steps, a common optimization strategy for BNNs. After training is finished, the real-valued weights and associated quantization operations can be discarded.","title":"Quantizer"},{"location":"guides/key-concepts/#pseudo-gradient","text":"The true gradient of a quantizer is in general zero almost everywhere and therefore cannot be used for gradient descent. Instead, optimization of BNNs rely on what we call pseudo-gradients, which are used during back-propagation. In the documentation for each quantizer you will find the definition and a graph of the pseudo-gradient.","title":"Pseudo-Gradient"},{"location":"guides/key-concepts/#using-quantizers-as-activations","text":"Although quantizers are usually passed to specialized arguments (see Quantized Layers ), they can be used just like tf.keras activations: # Use a quantizer als activation y = larq . layers . QuantDense ( 512 , activation = \"ste_sign\" )( x ) Just like activations, quantizers can also be used on their own: # The two lines below are equivalent x_binarized = larq . quantizers . ste_sign ( x ) x_binarized = tf . keras . layers . Activation ( \"ste_sign\" )( x )","title":"Using Quantizers as Activations"},{"location":"guides/key-concepts/#quantized-layers","text":"Each quantized layers requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. A quantized layer computes \\[ \\sigma(f(q_{\\, \\mathrm{kernel}}(\\boldsymbol{w}), q_{\\, \\mathrm{input}}(\\boldsymbol{x})) + b) \\] with full precision weights \\boldsymbol{w} \\boldsymbol{w} , arbitrary precision input \\boldsymbol{x} \\boldsymbol{x} , layer operation f f (e.g. f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} for a densely-connected layer), activation \\sigma \\sigma and bias b b . This will result in the following computational graph: kernel bias input output input_quantizer kernel_quantizer layer_operation add activation Larq layers are fully compatible with the Keras API so you can use them with Keras Layers interchangeably: Larq 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , activation = \"relu\" ), larq . layers . QuantDense ( 10 , activation = \"softmax\" ) ]) Keras 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 512 , activation = \"relu\" ), tf . keras . layers . Dense ( 10 , activation = \"softmax\" ) ]) A simple fully-connected Binarized Neural Network (BNN) using the Straight-Through Estimator can be defined in just a few lines of code using either the Keras sequential, functional or model subclassing APIs: Larq 1-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) Larq 1-bit model functional x = tf . keras . Input ( shape = ( 28 , 28 , 1 )) y = tf . keras . layers . Flatten ()( x ) y = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" )( y ) y = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )( y ) model = tf . keras . Model ( inputs = x , outputs = y ) Larq 1-bit model subclassing class MyModel ( tf . keras . Model ): def __init__ ( self ): super () . __init__ () self . flatten = tf . keras . layers . Flatten () self . dense1 = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) self . dense2 = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" ) def call ( self , inputs ): x = self . flatten ( inputs ) x = self . dense1 ( x ) return self . dense2 ( x ) model = MyModel ()","title":"Quantized Layers"},{"location":"guides/key-concepts/#using-custom-quantizers","text":"Quantizers are functions that transform a full precision input to a quantized output. Since this transformation usually is non-differentiable it is necessary to modify the gradient in order to be able to train the resulting QNN. This can be done with the tf.custom_gradient decorator. In this example we will define a binarization function with an identity gradient: @tf.custom_gradient def identity_sign ( x ): def grad ( dy ): return dy return tf . sign ( x ), grad This function can now be used as an input_quantizer or a kernel_quantizer : larq . layers . QuantDense ( 10 , input_quantizer = identity_sign , kernel_quantizer = identity_sign )","title":"Using Custom Quantizers"},{"location":"models/","text":"Larq Zoo Pretrained Models \u00b6 Larq Zoo provides reference implementations of deep neural networks with extremely low precision weights and activations that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning. We believe that a collection of tested implementations with pretrained weights is greatly beneficial for the field of Extremely Quantized Neural Networks. To improve reproducibility we have implemented a few commonly used models found in the literature. If you have developed or reimplemented a Binarized or other Extremely Quantized Neural Network and want to share it with the community such that future papers can build on top of your work, please add it to Larq Zoo or get in touch with us if you need any help. Available models \u00b6 The following models are trained on the ImageNet dataset. The Top-1 and Top-5 accuracy refers to the model's performance on the ImageNet validation dataset, memory refers to the memory after quantization of the weights. The model definitions and the train loop are available in the Larq Zoo repository . Model Top-1 Accuracy Top-5 Accuracy Parameters Memory Binary AlexNet 36.28 % 61.05 % 61 859 192 7.49 MB Bi-Real Net 55.88 % 78.62 % 11 736 232 4.04 MB XNOR-Net 43.03 % 67.32 % 62 396 768 22.81 MB Installation \u00b6 Larq Zoo is not included in Larq by default. To start using it, you can install it with Python's pip package manager: pip install larq-zoo Weights can be downloaded automatically when instantiating a model. They are stored at ~/.larq/models/ .","title":"Larq Zoo"},{"location":"models/#larq-zoo-pretrained-models","text":"Larq Zoo provides reference implementations of deep neural networks with extremely low precision weights and activations that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning. We believe that a collection of tested implementations with pretrained weights is greatly beneficial for the field of Extremely Quantized Neural Networks. To improve reproducibility we have implemented a few commonly used models found in the literature. If you have developed or reimplemented a Binarized or other Extremely Quantized Neural Network and want to share it with the community such that future papers can build on top of your work, please add it to Larq Zoo or get in touch with us if you need any help.","title":"Larq Zoo Pretrained Models"},{"location":"models/#available-models","text":"The following models are trained on the ImageNet dataset. The Top-1 and Top-5 accuracy refers to the model's performance on the ImageNet validation dataset, memory refers to the memory after quantization of the weights. The model definitions and the train loop are available in the Larq Zoo repository . Model Top-1 Accuracy Top-5 Accuracy Parameters Memory Binary AlexNet 36.28 % 61.05 % 61 859 192 7.49 MB Bi-Real Net 55.88 % 78.62 % 11 736 232 4.04 MB XNOR-Net 43.03 % 67.32 % 62 396 768 22.81 MB","title":"Available models"},{"location":"models/#installation","text":"Larq Zoo is not included in Larq by default. To start using it, you can install it with Python's pip package manager: pip install larq-zoo Weights can be downloaded automatically when instantiating a model. They are stored at ~/.larq/models/ .","title":"Installation"},{"location":"models/api/","text":"Larq Zoo API Documentation \u00b6 BinaryAlexNet \u00b6 BinaryAlexNet ( include_top = True , weights = \"imagenet\" , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the Binary AlexNet architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layers at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras Tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References Binarized Neural Networks BiRealNet \u00b6 BiRealNet ( include_top = True , weights = \"imagenet\" , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the Bi-Real Net architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras Tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm XNORNet \u00b6 XNORNet ( include_top = True , weights = \"imagenet\" , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the XNOR-Net architecture. Optionally loads weights pre-trained on ImageNet. // embed when document is loaded, to ensure vega library is available // this works on all modern browsers, except IE8 and older document.addEventListener(\"DOMContentLoaded\", function(event) { var opt = { \"mode\": \"vega-lite\", \"renderer\": \"canvas\", \"actions\": false, }; vegaEmbed('#altair-plot-c41b3207-e322-409d-a0d4-26b00df302f8', '/plots/xnornet.vg.json', opt).catch(console.err); }, {passive: true, once: true}); Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras Tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks decode_predictions \u00b6 decode_predictions ( preds , top = 5 , ** kwargs ) Decodes the prediction of an ImageNet model. Arguments preds : Numpy Tensor encoding a batch of predictions. top : Integer, how many top-guesses to return. Returns A list of lists of top class prediction tuples (class_name, class_description, score) . One list of tuples per sample in batch input. Raises ValueError : In case of invalid shape of the pred array (must be 2D). preprocess_input \u00b6 preprocess_input ( image ) Preprocesses a Tensor or Numpy array encoding a image. Arguments image : Numpy array or symbolic Tensor, 3D. Returns Preprocessed Tensor or Numpy array.","title":"API"},{"location":"models/api/#larq-zoo-api-documentation","text":"","title":"Larq Zoo API Documentation"},{"location":"models/api/#binaryalexnet","text":"BinaryAlexNet ( include_top = True , weights = \"imagenet\" , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the Binary AlexNet architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layers at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras Tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References Binarized Neural Networks","title":"BinaryAlexNet"},{"location":"models/api/#birealnet","text":"BiRealNet ( include_top = True , weights = \"imagenet\" , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the Bi-Real Net architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras Tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"BiRealNet"},{"location":"models/api/#xnornet","text":"XNORNet ( include_top = True , weights = \"imagenet\" , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the XNOR-Net architecture. Optionally loads weights pre-trained on ImageNet. // embed when document is loaded, to ensure vega library is available // this works on all modern browsers, except IE8 and older document.addEventListener(\"DOMContentLoaded\", function(event) { var opt = { \"mode\": \"vega-lite\", \"renderer\": \"canvas\", \"actions\": false, }; vegaEmbed('#altair-plot-c41b3207-e322-409d-a0d4-26b00df302f8', '/plots/xnornet.vg.json', opt).catch(console.err); }, {passive: true, once: true}); Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras Tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks","title":"XNORNet"},{"location":"models/api/#decode_predictions","text":"decode_predictions ( preds , top = 5 , ** kwargs ) Decodes the prediction of an ImageNet model. Arguments preds : Numpy Tensor encoding a batch of predictions. top : Integer, how many top-guesses to return. Returns A list of lists of top class prediction tuples (class_name, class_description, score) . One list of tuples per sample in batch input. Raises ValueError : In case of invalid shape of the pred array (must be 2D).","title":"decode_predictions"},{"location":"models/api/#preprocess_input","text":"preprocess_input ( image ) Preprocesses a Tensor or Numpy array encoding a image. Arguments image : Numpy array or symbolic Tensor, 3D. Returns Preprocessed Tensor or Numpy array.","title":"preprocess_input"},{"location":"models/examples/","text":"Larq Zoo Examples \u00b6 Classify ImageNet classes with Bi-Real Net \u00b6 import tensorflow as tf import larq_zoo as lqz model = lqz . BiRealNet ( weights = \"imagenet\" ) img_path = \"tests/fixtures/elephant.jpg\" img = tf . keras . preprocessing . image . load_img ( img_path , target_size = ( 224 , 224 )) x = tf . keras . preprocessing . image . img_to_array ( img ) x = lqz . preprocess_input ( x ) x = np . expand_dims ( x , axis = 0 ) preds = model . predict ( x ) # decode the results into a list of tuples (class, description, probability) # (one such list for each sample in the batch) print ( \"Predicted:\" , lqz . decode_predictions ( preds , top = 3 )[ 0 ]) # Predicted: [(\"n01871265\", \"tusker\", 0.7427464), (\"n02504458\", \"African_elephant\", 0.19439144), (\"n02504013\", \"Indian_elephant\", 0.058899447)] Extract features with Bi-Real Net \u00b6 import tensorflow as tf import larq_zoo as lqz model = lqz . BiRealNet ( weights = \"imagenet\" , include_top = False ) img_path = \"tests/fixtures/elephant.jpg\" img = tf . keras . preprocessing . image . load_img ( img_path , target_size = ( 224 , 224 )) x = tf . keras . preprocessing . image . img_to_array ( img ) x = lqz . preprocess_input ( x ) x = np . expand_dims ( x , axis = 0 ) features = model . predict ( x ) Extract features from an arbitrary intermediate layer with Bi-Real Net \u00b6 import tensorflow as tf import larq_zoo as lqz base_model = lqz . BiRealNet ( weights = \"imagenet\" ) model = tf . keras . models . Model ( inputs = base_model . input , outputs = base_model . get_layer ( \"average_pooling2d_8\" ) . output ) img_path = \"tests/fixtures/elephant.jpg\" img = tf . keras . preprocessing . image . load_img ( img_path , target_size = ( 224 , 224 )) x = tf . keras . preprocessing . image . img_to_array ( img ) x = lqz . preprocess_input ( x ) x = np . expand_dims ( x , axis = 0 ) average_pool_8_features = model . predict ( x ) Fine-tune Bi-Real Net on a new set of classes \u00b6 import tensorflow as tf import larq as lq import larq_zoo as lqz # create the base pre-trained model base_model = lqz . BiRealNet ( weights = \"imagenet\" , include_top = False ) # add a global spatial average pooling layer x = base_model . output x = tf . keras . layers . GlobalAveragePooling2D ()( x ) # let's add a binarized fully-connected layer x = lq . layers . QuantDense ( 1024 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , activation = \"relu\" , )( x ) x = tf . keras . layers . BatchNormalization ()( x ) # and a full precision logistic layer -- let's say we have 200 classes predictions = tf . keras . layers . Dense ( 200 , activation = \"softmax\" )( x ) # this is the model we will train model = tf . keras . models . Model ( inputs = base_model . input , outputs = predictions ) # first: train only the top layers (which were randomly initialized) # i.e. freeze all convolutional Bi-Real Net layers for layer in base_model . layers : layer . trainable = False # compile the model (should be done *after* setting layers to non-trainable) model . compile ( optimizer = \"rmsprop\" , loss = \"categorical_crossentropy\" ) # train the model on the new data for a few epochs model . fit ( ... ) # at this point, the top layers are well trained and we can start fine-tuning # convolutional layers from Bi-Real Net. We will freeze the bottom N layers # and train the remaining top layers. # let's visualize layer names and layer indices to see how many layers # we should freeze: for i , layer in enumerate ( base_model . layers ): print ( i , layer . name ) # we chose to train the top block, i.e. we will freeze # the first 49 layers and unfreeze the rest: for layer in model . layers [: 49 ]: layer . trainable = False for layer in model . layers [ 49 :]: layer . trainable = True # we need to recompile the model for these modifications to take effect # we use SGD with a low learning rate model . compile ( optimizer = tf . keras . optimizers . SGD ( lr = 0.0001 , momentum = 0.9 ), loss = \"categorical_crossentropy\" , ) # we train our model again (this time fine-tuning the top block # alongside the top Dense layers model . fit ( ... ) Build Bi-Real Net over a custom input Tensor \u00b6 import tensorflow as tf import larq_zoo as lqz # this could also be the output a different Keras model or layer input_tensor = tf . keras . layers . Input ( shape = ( 224 , 224 , 3 )) model = lqz . BiRealNet ( input_tensor = input_tensor , weights = \"imagenet\" )","title":"Examples"},{"location":"models/examples/#larq-zoo-examples","text":"","title":"Larq Zoo Examples"},{"location":"models/examples/#classify-imagenet-classes-with-bi-real-net","text":"import tensorflow as tf import larq_zoo as lqz model = lqz . BiRealNet ( weights = \"imagenet\" ) img_path = \"tests/fixtures/elephant.jpg\" img = tf . keras . preprocessing . image . load_img ( img_path , target_size = ( 224 , 224 )) x = tf . keras . preprocessing . image . img_to_array ( img ) x = lqz . preprocess_input ( x ) x = np . expand_dims ( x , axis = 0 ) preds = model . predict ( x ) # decode the results into a list of tuples (class, description, probability) # (one such list for each sample in the batch) print ( \"Predicted:\" , lqz . decode_predictions ( preds , top = 3 )[ 0 ]) # Predicted: [(\"n01871265\", \"tusker\", 0.7427464), (\"n02504458\", \"African_elephant\", 0.19439144), (\"n02504013\", \"Indian_elephant\", 0.058899447)]","title":"Classify ImageNet classes with Bi-Real Net"},{"location":"models/examples/#extract-features-with-bi-real-net","text":"import tensorflow as tf import larq_zoo as lqz model = lqz . BiRealNet ( weights = \"imagenet\" , include_top = False ) img_path = \"tests/fixtures/elephant.jpg\" img = tf . keras . preprocessing . image . load_img ( img_path , target_size = ( 224 , 224 )) x = tf . keras . preprocessing . image . img_to_array ( img ) x = lqz . preprocess_input ( x ) x = np . expand_dims ( x , axis = 0 ) features = model . predict ( x )","title":"Extract features with Bi-Real Net"},{"location":"models/examples/#extract-features-from-an-arbitrary-intermediate-layer-with-bi-real-net","text":"import tensorflow as tf import larq_zoo as lqz base_model = lqz . BiRealNet ( weights = \"imagenet\" ) model = tf . keras . models . Model ( inputs = base_model . input , outputs = base_model . get_layer ( \"average_pooling2d_8\" ) . output ) img_path = \"tests/fixtures/elephant.jpg\" img = tf . keras . preprocessing . image . load_img ( img_path , target_size = ( 224 , 224 )) x = tf . keras . preprocessing . image . img_to_array ( img ) x = lqz . preprocess_input ( x ) x = np . expand_dims ( x , axis = 0 ) average_pool_8_features = model . predict ( x )","title":"Extract features from an arbitrary intermediate layer with Bi-Real Net"},{"location":"models/examples/#fine-tune-bi-real-net-on-a-new-set-of-classes","text":"import tensorflow as tf import larq as lq import larq_zoo as lqz # create the base pre-trained model base_model = lqz . BiRealNet ( weights = \"imagenet\" , include_top = False ) # add a global spatial average pooling layer x = base_model . output x = tf . keras . layers . GlobalAveragePooling2D ()( x ) # let's add a binarized fully-connected layer x = lq . layers . QuantDense ( 1024 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , activation = \"relu\" , )( x ) x = tf . keras . layers . BatchNormalization ()( x ) # and a full precision logistic layer -- let's say we have 200 classes predictions = tf . keras . layers . Dense ( 200 , activation = \"softmax\" )( x ) # this is the model we will train model = tf . keras . models . Model ( inputs = base_model . input , outputs = predictions ) # first: train only the top layers (which were randomly initialized) # i.e. freeze all convolutional Bi-Real Net layers for layer in base_model . layers : layer . trainable = False # compile the model (should be done *after* setting layers to non-trainable) model . compile ( optimizer = \"rmsprop\" , loss = \"categorical_crossentropy\" ) # train the model on the new data for a few epochs model . fit ( ... ) # at this point, the top layers are well trained and we can start fine-tuning # convolutional layers from Bi-Real Net. We will freeze the bottom N layers # and train the remaining top layers. # let's visualize layer names and layer indices to see how many layers # we should freeze: for i , layer in enumerate ( base_model . layers ): print ( i , layer . name ) # we chose to train the top block, i.e. we will freeze # the first 49 layers and unfreeze the rest: for layer in model . layers [: 49 ]: layer . trainable = False for layer in model . layers [ 49 :]: layer . trainable = True # we need to recompile the model for these modifications to take effect # we use SGD with a low learning rate model . compile ( optimizer = tf . keras . optimizers . SGD ( lr = 0.0001 , momentum = 0.9 ), loss = \"categorical_crossentropy\" , ) # we train our model again (this time fine-tuning the top block # alongside the top Dense layers model . fit ( ... )","title":"Fine-tune Bi-Real Net on a new set of classes"},{"location":"models/examples/#build-bi-real-net-over-a-custom-input-tensor","text":"import tensorflow as tf import larq_zoo as lqz # this could also be the output a different Keras model or layer input_tensor = tf . keras . layers . Input ( shape = ( 224 , 224 , 3 )) model = lqz . BiRealNet ( input_tensor = input_tensor , weights = \"imagenet\" )","title":"Build Bi-Real Net over a custom input Tensor"}]}
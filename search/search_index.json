{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Larq \u00b6 Larq is an open-source deep learning library for training neural networks with extremely low precision weights and activations, such as Binarized Neural Networks (BNNs). Existing deep neural networks use 32 bits, 16 bits or 8 bits to encode each weight and activation, making them large, slow and power-hungry. This prohibits many applications in resource-constrained environments. Larq is the first step towards solving this. It is designed to provide an easy to use, composable way to train BNNs (1 bit) and other types of Quantized Neural Networks (QNNs) and is based on the tf.keras interface. Getting Started \u00b6 To build a QNN, Larq introduces the concept of quantized layers and quantizers . A quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each quantized layer requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. You can define a simple binarized fully-connected Keras model using the Straight-Through Estimator the following way: model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) This layer can be used inside a Keras model or with a custom training loop . Examples \u00b6 Check out our examples on how to train a Binarized Neural Network in just a few lines of code: Introduction to BNNs with Larq BinaryNet on CIFAR10 BinaryNet on CIFAR10 (Advanced) Requirements \u00b6 Before installing Larq, please install: Python version 3.6 or 3.7 Tensorflow version 1.13+ or 2.0.0 You can also check out one of our prebuilt docker images . Installation \u00b6 You can install Larq with Python's pip package manager: pip install larq","title":"Introduction"},{"location":"#larq","text":"Larq is an open-source deep learning library for training neural networks with extremely low precision weights and activations, such as Binarized Neural Networks (BNNs). Existing deep neural networks use 32 bits, 16 bits or 8 bits to encode each weight and activation, making them large, slow and power-hungry. This prohibits many applications in resource-constrained environments. Larq is the first step towards solving this. It is designed to provide an easy to use, composable way to train BNNs (1 bit) and other types of Quantized Neural Networks (QNNs) and is based on the tf.keras interface.","title":"Larq"},{"location":"#getting-started","text":"To build a QNN, Larq introduces the concept of quantized layers and quantizers . A quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each quantized layer requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. You can define a simple binarized fully-connected Keras model using the Straight-Through Estimator the following way: model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) This layer can be used inside a Keras model or with a custom training loop .","title":"Getting Started"},{"location":"#examples","text":"Check out our examples on how to train a Binarized Neural Network in just a few lines of code: Introduction to BNNs with Larq BinaryNet on CIFAR10 BinaryNet on CIFAR10 (Advanced)","title":"Examples"},{"location":"#requirements","text":"Before installing Larq, please install: Python version 3.6 or 3.7 Tensorflow version 1.13+ or 2.0.0 You can also check out one of our prebuilt docker images .","title":"Requirements"},{"location":"#installation","text":"You can install Larq with Python's pip package manager: pip install larq","title":"Installation"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at roeland@plumerai.co.uk or lukas@plumerai.co.uk. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at roeland@plumerai.co.uk or lukas@plumerai.co.uk. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contributing to Larq \u00b6 \ud83d\udc4d \ud83c\udf89 First off, thanks for taking the time to contribute! \ud83d\udc4d \ud83c\udf89 Working on your first Pull Request? You can learn how from this free series How to Contribute to an Open Source Project on GitHub . Project setup \u00b6 To send a Pull Request it is required to fork Larq on GitHub. After that clone it to a desired directory: git clone https://github.com/my-username/larq.git Install all required dependencies for local development by running: cd larq # go into the directory you just cloned pip install -e . [ tensorflow ] # Installs Tensorflow for CPU # pip install -e .[tensorflow_gpu] # Installs Tensorflow for GPU pip install -e . [ test ] # Installs all development dependencies Run Unit tests \u00b6 Inside the project directory run: pytest . Build documentation \u00b6 Installs dependencies for building the docs: pip install git+https://github.com/lgeiger/pydoc-markdown.git pip install -e . [ docs ] Inside the project directory run: python generate_api_docs.py mkdocs serve To publish a new version to github pages run: mkdocs gh-deploy --force Code style \u00b6 We use black to format all of our code. We recommend installing it as a plugin for your favorite code editor . Publish release \u00b6 Install dependencies python -m pip install --upgrade setuptools wheel twine Increment the version number in setup.py Push new tag git tag <version number> git push && git push --tags Build wheels rm -r build/* dist/* python setup.py sdist bdist_wheel Upload release python -m twine upload dist/*","title":"Contributing Guide"},{"location":"contributing/#contributing-to-larq","text":"\ud83d\udc4d \ud83c\udf89 First off, thanks for taking the time to contribute! \ud83d\udc4d \ud83c\udf89 Working on your first Pull Request? You can learn how from this free series How to Contribute to an Open Source Project on GitHub .","title":"Contributing to Larq"},{"location":"contributing/#project-setup","text":"To send a Pull Request it is required to fork Larq on GitHub. After that clone it to a desired directory: git clone https://github.com/my-username/larq.git Install all required dependencies for local development by running: cd larq # go into the directory you just cloned pip install -e . [ tensorflow ] # Installs Tensorflow for CPU # pip install -e .[tensorflow_gpu] # Installs Tensorflow for GPU pip install -e . [ test ] # Installs all development dependencies","title":"Project setup"},{"location":"contributing/#run-unit-tests","text":"Inside the project directory run: pytest .","title":"Run Unit tests"},{"location":"contributing/#build-documentation","text":"Installs dependencies for building the docs: pip install git+https://github.com/lgeiger/pydoc-markdown.git pip install -e . [ docs ] Inside the project directory run: python generate_api_docs.py mkdocs serve To publish a new version to github pages run: mkdocs gh-deploy --force","title":"Build documentation"},{"location":"contributing/#code-style","text":"We use black to format all of our code. We recommend installing it as a plugin for your favorite code editor .","title":"Code style"},{"location":"contributing/#publish-release","text":"Install dependencies python -m pip install --upgrade setuptools wheel twine Increment the version number in setup.py Push new tag git tag <version number> git push && git push --tags Build wheels rm -r build/* dist/* python setup.py sdist bdist_wheel Upload release python -m twine upload dist/*","title":"Publish release"},{"location":"guide/","text":"User Guide \u00b6 To build a Quantized Neural Network (QNN), Larq introduces the concept of quantized layers and quantizers . A quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each quantized layer requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. A quantized layer computes \\[ \\sigma(f(q_{\\, \\mathrm{kernel}}(\\boldsymbol{w}), q_{\\, \\mathrm{input}}(\\boldsymbol{x})) + b) \\] with full precision weights \\boldsymbol{w} \\boldsymbol{w} , arbitrary precision input \\boldsymbol{x} \\boldsymbol{x} , layer operation f f (e.g. f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} for a densely-connected layer), activation \\sigma \\sigma and bias b b . This will result in the following computational graph: kernel bias input output input_quantizer kernel_quantizer layer_operation add activation Larq layers are fully compatible with the Keras API so you can use them with Keras Layers interchangeably: Larq 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , activation = \"relu\" ), larq . layers . QuantDense ( 10 , activation = \"softmax\" ) ]) Keras 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 512 , activation = \"relu\" ), tf . keras . layers . Dense ( 10 , activation = \"softmax\" ) ]) A simple fully-connected Binarized Neural Network (BNN) using the Straight-Through Estimator can be defined in just a few lines of code using either the Keras sequential, functional or model subclassing APIs: Larq 1-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) Larq 1-bit model functional x = tf . keras . Input ( shape = ( 28 , 28 , 1 )) y = tf . keras . layers . Flatten ()( x ) y = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" )( y ) y = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )( y ) model = tf . keras . Model ( inputs = x , outputs = y ) Larq 1-bit model subclassing class MyModel ( tf . keras . Model ): def __init__ ( self ): super () . __init__ () self . flatten = tf . keras . layers . Flatten () self . dense1 = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) self . dense2 = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" ) def call ( self , inputs ): x = self . flatten ( inputs ) x = self . dense1 ( x ) return self . dense2 ( x ) model = MyModel () Using Custom Quantizers \u00b6 Quantizers are functions that transform a full precision input to a quantized output. Since this transformation usually is non-differentiable it is necessary to modify the gradient in order to be able to train the resulting QNN. This can be done with the tf.custom_gradient decorator. In this example we will define a binarization function with an identity gradient: @tf.custom_gradient def identity_sign ( x ): def grad ( dy ): return dy return tf . sign ( x ), grad This function can now be used as an input_quantizer or a kernel_quantizer : larq . layers . QuantDense ( 10 , input_quantizer = identity_sign , kernel_quantizer = identity_sign )","title":"User Guide"},{"location":"guide/#user-guide","text":"To build a Quantized Neural Network (QNN), Larq introduces the concept of quantized layers and quantizers . A quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. Each quantized layer requires an input_quantizer and a kernel_quantizer that describe the way of quantizing the incoming activations and weights of the layer respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. A quantized layer computes \\[ \\sigma(f(q_{\\, \\mathrm{kernel}}(\\boldsymbol{w}), q_{\\, \\mathrm{input}}(\\boldsymbol{x})) + b) \\] with full precision weights \\boldsymbol{w} \\boldsymbol{w} , arbitrary precision input \\boldsymbol{x} \\boldsymbol{x} , layer operation f f (e.g. f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} f(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{x}^T \\boldsymbol{w} for a densely-connected layer), activation \\sigma \\sigma and bias b b . This will result in the following computational graph: kernel bias input output input_quantizer kernel_quantizer layer_operation add activation Larq layers are fully compatible with the Keras API so you can use them with Keras Layers interchangeably: Larq 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , activation = \"relu\" ), larq . layers . QuantDense ( 10 , activation = \"softmax\" ) ]) Keras 32-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 512 , activation = \"relu\" ), tf . keras . layers . Dense ( 10 , activation = \"softmax\" ) ]) A simple fully-connected Binarized Neural Network (BNN) using the Straight-Through Estimator can be defined in just a few lines of code using either the Keras sequential, functional or model subclassing APIs: Larq 1-bit model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ), larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )]) Larq 1-bit model functional x = tf . keras . Input ( shape = ( 28 , 28 , 1 )) y = tf . keras . layers . Flatten ()( x ) y = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" )( y ) y = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" )( y ) model = tf . keras . Model ( inputs = x , outputs = y ) Larq 1-bit model subclassing class MyModel ( tf . keras . Model ): def __init__ ( self ): super () . __init__ () self . flatten = tf . keras . layers . Flatten () self . dense1 = larq . layers . QuantDense ( 512 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) self . dense2 = larq . layers . QuantDense ( 10 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , activation = \"softmax\" ) def call ( self , inputs ): x = self . flatten ( inputs ) x = self . dense1 ( x ) return self . dense2 ( x ) model = MyModel ()","title":"User Guide"},{"location":"guide/#using-custom-quantizers","text":"Quantizers are functions that transform a full precision input to a quantized output. Since this transformation usually is non-differentiable it is necessary to modify the gradient in order to be able to train the resulting QNN. This can be done with the tf.custom_gradient decorator. In this example we will define a binarization function with an identity gradient: @tf.custom_gradient def identity_sign ( x ): def grad ( dy ): return dy return tf . sign ( x ), grad This function can now be used as an input_quantizer or a kernel_quantizer : larq . layers . QuantDense ( 10 , input_quantizer = identity_sign , kernel_quantizer = identity_sign )","title":"Using Custom Quantizers"},{"location":"papers/","text":"Papers using Larq \u00b6 One of the main focuses of Larq is to accelerate research on neural networks with extremely low precision weights and activations. If you puplish a paper using using Larq please let us know and add it to the list below . Feel free to also add the author names, abstract and links to the paper and source code. code library_books Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, Roeland Nusselder Optimization of Binarized Neural Networks (BNNs) currently relies on real-valued latent weights to accumulate small update steps. In this paper, we argue that these latent weights cannot be treated analogously to weights in real-valued networks. Instead their main role is to provide inertia during training. We interpret current methods in terms of inertia and provide novel insights into the optimization of BNNs. We subsequently introduce the first optimizer specifically designed for BNNs, Binary Optimizer (Bop), and demonstrate its performance on CIFAR-10 and ImageNet. Together, the redefinition of latent weights as inertia and the introduction of Bop enable a better understanding of BNN optimization and open up the way for further improvements in training methodologies for BNNs.","title":"Papers using Larq"},{"location":"papers/#papers-using-larq","text":"One of the main focuses of Larq is to accelerate research on neural networks with extremely low precision weights and activations. If you puplish a paper using using Larq please let us know and add it to the list below . Feel free to also add the author names, abstract and links to the paper and source code.","title":"Papers using Larq"},{"location":"api/activations/","text":"larq.activations \u00b6 Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers: import tensorflow as tf import larq as lq model . add ( lq . layers . QuantDense ( 64 )) model . add ( tf . keras . layers . Activation ( 'hard_tanh' )) This is equivalent to: model . add ( lq . layers . QuantDense ( 64 , activation = 'hard_tanh' )) You can also pass an element-wise TensorFlow function as an activation: model . add ( lq . layers . QuantDense ( 64 , activation = lq . activations . hard_tanh )) hard_tanh \u00b6 hard_tanh ( x ) Hard tanh activation function. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Hard tanh activation. leaky_tanh \u00b6 leaky_tanh ( x , alpha = 0.2 ) Leaky tanh activation function. Similar to hard tanh, but with non-zero slopes as in leaky ReLU. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. alpha : Slope of the activation function outside of [-1, 1]. Returns Leaky tanh activation.","title":"Activations"},{"location":"api/activations/#larqactivations","text":"Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers: import tensorflow as tf import larq as lq model . add ( lq . layers . QuantDense ( 64 )) model . add ( tf . keras . layers . Activation ( 'hard_tanh' )) This is equivalent to: model . add ( lq . layers . QuantDense ( 64 , activation = 'hard_tanh' )) You can also pass an element-wise TensorFlow function as an activation: model . add ( lq . layers . QuantDense ( 64 , activation = lq . activations . hard_tanh ))","title":"larq.activations"},{"location":"api/activations/#hard_tanh","text":"hard_tanh ( x ) Hard tanh activation function. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Hard tanh activation.","title":"hard_tanh"},{"location":"api/activations/#leaky_tanh","text":"leaky_tanh ( x , alpha = 0.2 ) Leaky tanh activation function. Similar to hard tanh, but with non-zero slopes as in leaky ReLU. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. alpha : Slope of the activation function outside of [-1, 1]. Returns Leaky tanh activation.","title":"leaky_tanh"},{"location":"api/callbacks/","text":"larq.callbacks \u00b6 HyperparameterScheduler \u00b6 HyperparameterScheduler ( schedule , hyperparameter , verbose = 0 ) Generic hyperparameter scheduler. Arguments schedule : a function that takes an epoch index as input (integer, indexed from 0) and returns a new hyperparameter as output. hyperparameter : str. the name of the hyperparameter to be scheduled. verbose : int. 0: quiet, 1: update messages.","title":"Callbacks"},{"location":"api/callbacks/#larqcallbacks","text":"","title":"larq.callbacks"},{"location":"api/callbacks/#hyperparameterscheduler","text":"HyperparameterScheduler ( schedule , hyperparameter , verbose = 0 ) Generic hyperparameter scheduler. Arguments schedule : a function that takes an epoch index as input (integer, indexed from 0) and returns a new hyperparameter as output. hyperparameter : str. the name of the hyperparameter to be scheduled. verbose : int. 0: quiet, 1: update messages.","title":"HyperparameterScheduler"},{"location":"api/constraints/","text":"larq.constraints \u00b6 Functions from the constraints module allow setting constraints (eg. weight clipping) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers QuantDense , QuantConv1D , QuantConv2D and QuantConv3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. import larq as lq lq . layers . QuantDense ( 64 , kernel_constraint = \"weight_clip\" ) lq . layers . QuantDense ( 64 , kernel_constraint = lq . constraints . WeightClip ( 2. )) WeightClip \u00b6 WeightClip ( clip_value = 1 ) Weight Clip constraint Constrains the weights incident to each hidden unit to be between [-clip_value, clip_value] . Arguments clip_value : The value to clip incoming weights. weight_clip \u00b6 weight_clip ( clip_value = 1 )","title":"Constraints"},{"location":"api/constraints/#larqconstraints","text":"Functions from the constraints module allow setting constraints (eg. weight clipping) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers QuantDense , QuantConv1D , QuantConv2D and QuantConv3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. import larq as lq lq . layers . QuantDense ( 64 , kernel_constraint = \"weight_clip\" ) lq . layers . QuantDense ( 64 , kernel_constraint = lq . constraints . WeightClip ( 2. ))","title":"larq.constraints"},{"location":"api/constraints/#weightclip","text":"WeightClip ( clip_value = 1 ) Weight Clip constraint Constrains the weights incident to each hidden unit to be between [-clip_value, clip_value] . Arguments clip_value : The value to clip incoming weights.","title":"WeightClip"},{"location":"api/constraints/#weight_clip","text":"weight_clip ( clip_value = 1 )","title":"weight_clip"},{"location":"api/layers/","text":"larq.layers \u00b6 Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer. QuantDense \u00b6 QuantDense ( units , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Just your regular densely-connected quantized NN layer. QuantDense implements the operation: output = activation(dot(input_quantizer(input), kernel_quantizer(kernel)) + bias) , where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Dense . If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Sequential () model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , input_shape = ( 16 ,), ) ) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) ) Arguments units : Positive integer, dimensionality of the output space. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel weights matrix. bias_constraint : Constraint function applied to the bias vector. Input shape N-D tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape N-D tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) . QuantConv1D \u00b6 QuantConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = 'channels_last' , dilation_rate = 1 , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 1D quantized convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv1D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for variable-length sequences of 128-dimensional vectors. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1 :]. Useful when modeling temporal data where the model should not violate the temporal order. See [WaveNet : A Generative Model for Raw Audio, section 2.1](https ://arxiv.org/abs/1609.03499). data_format : A string, one of channels_last (default) or channels_first . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) . steps value might have changed due to padding or strides. QuantConv2D \u00b6 QuantConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 2D quantized convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. QuantConv3D \u00b6 QuantConv3D ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format='channels_last'. new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding. QuantSeparableConv1D \u00b6 QuantSeparableConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , dilation_rate = 1 , depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 1D quantized convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of filters in the convolution). kernel_size : A single integer specifying the spatial dimensions of the filters. strides : A single integer specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"same\" , or \"causal\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . dilation_rate : A single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier . activation : Activation function. Set it to None to maintain a linear activation. use_bias : Boolean, whether the layer uses a bias. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel. pointwise_quantizer : Quantization function applied to the pointwise kernel. depthwise_initializer : An initializer for the depthwise convolution kernel. pointwise_initializer : An initializer for the pointwise convolution kernel. bias_initializer : An initializer for the bias vector. If None, the default initializer will be used. depthwise_regularizer : Optional regularizer for the depthwise convolution kernel. pointwise_regularizer : Optional regularizer for the pointwise convolution kernel. bias_regularizer : Optional regularizer for the bias vector. activity_regularizer : Optional regularizer function for the output. depthwise_constraint : Optional projection function to be applied to the depthwise kernel after being updated by an Optimizer (e.g. used for norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. pointwise_constraint : Optional projection function to be applied to the pointwise kernel after being updated by an Optimizer . bias_constraint : Optional projection function to be applied to the bias after being updated by an Optimizer . trainable : Boolean, if True the weights of this layer will be marked as trainable (and listed in layer.trainable_weights ). name : A string, the name of the layer. QuantSeparableConv2D \u00b6 QuantSeparableConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 2D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel matrix. pointwise_quantizer : Quantization function applied to the pointwise kernel matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. pointwise_initializer : Initializer for the pointwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. pointwise_constraint : Constraint function applied to the pointwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. QuantDepthwiseConv2D \u00b6 QuantDepthwiseConv2D ( kernel_size , strides = ( 1 , 1 ), padding = 'valid' , depth_multiplier = 1 , data_format = None , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , bias_constraint = None , ** kwargs ) \"Quantized depthwise separable 2D convolution. Depthwise Separable convolutions consists in performing just the first step in a depthwise spatial convolution (which acts on each input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Arguments kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of 'valid' or 'same' (case-insensitive). depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be 'channels_last'. activation : Activation function to use. If you don't specify anything, no activation is applied (ie. a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise_kernel weights matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its 'activation'). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: [batch, channels, rows, cols] if data_format='channels_first' or 4D tensor with shape: [batch, rows, cols, channels] if data_format='channels_last'. Output shape 4D tensor with shape: [batch, filters, new_rows, new_cols] if data_format='channels_first' or 4D tensor with shape: [batch, new_rows, new_cols, filters] if data_format='channels_last'. rows and cols values might have changed due to padding. QuantConv2DTranspose \u00b6 QuantConv2DTranspose ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks QuantConv3DTranspose \u00b6 QuantConv3DTranspose ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format='channels_first' or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format='channels_first' or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format='channels_last'. depth and rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks QuantLocallyConnected1D \u00b6 QuantLocallyConnected1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 1D inputs. The QuantLocallyConnected1D layer works similarly to the QuantConv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected1D . Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Sequential () model . add ( QuantLocallyConnected1D ( 64 , 3 , input_shape = ( 10 , 32 ))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model . add ( QuantLocallyConnected1D ( 32 , 3 )) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides. QuantLocallyConnected2D \u00b6 QuantLocallyConnected2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 2D inputs. The QuantLocallyConnected2D layer works similarly to the QuantConv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected2D . Example # apply a 3x3 unshared weights convolution with 64 output filters on a 32 x32 image # with `data_format=\"channels_last\"`: model = Sequential () model . add ( QuantLocallyConnected2D ( 64 , ( 3 , 3 ), input_shape = ( 32 , 32 , 3 ))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model . add ( QuantLocallyConnected2D ( 32 , ( 3 , 3 ))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"Layers"},{"location":"api/layers/#larqlayers","text":"Each Quantized Layer requires a input_quantizer and kernel_quantizer that describes the way of quantizing the activation of the previous layer and the weights respectively. If both input_quantizer and kernel_quantizer are None the layer is equivalent to a full precision layer.","title":"larq.layers"},{"location":"api/layers/#quantdense","text":"QuantDense ( units , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Just your regular densely-connected quantized NN layer. QuantDense implements the operation: output = activation(dot(input_quantizer(input), kernel_quantizer(kernel)) + bias) , where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Dense . If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Sequential () model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , input_shape = ( 16 ,), ) ) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model . add ( QuantDense ( 32 , input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , ) ) Arguments units : Positive integer, dimensionality of the output space. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel weights matrix. bias_constraint : Constraint function applied to the bias vector. Input shape N-D tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape N-D tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) .","title":"QuantDense"},{"location":"api/layers/#quantconv1d","text":"QuantConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = 'channels_last' , dilation_rate = 1 , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 1D quantized convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv1D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for variable-length sequences of 128-dimensional vectors. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1 :]. Useful when modeling temporal data where the model should not violate the temporal order. See [WaveNet : A Generative Model for Raw Audio, section 2.1](https ://arxiv.org/abs/1609.03499). data_format : A string, one of channels_last (default) or channels_first . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) . steps value might have changed due to padding or strides.","title":"QuantConv1D"},{"location":"api/layers/#quantconv2d","text":"QuantConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 2D quantized convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantConv2D"},{"location":"api/layers/#quantconv3d","text":"QuantConv3D ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3D . If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format='channels_first' or 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format='channels_last'. new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding.","title":"QuantConv3D"},{"location":"api/layers/#quantseparableconv1d","text":"QuantSeparableConv1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , dilation_rate = 1 , depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 1D quantized convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of filters in the convolution). kernel_size : A single integer specifying the spatial dimensions of the filters. strides : A single integer specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"same\" , or \"causal\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . dilation_rate : A single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier . activation : Activation function. Set it to None to maintain a linear activation. use_bias : Boolean, whether the layer uses a bias. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel. pointwise_quantizer : Quantization function applied to the pointwise kernel. depthwise_initializer : An initializer for the depthwise convolution kernel. pointwise_initializer : An initializer for the pointwise convolution kernel. bias_initializer : An initializer for the bias vector. If None, the default initializer will be used. depthwise_regularizer : Optional regularizer for the depthwise convolution kernel. pointwise_regularizer : Optional regularizer for the pointwise convolution kernel. bias_regularizer : Optional regularizer for the bias vector. activity_regularizer : Optional regularizer function for the output. depthwise_constraint : Optional projection function to be applied to the depthwise kernel after being updated by an Optimizer (e.g. used for norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. pointwise_constraint : Optional projection function to be applied to the pointwise kernel after being updated by an Optimizer . bias_constraint : Optional projection function to be applied to the bias after being updated by an Optimizer . trainable : Boolean, if True the weights of this layer will be marked as trainable (and listed in layer.trainable_weights ). name : A string, the name of the layer.","title":"QuantSeparableConv1D"},{"location":"api/layers/#quantseparableconv2d","text":"QuantSeparableConv2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , dilation_rate = ( 1 , 1 ), depth_multiplier = 1 , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , pointwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , pointwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , pointwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , pointwise_constraint = None , bias_constraint = None , ** kwargs ) Depthwise separable 2D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. input_quantizer , depthwise_quantizer and pointwise_quantizer are the element-wise quantization functions to use. If all quantization functions are None this layer is equivalent to SeparableConv1D . If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise kernel matrix. pointwise_quantizer : Quantization function applied to the pointwise kernel matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. pointwise_initializer : Initializer for the pointwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. pointwise_constraint : Constraint function applied to the pointwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantSeparableConv2D"},{"location":"api/layers/#quantdepthwiseconv2d","text":"QuantDepthwiseConv2D ( kernel_size , strides = ( 1 , 1 ), padding = 'valid' , depth_multiplier = 1 , data_format = None , activation = None , use_bias = True , input_quantizer = None , depthwise_quantizer = None , depthwise_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , depthwise_regularizer = None , bias_regularizer = None , activity_regularizer = None , depthwise_constraint = None , bias_constraint = None , ** kwargs ) \"Quantized depthwise separable 2D convolution. Depthwise Separable convolutions consists in performing just the first step in a depthwise spatial convolution (which acts on each input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Arguments kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of 'valid' or 'same' (case-insensitive). depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be 'channels_last'. activation : Activation function to use. If you don't specify anything, no activation is applied (ie. a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. depthwise_quantizer : Quantization function applied to the depthwise_kernel weights matrix. depthwise_initializer : Initializer for the depthwise kernel matrix. bias_initializer : Initializer for the bias vector. depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its 'activation'). depthwise_constraint : Constraint function applied to the depthwise kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: [batch, channels, rows, cols] if data_format='channels_first' or 4D tensor with shape: [batch, rows, cols, channels] if data_format='channels_last'. Output shape 4D tensor with shape: [batch, filters, new_rows, new_cols] if data_format='channels_first' or 4D tensor with shape: [batch, new_rows, new_cols, filters] if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantDepthwiseConv2D"},{"location":"api/layers/#quantconv2dtranspose","text":"QuantConv2DTranspose ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , dilation_rate = ( 1 , 1 ), activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv2DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks","title":"QuantConv2DTranspose"},{"location":"api/layers/#quantconv3dtranspose","text":"QuantConv3DTranspose ( filters , kernel_size , strides = ( 1 , 1 , 1 ), padding = 'valid' , output_padding = None , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) Transposed quantized convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to Conv3DTranspose . When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format='channels_first' or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format='channels_last'. Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format='channels_first' or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format='channels_last'. depth and rows and cols values might have changed due to padding. References A guide to convolution arithmetic for deep learning Deconvolutional Networks","title":"QuantConv3DTranspose"},{"location":"api/layers/#quantlocallyconnected1d","text":"QuantLocallyConnected1D ( filters , kernel_size , strides = 1 , padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 1D inputs. The QuantLocallyConnected1D layer works similarly to the QuantConv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected1D . Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Sequential () model . add ( QuantLocallyConnected1D ( 64 , 3 , input_shape = ( 10 , 32 ))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model . add ( QuantLocallyConnected1D ( 32 , 3 )) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, length, channels) while channels_first corresponds to inputs with shape (batch, channels, length) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides.","title":"QuantLocallyConnected1D"},{"location":"api/layers/#quantlocallyconnected2d","text":"QuantLocallyConnected2D ( filters , kernel_size , strides = ( 1 , 1 ), padding = 'valid' , data_format = None , activation = None , use_bias = True , input_quantizer = None , kernel_quantizer = None , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , implementation = 1 , ** kwargs ) Locally-connected quantized layer for 2D inputs. The QuantLocallyConnected2D layer works similarly to the QuantConv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. input_quantizer and kernel_quantizer are the element-wise quantization functions to use. If both quantization functions are None this layer is equivalent to LocallyConnected2D . Example # apply a 3x3 unshared weights convolution with 64 output filters on a 32 x32 image # with `data_format=\"channels_last\"`: model = Sequential () model . add ( QuantLocallyConnected2D ( 64 , ( 3 , 3 ), input_shape = ( 32 , 32 , 3 ))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model . add ( QuantLocallyConnected2D ( 32 , ( 3 , 3 ))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". activation : Activation function to use. If you don't specify anything, no activation is applied ( a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. input_quantizer : Quantization function applied to the input of the layer. kernel_quantizer : Quantization function applied to the kernel weights matrix. kernel_initializer : Initializer for the kernel weights matrix. bias_initializer : Initializer for the bias vector. kernel_regularizer : Regularizer function applied to the kernel weights matrix. bias_regularizer : Regularizer function applied to the bias vector. activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). kernel_constraint : Constraint function applied to the kernel matrix. bias_constraint : Constraint function applied to the bias vector. implementation : implementation mode, either 1 or 2 . 1 loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops. 2 stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops. Depending on the inputs, layer parameters, hardware, and tf.executing_eagerly() one implementation can be dramatically faster (e.g. 50X) than another. It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Following scenarios could benefit from setting implementation=2 : eager execution; inference; running on CPU; large amount of RAM available; small models (few filters, small kernel); using padding=same (only possible with implementation=2 ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"QuantLocallyConnected2D"},{"location":"api/math/","text":"larq.math \u00b6 Math operations that are specific to extremely quantized networks. sign \u00b6 sign ( x ) A sign function that will never be zero \\[ f(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] Arguments x : Input Tensor Returns A Tensor with same type as x .","title":"Math"},{"location":"api/math/#larqmath","text":"Math operations that are specific to extremely quantized networks.","title":"larq.math"},{"location":"api/math/#sign","text":"sign ( x ) A sign function that will never be zero \\[ f(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] Arguments x : Input Tensor Returns A Tensor with same type as x .","title":"sign"},{"location":"api/models/","text":"larq.models \u00b6 summary \u00b6 summary ( model , print_fn = None ) Prints a string summary of the network. Arguments model : tf.keras model instance. print_fn : Print function to use. Defaults to print . You can set it to a custom function in order to capture the string summary. Raises ValueError : if called before the model is built.","title":"Models"},{"location":"api/models/#larqmodels","text":"","title":"larq.models"},{"location":"api/models/#summary","text":"summary ( model , print_fn = None ) Prints a string summary of the network. Arguments model : tf.keras model instance. print_fn : Print function to use. Defaults to print . You can set it to a custom function in order to capture the string summary. Raises ValueError : if called before the model is built.","title":"summary"},{"location":"api/optimizers/","text":"larq.optimizers \u00b6 Bop \u00b6 Bop ( fp_optimizer , threshold = 1e-05 , gamma = 0.01 , name = 'Bop' , ** kwargs ) Binary optimizer (Bop). Bop is a latent-free optimizer for Binarized Neural Networks (BNNs). Example optimizer = lq . optimizers . Bop ( fp_optimizer = tf . keras . optimizers . Adam ( 0.01 )) Arguments fp_optimizer : a tf.keras.optimizers.Optimizer . threshold : determines to whether to flip each weight. gamma : the adaptivity rate. name : name of the optimizer. References Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization XavierLearningRateScaling \u00b6 XavierLearningRateScaling ( optimizer , model ) Optimizer wrapper for Xavier Learning Rate Scaling Scale the weights learning rates respectively with the weights initialization This is a wrapper and does not implement any optimization algorithm. Example optimizer = lq . optimizers . XavierLearningRateScaling ( tf . keras . optimizers . Adam ( 0.01 ), model ) Arguments optimizer : A tf.keras.optimizers.Optimizer model : A tf.keras.Model References BinaryConnect: Training Deep Neural Networks with binary weights during propagations","title":"Optimizers"},{"location":"api/optimizers/#larqoptimizers","text":"","title":"larq.optimizers"},{"location":"api/optimizers/#bop","text":"Bop ( fp_optimizer , threshold = 1e-05 , gamma = 0.01 , name = 'Bop' , ** kwargs ) Binary optimizer (Bop). Bop is a latent-free optimizer for Binarized Neural Networks (BNNs). Example optimizer = lq . optimizers . Bop ( fp_optimizer = tf . keras . optimizers . Adam ( 0.01 )) Arguments fp_optimizer : a tf.keras.optimizers.Optimizer . threshold : determines to whether to flip each weight. gamma : the adaptivity rate. name : name of the optimizer. References Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization","title":"Bop"},{"location":"api/optimizers/#xavierlearningratescaling","text":"XavierLearningRateScaling ( optimizer , model ) Optimizer wrapper for Xavier Learning Rate Scaling Scale the weights learning rates respectively with the weights initialization This is a wrapper and does not implement any optimization algorithm. Example optimizer = lq . optimizers . XavierLearningRateScaling ( tf . keras . optimizers . Adam ( 0.01 ), model ) Arguments optimizer : A tf.keras.optimizers.Optimizer model : A tf.keras.Model References BinaryConnect: Training Deep Neural Networks with binary weights during propagations","title":"XavierLearningRateScaling"},{"location":"api/quantizers/","text":"larq.quantizers \u00b6 A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass. ste_sign \u00b6 ste_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the Straight-Through Estimator (essentially the binarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 approx_sign \u00b6 approx_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the ApproxSign method. \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} (2 - 2 \\left|x\\right|) & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} \\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.5 1.0 1.5 2.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm magnitude_aware_sign \u00b6 magnitude_aware_sign ( x ) Magnitude-aware sign for Bi-Real Net. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22122 \u22121 0 1 2 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor Returns Scaled binarized tensor (with values in \\{-a, a\\} \\{-a, a\\} , where a a is a float). References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm SteTern \u00b6 SteTern ( threshold_value = 0.1 , ternary_weight_networks = False ) Ternarization function. \\[ q(x) = \\begin{cases} +1 & x > \\Delta \\\\ 0 & |x| > \\Delta \\\\ -1 & x < \\Delta \\end{cases} \\] where \\Delta \\Delta is defined as the threshold and can be passed as an argument, or can be calculated as per the Ternary Weight Networks original paper, such that \\[ \\Delta = \\frac{0.7}{n} \\sum_{i=1}^{n} |W_i| \\] where we assume that W_i W_i is generated from a normal distribution. The gradient is estimated using the Straight-Through Estimator (essentially the Ternarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. threshold_value : The value for the threshold, \\Delta \\Delta . ternary_weight_networks : Boolean of whether to use the Ternary Weight Networks threshold calculation. Returns Ternarized tensor. References Ternary Weight Networks","title":"Quantizers"},{"location":"api/quantizers/#larqquantizers","text":"A Quantizer defines the way of transforming a full precision input to a quantized output and the pseudo-gradient method used for the backwards pass.","title":"larq.quantizers"},{"location":"api/quantizers/#ste_sign","text":"ste_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the Straight-Through Estimator (essentially the binarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1","title":"ste_sign"},{"location":"api/quantizers/#approx_sign","text":"approx_sign ( x ) Sign binarization function. \\[ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} \\] The gradient is estimated using the ApproxSign method. \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} (2 - 2 \\left|x\\right|) & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} \\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.5 1.0 1.5 2.0 dy / dx Backward pass Arguments x : Input tensor. Returns Binarized tensor. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"approx_sign"},{"location":"api/quantizers/#magnitude_aware_sign","text":"magnitude_aware_sign ( x ) Magnitude-aware sign for Bi-Real Net. *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22122 \u22121 0 1 2 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor Returns Scaled binarized tensor (with values in \\{-a, a\\} \\{-a, a\\} , where a a is a float). References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"magnitude_aware_sign"},{"location":"api/quantizers/#stetern","text":"SteTern ( threshold_value = 0.1 , ternary_weight_networks = False ) Ternarization function. \\[ q(x) = \\begin{cases} +1 & x > \\Delta \\\\ 0 & |x| > \\Delta \\\\ -1 & x < \\Delta \\end{cases} \\] where \\Delta \\Delta is defined as the threshold and can be passed as an argument, or can be calculated as per the Ternary Weight Networks original paper, such that \\[ \\Delta = \\frac{0.7}{n} \\sum_{i=1}^{n} |W_i| \\] where we assume that W_i W_i is generated from a normal distribution. The gradient is estimated using the Straight-Through Estimator (essentially the Ternarization is replaced by a clipped identity on the backward pass). \\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases}\\] *{stroke-linecap:butt;stroke-linejoin:round;} \u22122 \u22121 0 1 2 x \u22121.0 \u22120.5 0.0 0.5 1.0 y Forward pass \u22122 \u22121 0 1 2 x 0.0 0.2 0.4 0.6 0.8 1.0 dy / dx Backward pass Arguments x : Input tensor. threshold_value : The value for the threshold, \\Delta \\Delta . ternary_weight_networks : Boolean of whether to use the Ternary Weight Networks threshold calculation. Returns Ternarized tensor. References Ternary Weight Networks","title":"SteTern"},{"location":"api/zoo/","text":"larq_zoo \u00b6 BinaryAlexNet \u00b6 BinaryAlexNet ( include_top = True , weights = 'imagenet' , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the BinaryAlexNet architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. BiRealNet \u00b6 BiRealNet ( include_top = True , weights = 'imagenet' , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the Bi-Real Net architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm wrapper \u00b6 wrapper ( * args , ** kwargs ) preprocess_input \u00b6 preprocess_input ( image ) Preprocesses a Tensor or Numpy array encoding a image. Arguments image : Numpy array or symbolic Tensor, 3D. Returns Preprocessed Tensor or Numpy array.","title":"larq_zoo"},{"location":"api/zoo/#larq_zoo","text":"","title":"larq_zoo"},{"location":"api/zoo/#binaryalexnet","text":"BinaryAlexNet ( include_top = True , weights = 'imagenet' , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the BinaryAlexNet architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape.","title":"BinaryAlexNet"},{"location":"api/zoo/#birealnet","text":"BiRealNet ( include_top = True , weights = 'imagenet' , input_tensor = None , input_shape = None , classes = 1000 ) Instantiates the Bi-Real Net architecture. Optionally loads weights pre-trained on ImageNet. Arguments include_top : whether to include the fully-connected layer at the top of the network. weights : one of None (random initialization), \"imagenet\" (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor : optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape : optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Keras model instance. Raises ValueError : in case of invalid argument for weights , or invalid input shape. References Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm","title":"BiRealNet"},{"location":"api/zoo/#wrapper","text":"wrapper ( * args , ** kwargs )","title":"wrapper"},{"location":"api/zoo/#preprocess_input","text":"preprocess_input ( image ) Preprocesses a Tensor or Numpy array encoding a image. Arguments image : Numpy array or symbolic Tensor, 3D. Returns Preprocessed Tensor or Numpy array.","title":"preprocess_input"},{"location":"examples/binarynet_advanced_cifar10/","text":"BinaryNet on CIFAR10 (Advanced) \u00b6 Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy of around 90% using a heavy lifting GPU like a Nvidia V100. On a Nvidia V100 it takes approximately 250 minutes to train. Compared to the original papers, BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not implement image whitening, but we use image augmentation, and a stepped learning rate scheduler. import tensorflow as tf import larq as lq import numpy as np Import CIFAR10 Dataset \u00b6 Here we download the CIFAR10 dataset: train_data , test_data = tf . keras . datasets . cifar10 . load_data () Next, we define our image augmentation technqiues, and create the dataset: def resize_and_flip ( image , labels , training ): image = tf . cast ( image , tf . float32 ) / ( 255. / 2. ) - 1. if training : image = tf . image . resize_image_with_crop_or_pad ( image , 40 , 40 ) image = tf . random_crop ( image , [ 32 , 32 , 3 ]) image = tf . image . random_flip_left_right ( image ) return image , labels def create_dataset ( data , batch_size , training ): images , labels = data labels = tf . one_hot ( np . squeeze ( labels ), 10 ) dataset = tf . data . Dataset . from_tensor_slices (( images , labels )) dataset = dataset . repeat () if training : dataset = dataset . shuffle ( 1000 ) dataset = dataset . map ( lambda x , y : resize_and_flip ( x , y , training )) dataset = dataset . batch ( batch_size ) return dataset batch_size = 50 train_dataset = create_dataset ( train_data , batch_size , True ) test_dataset = create_dataset ( test_data , batch_size , False ) Build BinaryNet \u00b6 Here we build the binarynet model layer by layer using a keras sequential model: # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) Larq allows you to print a summary of the model that includes bit-precision information: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+ Model Training \u00b6 We compile and train the model as you are used to in Keras: initial_lr = 1e-3 var_decay = 1e-5 optimizer = tf . keras . optimizers . Adam ( lr = initial_lr , decay = var_decay ) model . compile ( optimizer = lq . optimizers . XavierLearningRateScaling ( optimizer , model ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) def lr_schedule ( epoch ): return initial_lr * 0.1 ** ( epoch // 100 ) trained_model = model . fit ( train_dataset , epochs = 500 , steps_per_epoch = train_data [ 1 ] . shape [ 0 ] // batch_size , validation_data = test_dataset , validation_steps = test_data [ 1 ] . shape [ 0 ] // batch_size , verbose = 1 , callbacks = [ tf . keras . callbacks . LearningRateScheduler ( lr_schedule )] )","title":"BinaryNet on CIFAR10 (Advanced)"},{"location":"examples/binarynet_advanced_cifar10/#binarynet-on-cifar10-advanced","text":"Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy of around 90% using a heavy lifting GPU like a Nvidia V100. On a Nvidia V100 it takes approximately 250 minutes to train. Compared to the original papers, BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not implement image whitening, but we use image augmentation, and a stepped learning rate scheduler. import tensorflow as tf import larq as lq import numpy as np","title":"BinaryNet on CIFAR10 (Advanced)"},{"location":"examples/binarynet_advanced_cifar10/#import-cifar10-dataset","text":"Here we download the CIFAR10 dataset: train_data , test_data = tf . keras . datasets . cifar10 . load_data () Next, we define our image augmentation technqiues, and create the dataset: def resize_and_flip ( image , labels , training ): image = tf . cast ( image , tf . float32 ) / ( 255. / 2. ) - 1. if training : image = tf . image . resize_image_with_crop_or_pad ( image , 40 , 40 ) image = tf . random_crop ( image , [ 32 , 32 , 3 ]) image = tf . image . random_flip_left_right ( image ) return image , labels def create_dataset ( data , batch_size , training ): images , labels = data labels = tf . one_hot ( np . squeeze ( labels ), 10 ) dataset = tf . data . Dataset . from_tensor_slices (( images , labels )) dataset = dataset . repeat () if training : dataset = dataset . shuffle ( 1000 ) dataset = dataset . map ( lambda x , y : resize_and_flip ( x , y , training )) dataset = dataset . batch ( batch_size ) return dataset batch_size = 50 train_dataset = create_dataset ( train_data , batch_size , True ) test_dataset = create_dataset ( test_data , batch_size , False )","title":"Import CIFAR10 Dataset"},{"location":"examples/binarynet_advanced_cifar10/#build-binarynet","text":"Here we build the binarynet model layer by layer using a keras sequential model: # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) Larq allows you to print a summary of the model that includes bit-precision information: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+","title":"Build BinaryNet"},{"location":"examples/binarynet_advanced_cifar10/#model-training","text":"We compile and train the model as you are used to in Keras: initial_lr = 1e-3 var_decay = 1e-5 optimizer = tf . keras . optimizers . Adam ( lr = initial_lr , decay = var_decay ) model . compile ( optimizer = lq . optimizers . XavierLearningRateScaling ( optimizer , model ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) def lr_schedule ( epoch ): return initial_lr * 0.1 ** ( epoch // 100 ) trained_model = model . fit ( train_dataset , epochs = 500 , steps_per_epoch = train_data [ 1 ] . shape [ 0 ] // batch_size , validation_data = test_dataset , validation_steps = test_data [ 1 ] . shape [ 0 ] // batch_size , verbose = 1 , callbacks = [ tf . keras . callbacks . LearningRateScheduler ( lr_schedule )] )","title":"Model Training"},{"location":"examples/binarynet_cifar10/","text":"BinaryNet on CIFAR10 \u00b6 Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy approximately 83% on laptop hardware. On a Nvidia GTX 1050 Ti Max-Q it takes approximately 200 minutes to train. For simplicity, compared to the original papers BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not impliment learning rate scaling, or image whitening. import tensorflow as tf import larq as lq import numpy as np import matplotlib.pyplot as plt Import CIFAR10 Dataset \u00b6 We download and normalize the CIFAR10 dataset. num_classes = 10 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . cifar10 . load_data () train_images = train_images . reshape (( 50000 , 32 , 32 , 3 )) . astype ( \"float32\" ) test_images = test_images . reshape (( 10000 , 32 , 32 , 3 )) . astype ( \"float32\" ) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1 train_labels = tf . keras . utils . to_categorical ( train_labels , num_classes ) test_labels = tf . keras . utils . to_categorical ( test_labels , num_classes ) Build BinaryNet \u00b6 Here we build the BinaryNet model layer by layer using the Keras Sequential API . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) One can output a summary of the model: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+ Model Training \u00b6 Compile the model and train the model model . compile ( tf . keras . optimizers . Adam ( lr = 0.01 , decay = 0.0001 ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) trained_model = model . fit ( train_images , train_labels , batch_size = 50 , epochs = 100 , validation_data = ( test_images , test_labels ), shuffle = True ) Train on 50000 samples , validate on 10000 samples Epoch 1 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 1 . 5733 - acc : 0 . 4533 - val_loss : 1 . 6368 - val_acc : 0 . 4244 Epoch 2 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 1 . 1485 - acc : 0 . 6387 - val_loss : 1 . 8497 - val_acc : 0 . 3764 Epoch 3 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 9641 - acc : 0 . 7207 - val_loss : 1 . 5696 - val_acc : 0 . 4794 Epoch 4 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 8452 - acc : 0 . 7728 - val_loss : 1 . 5765 - val_acc : 0 . 4669 Epoch 5 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 7553 - acc : 0 . 8114 - val_loss : 1 . 0653 - val_acc : 0 . 6928 Epoch 6 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 6841 - acc : 0 . 8447 - val_loss : 1 . 0944 - val_acc : 0 . 6880 Epoch 7 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 6356 - acc : 0 . 8685 - val_loss : 0 . 9909 - val_acc : 0 . 7317 Epoch 8 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5907 - acc : 0 . 8910 - val_loss : 0 . 9453 - val_acc : 0 . 7446 Epoch 9 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5610 - acc : 0 . 9043 - val_loss : 0 . 9441 - val_acc : 0 . 7460 Epoch 10 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 5295 - acc : 0 . 9201 - val_loss : 0 . 8892 - val_acc : 0 . 7679 Epoch 11 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 5100 - acc : 0 . 9309 - val_loss : 0 . 8808 - val_acc : 0 . 7818 Epoch 12 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4926 - acc : 0 . 9397 - val_loss : 0 . 8404 - val_acc : 0 . 7894 Epoch 13 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4807 - acc : 0 . 9470 - val_loss : 0 . 8600 - val_acc : 0 . 7928 Epoch 14 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4661 - acc : 0 . 9529 - val_loss : 0 . 9046 - val_acc : 0 . 7732 Epoch 15 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 4588 - acc : 0 . 9571 - val_loss : 0 . 8505 - val_acc : 0 . 7965 Epoch 16 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4558 - acc : 0 . 9593 - val_loss : 0 . 8748 - val_acc : 0 . 7859 Epoch 17 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4434 - acc : 0 . 9649 - val_loss : 0 . 9109 - val_acc : 0 . 7656 Epoch 18 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4449 - acc : 0 . 9643 - val_loss : 0 . 8532 - val_acc : 0 . 7971 Epoch 19 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4349 - acc : 0 . 9701 - val_loss : 0 . 8677 - val_acc : 0 . 7951 Epoch 20 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4351 - acc : 0 . 9698 - val_loss : 0 . 9145 - val_acc : 0 . 7740 Epoch 21 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4268 - acc : 0 . 9740 - val_loss : 0 . 8308 - val_acc : 0 . 8065 Epoch 22 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4243 - acc : 0 . 9741 - val_loss : 0 . 8229 - val_acc : 0 . 8075 Epoch 23 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4201 - acc : 0 . 9764 - val_loss : 0 . 8411 - val_acc : 0 . 8062 Epoch 24 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4190 - acc : 0 . 9769 - val_loss : 0 . 8649 - val_acc : 0 . 7951 Epoch 25 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4139 - acc : 0 . 9787 - val_loss : 0 . 8257 - val_acc : 0 . 8071 Epoch 26 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4154 - acc : 0 . 9779 - val_loss : 0 . 8041 - val_acc : 0 . 8205 Epoch 27 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4128 - acc : 0 . 9798 - val_loss : 0 . 8296 - val_acc : 0 . 8115 Epoch 28 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4121 - acc : 0 . 9798 - val_loss : 0 . 8241 - val_acc : 0 . 8074 Epoch 29 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4093 - acc : 0 . 9807 - val_loss : 0 . 8575 - val_acc : 0 . 7913 Epoch 30 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4048 - acc : 0 . 9826 - val_loss : 0 . 8118 - val_acc : 0 . 8166 Epoch 31 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4041 - acc : 0 . 9837 - val_loss : 0 . 8375 - val_acc : 0 . 8082 Epoch 32 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4045 - acc : 0 . 9831 - val_loss : 0 . 8604 - val_acc : 0 . 8091 Epoch 33 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4047 - acc : 0 . 9823 - val_loss : 0 . 8797 - val_acc : 0 . 7931 Epoch 34 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4023 - acc : 0 . 9842 - val_loss : 0 . 8694 - val_acc : 0 . 8020 Epoch 35 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3995 - acc : 0 . 9858 - val_loss : 0 . 8161 - val_acc : 0 . 8186 Epoch 36 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3976 - acc : 0 . 9859 - val_loss : 0 . 8495 - val_acc : 0 . 7988 Epoch 37 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4021 - acc : 0 . 9847 - val_loss : 0 . 8542 - val_acc : 0 . 8062 Epoch 38 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3939 - acc : 0 . 9869 - val_loss : 0 . 8347 - val_acc : 0 . 8122 Epoch 39 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3955 - acc : 0 . 9856 - val_loss : 0 . 8521 - val_acc : 0 . 7993 Epoch 40 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3907 - acc : 0 . 9885 - val_loss : 0 . 9023 - val_acc : 0 . 7992 Epoch 41 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3911 - acc : 0 . 9873 - val_loss : 0 . 8597 - val_acc : 0 . 8010 Epoch 42 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3917 - acc : 0 . 9885 - val_loss : 0 . 8968 - val_acc : 0 . 7936 Epoch 43 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3931 - acc : 0 . 9874 - val_loss : 0 . 8318 - val_acc : 0 . 8169 Epoch 44 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3897 - acc : 0 . 9893 - val_loss : 0 . 8811 - val_acc : 0 . 7988 Epoch 45 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9888 - val_loss : 0 . 8453 - val_acc : 0 . 8094 Epoch 46 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9889 - val_loss : 0 . 8195 - val_acc : 0 . 8179 Epoch 47 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3891 - acc : 0 . 9890 - val_loss : 0 . 8373 - val_acc : 0 . 8137 Epoch 48 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3902 - acc : 0 . 9888 - val_loss : 0 . 8457 - val_acc : 0 . 8120 Epoch 49 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3864 - acc : 0 . 9903 - val_loss : 0 . 9012 - val_acc : 0 . 7907 Epoch 50 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3859 - acc : 0 . 9903 - val_loss : 0 . 8291 - val_acc : 0 . 8053 Epoch 51 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3830 - acc : 0 . 9915 - val_loss : 0 . 8494 - val_acc : 0 . 8139 Epoch 52 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3828 - acc : 0 . 9907 - val_loss : 0 . 8447 - val_acc : 0 . 8135 Epoch 53 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9910 - val_loss : 0 . 8539 - val_acc : 0 . 8120 Epoch 54 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3832 - acc : 0 . 9905 - val_loss : 0 . 8592 - val_acc : 0 . 8098 Epoch 55 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9908 - val_loss : 0 . 8585 - val_acc : 0 . 8087 Epoch 56 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3817 - acc : 0 . 9911 - val_loss : 0 . 8840 - val_acc : 0 . 7889 Epoch 57 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3827 - acc : 0 . 9914 - val_loss : 0 . 8205 - val_acc : 0 . 8250 Epoch 58 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3818 - acc : 0 . 9912 - val_loss : 0 . 8571 - val_acc : 0 . 8051 Epoch 59 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3811 - acc : 0 . 9919 - val_loss : 0 . 8155 - val_acc : 0 . 8254 Epoch 60 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3803 - acc : 0 . 9919 - val_loss : 0 . 8617 - val_acc : 0 . 8040 Epoch 61 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3793 - acc : 0 . 9926 - val_loss : 0 . 8212 - val_acc : 0 . 8192 Epoch 62 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3825 - acc : 0 . 9912 - val_loss : 0 . 8139 - val_acc : 0 . 8277 Epoch 63 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9923 - val_loss : 0 . 8304 - val_acc : 0 . 8121 Epoch 64 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3809 - acc : 0 . 9918 - val_loss : 0 . 7961 - val_acc : 0 . 8289 Epoch 65 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3750 - acc : 0 . 9930 - val_loss : 0 . 8676 - val_acc : 0 . 8110 Epoch 66 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3789 - acc : 0 . 9928 - val_loss : 0 . 8308 - val_acc : 0 . 8148 Epoch 67 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3783 - acc : 0 . 9929 - val_loss : 0 . 8595 - val_acc : 0 . 8097 Epoch 68 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9935 - val_loss : 0 . 8359 - val_acc : 0 . 8065 Epoch 69 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9927 - val_loss : 0 . 8189 - val_acc : 0 . 8255 Epoch 70 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3786 - acc : 0 . 9924 - val_loss : 0 . 8754 - val_acc : 0 . 8001 Epoch 71 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9936 - val_loss : 0 . 8188 - val_acc : 0 . 8262 Epoch 72 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9932 - val_loss : 0 . 8540 - val_acc : 0 . 8169 Epoch 73 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3740 - acc : 0 . 9934 - val_loss : 0 . 8127 - val_acc : 0 . 8258 Epoch 74 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9932 - val_loss : 0 . 8662 - val_acc : 0 . 8018 Epoch 75 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3721 - acc : 0 . 9941 - val_loss : 0 . 8359 - val_acc : 0 . 8213 Epoch 76 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3746 - acc : 0 . 9937 - val_loss : 0 . 8462 - val_acc : 0 . 8178 Epoch 77 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3741 - acc : 0 . 9936 - val_loss : 0 . 8983 - val_acc : 0 . 7972 Epoch 78 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3751 - acc : 0 . 9933 - val_loss : 0 . 8525 - val_acc : 0 . 8173 Epoch 79 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3762 - acc : 0 . 9931 - val_loss : 0 . 8190 - val_acc : 0 . 8201 Epoch 80 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3737 - acc : 0 . 9940 - val_loss : 0 . 8441 - val_acc : 0 . 8196 Epoch 81 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3729 - acc : 0 . 9935 - val_loss : 0 . 8151 - val_acc : 0 . 8267 Epoch 82 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3735 - acc : 0 . 9938 - val_loss : 0 . 8405 - val_acc : 0 . 8163 Epoch 83 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3723 - acc : 0 . 9939 - val_loss : 0 . 8225 - val_acc : 0 . 8243 Epoch 84 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3738 - acc : 0 . 9938 - val_loss : 0 . 8413 - val_acc : 0 . 8115 Epoch 85 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3714 - acc : 0 . 9947 - val_loss : 0 . 9080 - val_acc : 0 . 7932 Epoch 86 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3744 - acc : 0 . 9942 - val_loss : 0 . 8467 - val_acc : 0 . 8135 Epoch 87 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3705 - acc : 0 . 9948 - val_loss : 0 . 8491 - val_acc : 0 . 8163 Epoch 88 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3733 - acc : 0 . 9944 - val_loss : 0 . 8005 - val_acc : 0 . 8214 Epoch 89 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3693 - acc : 0 . 9949 - val_loss : 0 . 7791 - val_acc : 0 . 8321 Epoch 90 / 100 50000 / 50000 [ ============================== ] - 135 s 3 ms / step - loss : 0 . 3724 - acc : 0 . 9942 - val_loss : 0 . 8458 - val_acc : 0 . 8124 Epoch 91 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3732 - acc : 0 . 9947 - val_loss : 0 . 8315 - val_acc : 0 . 8164 Epoch 92 / 100 50000 / 50000 [ ============================== ] - 127 s 3 ms / step - loss : 0 . 3699 - acc : 0 . 9950 - val_loss : 0 . 8140 - val_acc : 0 . 8226 Epoch 93 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 0 . 3694 - acc : 0 . 9950 - val_loss : 0 . 8342 - val_acc : 0 . 8210 Epoch 94 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8938 - val_acc : 0 . 8019 Epoch 95 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8771 - val_acc : 0 . 8066 Epoch 96 / 100 50000 / 50000 [ ============================== ] - 164 s 3 ms / step - loss : 0 . 3712 - acc : 0 . 9946 - val_loss : 0 . 8396 - val_acc : 0 . 8211 Epoch 97 / 100 50000 / 50000 [ ============================== ] - 155 s 3 ms / step - loss : 0 . 3689 - acc : 0 . 9949 - val_loss : 0 . 8728 - val_acc : 0 . 8112 Epoch 98 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3663 - acc : 0 . 9953 - val_loss : 0 . 9615 - val_acc : 0 . 7902 Epoch 99 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3714 - acc : 0 . 9944 - val_loss : 0 . 8414 - val_acc : 0 . 8188 Epoch 100 / 100 50000 / 50000 [ ============================== ] - 138 s 3 ms / step - loss : 0 . 3682 - acc : 0 . 9956 - val_loss : 0 . 8055 - val_acc : 0 . 8266 Model Output \u00b6 We can now plot the final validation accuracy and loss: plt . plot ( trained_model . history [ 'acc' ]) plt . plot ( trained_model . history [ 'val_acc' ]) plt . title ( 'model accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . max ( trained_model . history [ 'acc' ])) print ( np . max ( trained_model . history [ 'val_acc' ])) 0 . 9956000019311905 0 . 8320999944210052 plt . plot ( trained_model . history [ 'loss' ]) plt . plot ( trained_model . history [ 'val_loss' ]) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . min ( trained_model . history [ 'loss' ])) print ( np . min ( trained_model . history [ 'val_loss' ])) 0 . 3663262344896793 0 . 7790719392895699","title":"BinaryNet on CIFAR10"},{"location":"examples/binarynet_cifar10/#binarynet-on-cifar10","text":"Run on Binder View on GitHub In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy approximately 83% on laptop hardware. On a Nvidia GTX 1050 Ti Max-Q it takes approximately 200 minutes to train. For simplicity, compared to the original papers BinaryConnect: Training Deep Neural Networks with binary weights during propagations , and Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 , we do not impliment learning rate scaling, or image whitening. import tensorflow as tf import larq as lq import numpy as np import matplotlib.pyplot as plt","title":"BinaryNet on CIFAR10"},{"location":"examples/binarynet_cifar10/#import-cifar10-dataset","text":"We download and normalize the CIFAR10 dataset. num_classes = 10 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . cifar10 . load_data () train_images = train_images . reshape (( 50000 , 32 , 32 , 3 )) . astype ( \"float32\" ) test_images = test_images . reshape (( 10000 , 32 , 32 , 3 )) . astype ( \"float32\" ) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1 train_labels = tf . keras . utils . to_categorical ( train_labels , num_classes ) test_labels = tf . keras . utils . to_categorical ( test_labels , num_classes )","title":"Import CIFAR10 Dataset"},{"location":"examples/binarynet_cifar10/#build-binarynet","text":"Here we build the BinaryNet model layer by layer using the Keras Sequential API . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False ) model = tf . keras . models . Sequential ([ # In the first layer we only quantize the weights and not the input lq . layers . QuantConv2D ( 128 , 3 , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 32 , 32 , 3 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 128 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 256 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantConv2D ( 512 , 3 , padding = \"same\" , ** kwargs ), tf . keras . layers . MaxPool2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 )), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Flatten (), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 1024 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), lq . layers . QuantDense ( 10 , ** kwargs ), tf . keras . layers . BatchNormalization ( momentum = 0.999 , scale = False ), tf . keras . layers . Activation ( \"softmax\" ) ]) One can output a summary of the model: lq . models . summary ( model ) + sequential_1 stats -------------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + -------------------------------------------------------------------------------------+ | quant_conv2d_6 - ( - 1 , 30 , 30 , 128 ) 3456 0 0 . 42 | | batch_normalization_9 - ( - 1 , 30 , 30 , 128 ) 0 384 1 . 50 | | quant_conv2d_7 1 ( - 1 , 30 , 30 , 128 ) 147456 0 18 . 00 | | max_pooling2d_3 - ( - 1 , 15 , 15 , 128 ) 0 0 0 . 00 | | batch_normalization_10 - ( - 1 , 15 , 15 , 128 ) 0 384 1 . 50 | | quant_conv2d_8 1 ( - 1 , 15 , 15 , 256 ) 294912 0 36 . 00 | | batch_normalization_11 - ( - 1 , 15 , 15 , 256 ) 0 768 3 . 00 | | quant_conv2d_9 1 ( - 1 , 15 , 15 , 256 ) 589824 0 72 . 00 | | max_pooling2d_4 - ( - 1 , 7 , 7 , 256 ) 0 0 0 . 00 | | batch_normalization_12 - ( - 1 , 7 , 7 , 256 ) 0 768 3 . 00 | | quant_conv2d_10 1 ( - 1 , 7 , 7 , 512 ) 1179648 0 144 . 00 | | batch_normalization_13 - ( - 1 , 7 , 7 , 512 ) 0 1536 6 . 00 | | quant_conv2d_11 1 ( - 1 , 7 , 7 , 512 ) 2359296 0 288 . 00 | | max_pooling2d_5 - ( - 1 , 3 , 3 , 512 ) 0 0 0 . 00 | | batch_normalization_14 - ( - 1 , 3 , 3 , 512 ) 0 1536 6 . 00 | | flatten_1 - ( - 1 , 4608 ) 0 0 0 . 00 | | quant_dense_3 1 ( - 1 , 1024 ) 4718592 0 576 . 00 | | batch_normalization_15 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_4 1 ( - 1 , 1024 ) 1048576 0 128 . 00 | | batch_normalization_16 - ( - 1 , 1024 ) 0 3072 12 . 00 | | quant_dense_5 1 ( - 1 , 10 ) 10240 0 1 . 25 | | batch_normalization_17 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + -------------------------------------------------------------------------------------+ | Total 10352000 11550 1308 . 79 | + -------------------------------------------------------------------------------------+ + sequential_1 summary -------------+ | Total params 10363550 | | Trainable params 10355850 | | Non - trainable params 7700 | | Float - 32 Equivalent 39 . 53 MB | | Compression of Memory 30 . 93 | + ---------------------------------+","title":"Build BinaryNet"},{"location":"examples/binarynet_cifar10/#model-training","text":"Compile the model and train the model model . compile ( tf . keras . optimizers . Adam ( lr = 0.01 , decay = 0.0001 ), loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ], ) trained_model = model . fit ( train_images , train_labels , batch_size = 50 , epochs = 100 , validation_data = ( test_images , test_labels ), shuffle = True ) Train on 50000 samples , validate on 10000 samples Epoch 1 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 1 . 5733 - acc : 0 . 4533 - val_loss : 1 . 6368 - val_acc : 0 . 4244 Epoch 2 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 1 . 1485 - acc : 0 . 6387 - val_loss : 1 . 8497 - val_acc : 0 . 3764 Epoch 3 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 9641 - acc : 0 . 7207 - val_loss : 1 . 5696 - val_acc : 0 . 4794 Epoch 4 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 8452 - acc : 0 . 7728 - val_loss : 1 . 5765 - val_acc : 0 . 4669 Epoch 5 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 7553 - acc : 0 . 8114 - val_loss : 1 . 0653 - val_acc : 0 . 6928 Epoch 6 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 6841 - acc : 0 . 8447 - val_loss : 1 . 0944 - val_acc : 0 . 6880 Epoch 7 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 6356 - acc : 0 . 8685 - val_loss : 0 . 9909 - val_acc : 0 . 7317 Epoch 8 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5907 - acc : 0 . 8910 - val_loss : 0 . 9453 - val_acc : 0 . 7446 Epoch 9 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 5610 - acc : 0 . 9043 - val_loss : 0 . 9441 - val_acc : 0 . 7460 Epoch 10 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 5295 - acc : 0 . 9201 - val_loss : 0 . 8892 - val_acc : 0 . 7679 Epoch 11 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 5100 - acc : 0 . 9309 - val_loss : 0 . 8808 - val_acc : 0 . 7818 Epoch 12 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4926 - acc : 0 . 9397 - val_loss : 0 . 8404 - val_acc : 0 . 7894 Epoch 13 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4807 - acc : 0 . 9470 - val_loss : 0 . 8600 - val_acc : 0 . 7928 Epoch 14 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4661 - acc : 0 . 9529 - val_loss : 0 . 9046 - val_acc : 0 . 7732 Epoch 15 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 4588 - acc : 0 . 9571 - val_loss : 0 . 8505 - val_acc : 0 . 7965 Epoch 16 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4558 - acc : 0 . 9593 - val_loss : 0 . 8748 - val_acc : 0 . 7859 Epoch 17 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4434 - acc : 0 . 9649 - val_loss : 0 . 9109 - val_acc : 0 . 7656 Epoch 18 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4449 - acc : 0 . 9643 - val_loss : 0 . 8532 - val_acc : 0 . 7971 Epoch 19 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4349 - acc : 0 . 9701 - val_loss : 0 . 8677 - val_acc : 0 . 7951 Epoch 20 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4351 - acc : 0 . 9698 - val_loss : 0 . 9145 - val_acc : 0 . 7740 Epoch 21 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4268 - acc : 0 . 9740 - val_loss : 0 . 8308 - val_acc : 0 . 8065 Epoch 22 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4243 - acc : 0 . 9741 - val_loss : 0 . 8229 - val_acc : 0 . 8075 Epoch 23 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4201 - acc : 0 . 9764 - val_loss : 0 . 8411 - val_acc : 0 . 8062 Epoch 24 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4190 - acc : 0 . 9769 - val_loss : 0 . 8649 - val_acc : 0 . 7951 Epoch 25 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4139 - acc : 0 . 9787 - val_loss : 0 . 8257 - val_acc : 0 . 8071 Epoch 26 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4154 - acc : 0 . 9779 - val_loss : 0 . 8041 - val_acc : 0 . 8205 Epoch 27 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4128 - acc : 0 . 9798 - val_loss : 0 . 8296 - val_acc : 0 . 8115 Epoch 28 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4121 - acc : 0 . 9798 - val_loss : 0 . 8241 - val_acc : 0 . 8074 Epoch 29 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4093 - acc : 0 . 9807 - val_loss : 0 . 8575 - val_acc : 0 . 7913 Epoch 30 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4048 - acc : 0 . 9826 - val_loss : 0 . 8118 - val_acc : 0 . 8166 Epoch 31 / 100 50000 / 50000 [ ============================== ] - 126 s 3 ms / step - loss : 0 . 4041 - acc : 0 . 9837 - val_loss : 0 . 8375 - val_acc : 0 . 8082 Epoch 32 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 4045 - acc : 0 . 9831 - val_loss : 0 . 8604 - val_acc : 0 . 8091 Epoch 33 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4047 - acc : 0 . 9823 - val_loss : 0 . 8797 - val_acc : 0 . 7931 Epoch 34 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 4023 - acc : 0 . 9842 - val_loss : 0 . 8694 - val_acc : 0 . 8020 Epoch 35 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3995 - acc : 0 . 9858 - val_loss : 0 . 8161 - val_acc : 0 . 8186 Epoch 36 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3976 - acc : 0 . 9859 - val_loss : 0 . 8495 - val_acc : 0 . 7988 Epoch 37 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 4021 - acc : 0 . 9847 - val_loss : 0 . 8542 - val_acc : 0 . 8062 Epoch 38 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3939 - acc : 0 . 9869 - val_loss : 0 . 8347 - val_acc : 0 . 8122 Epoch 39 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3955 - acc : 0 . 9856 - val_loss : 0 . 8521 - val_acc : 0 . 7993 Epoch 40 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3907 - acc : 0 . 9885 - val_loss : 0 . 9023 - val_acc : 0 . 7992 Epoch 41 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3911 - acc : 0 . 9873 - val_loss : 0 . 8597 - val_acc : 0 . 8010 Epoch 42 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3917 - acc : 0 . 9885 - val_loss : 0 . 8968 - val_acc : 0 . 7936 Epoch 43 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3931 - acc : 0 . 9874 - val_loss : 0 . 8318 - val_acc : 0 . 8169 Epoch 44 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3897 - acc : 0 . 9893 - val_loss : 0 . 8811 - val_acc : 0 . 7988 Epoch 45 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9888 - val_loss : 0 . 8453 - val_acc : 0 . 8094 Epoch 46 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3876 - acc : 0 . 9889 - val_loss : 0 . 8195 - val_acc : 0 . 8179 Epoch 47 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3891 - acc : 0 . 9890 - val_loss : 0 . 8373 - val_acc : 0 . 8137 Epoch 48 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3902 - acc : 0 . 9888 - val_loss : 0 . 8457 - val_acc : 0 . 8120 Epoch 49 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3864 - acc : 0 . 9903 - val_loss : 0 . 9012 - val_acc : 0 . 7907 Epoch 50 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3859 - acc : 0 . 9903 - val_loss : 0 . 8291 - val_acc : 0 . 8053 Epoch 51 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3830 - acc : 0 . 9915 - val_loss : 0 . 8494 - val_acc : 0 . 8139 Epoch 52 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3828 - acc : 0 . 9907 - val_loss : 0 . 8447 - val_acc : 0 . 8135 Epoch 53 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9910 - val_loss : 0 . 8539 - val_acc : 0 . 8120 Epoch 54 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3832 - acc : 0 . 9905 - val_loss : 0 . 8592 - val_acc : 0 . 8098 Epoch 55 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3823 - acc : 0 . 9908 - val_loss : 0 . 8585 - val_acc : 0 . 8087 Epoch 56 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3817 - acc : 0 . 9911 - val_loss : 0 . 8840 - val_acc : 0 . 7889 Epoch 57 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3827 - acc : 0 . 9914 - val_loss : 0 . 8205 - val_acc : 0 . 8250 Epoch 58 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3818 - acc : 0 . 9912 - val_loss : 0 . 8571 - val_acc : 0 . 8051 Epoch 59 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3811 - acc : 0 . 9919 - val_loss : 0 . 8155 - val_acc : 0 . 8254 Epoch 60 / 100 50000 / 50000 [ ============================== ] - 125 s 3 ms / step - loss : 0 . 3803 - acc : 0 . 9919 - val_loss : 0 . 8617 - val_acc : 0 . 8040 Epoch 61 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3793 - acc : 0 . 9926 - val_loss : 0 . 8212 - val_acc : 0 . 8192 Epoch 62 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3825 - acc : 0 . 9912 - val_loss : 0 . 8139 - val_acc : 0 . 8277 Epoch 63 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9923 - val_loss : 0 . 8304 - val_acc : 0 . 8121 Epoch 64 / 100 50000 / 50000 [ ============================== ] - 125 s 2 ms / step - loss : 0 . 3809 - acc : 0 . 9918 - val_loss : 0 . 7961 - val_acc : 0 . 8289 Epoch 65 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3750 - acc : 0 . 9930 - val_loss : 0 . 8676 - val_acc : 0 . 8110 Epoch 66 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3789 - acc : 0 . 9928 - val_loss : 0 . 8308 - val_acc : 0 . 8148 Epoch 67 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3783 - acc : 0 . 9929 - val_loss : 0 . 8595 - val_acc : 0 . 8097 Epoch 68 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9935 - val_loss : 0 . 8359 - val_acc : 0 . 8065 Epoch 69 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3784 - acc : 0 . 9927 - val_loss : 0 . 8189 - val_acc : 0 . 8255 Epoch 70 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3786 - acc : 0 . 9924 - val_loss : 0 . 8754 - val_acc : 0 . 8001 Epoch 71 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9936 - val_loss : 0 . 8188 - val_acc : 0 . 8262 Epoch 72 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3758 - acc : 0 . 9932 - val_loss : 0 . 8540 - val_acc : 0 . 8169 Epoch 73 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3740 - acc : 0 . 9934 - val_loss : 0 . 8127 - val_acc : 0 . 8258 Epoch 74 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3749 - acc : 0 . 9932 - val_loss : 0 . 8662 - val_acc : 0 . 8018 Epoch 75 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3721 - acc : 0 . 9941 - val_loss : 0 . 8359 - val_acc : 0 . 8213 Epoch 76 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3746 - acc : 0 . 9937 - val_loss : 0 . 8462 - val_acc : 0 . 8178 Epoch 77 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3741 - acc : 0 . 9936 - val_loss : 0 . 8983 - val_acc : 0 . 7972 Epoch 78 / 100 50000 / 50000 [ ============================== ] - 122 s 2 ms / step - loss : 0 . 3751 - acc : 0 . 9933 - val_loss : 0 . 8525 - val_acc : 0 . 8173 Epoch 79 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3762 - acc : 0 . 9931 - val_loss : 0 . 8190 - val_acc : 0 . 8201 Epoch 80 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3737 - acc : 0 . 9940 - val_loss : 0 . 8441 - val_acc : 0 . 8196 Epoch 81 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3729 - acc : 0 . 9935 - val_loss : 0 . 8151 - val_acc : 0 . 8267 Epoch 82 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3735 - acc : 0 . 9938 - val_loss : 0 . 8405 - val_acc : 0 . 8163 Epoch 83 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3723 - acc : 0 . 9939 - val_loss : 0 . 8225 - val_acc : 0 . 8243 Epoch 84 / 100 50000 / 50000 [ ============================== ] - 123 s 2 ms / step - loss : 0 . 3738 - acc : 0 . 9938 - val_loss : 0 . 8413 - val_acc : 0 . 8115 Epoch 85 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3714 - acc : 0 . 9947 - val_loss : 0 . 9080 - val_acc : 0 . 7932 Epoch 86 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3744 - acc : 0 . 9942 - val_loss : 0 . 8467 - val_acc : 0 . 8135 Epoch 87 / 100 50000 / 50000 [ ============================== ] - 124 s 2 ms / step - loss : 0 . 3705 - acc : 0 . 9948 - val_loss : 0 . 8491 - val_acc : 0 . 8163 Epoch 88 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3733 - acc : 0 . 9944 - val_loss : 0 . 8005 - val_acc : 0 . 8214 Epoch 89 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3693 - acc : 0 . 9949 - val_loss : 0 . 7791 - val_acc : 0 . 8321 Epoch 90 / 100 50000 / 50000 [ ============================== ] - 135 s 3 ms / step - loss : 0 . 3724 - acc : 0 . 9942 - val_loss : 0 . 8458 - val_acc : 0 . 8124 Epoch 91 / 100 50000 / 50000 [ ============================== ] - 128 s 3 ms / step - loss : 0 . 3732 - acc : 0 . 9947 - val_loss : 0 . 8315 - val_acc : 0 . 8164 Epoch 92 / 100 50000 / 50000 [ ============================== ] - 127 s 3 ms / step - loss : 0 . 3699 - acc : 0 . 9950 - val_loss : 0 . 8140 - val_acc : 0 . 8226 Epoch 93 / 100 50000 / 50000 [ ============================== ] - 131 s 3 ms / step - loss : 0 . 3694 - acc : 0 . 9950 - val_loss : 0 . 8342 - val_acc : 0 . 8210 Epoch 94 / 100 50000 / 50000 [ ============================== ] - 134 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8938 - val_acc : 0 . 8019 Epoch 95 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3698 - acc : 0 . 9946 - val_loss : 0 . 8771 - val_acc : 0 . 8066 Epoch 96 / 100 50000 / 50000 [ ============================== ] - 164 s 3 ms / step - loss : 0 . 3712 - acc : 0 . 9946 - val_loss : 0 . 8396 - val_acc : 0 . 8211 Epoch 97 / 100 50000 / 50000 [ ============================== ] - 155 s 3 ms / step - loss : 0 . 3689 - acc : 0 . 9949 - val_loss : 0 . 8728 - val_acc : 0 . 8112 Epoch 98 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3663 - acc : 0 . 9953 - val_loss : 0 . 9615 - val_acc : 0 . 7902 Epoch 99 / 100 50000 / 50000 [ ============================== ] - 133 s 3 ms / step - loss : 0 . 3714 - acc : 0 . 9944 - val_loss : 0 . 8414 - val_acc : 0 . 8188 Epoch 100 / 100 50000 / 50000 [ ============================== ] - 138 s 3 ms / step - loss : 0 . 3682 - acc : 0 . 9956 - val_loss : 0 . 8055 - val_acc : 0 . 8266","title":"Model Training"},{"location":"examples/binarynet_cifar10/#model-output","text":"We can now plot the final validation accuracy and loss: plt . plot ( trained_model . history [ 'acc' ]) plt . plot ( trained_model . history [ 'val_acc' ]) plt . title ( 'model accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . max ( trained_model . history [ 'acc' ])) print ( np . max ( trained_model . history [ 'val_acc' ])) 0 . 9956000019311905 0 . 8320999944210052 plt . plot ( trained_model . history [ 'loss' ]) plt . plot ( trained_model . history [ 'val_loss' ]) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) print ( np . min ( trained_model . history [ 'loss' ])) print ( np . min ( trained_model . history [ 'val_loss' ])) 0 . 3663262344896793 0 . 7790719392895699","title":"Model Output"},{"location":"examples/mnist/","text":"Introduction to BNNs with Larq \u00b6 Run on Binder View on GitHub This tutorial demonstrates how to train a simple binarized Convolutional Neural Network (CNN) to classify MNIST digits. This simple network will achieve approximately 98% accuracy on the MNIST test set. This tutorial uses Larq and the Keras Sequential API , so creating and training our model will require only a few lines of code. import tensorflow as tf import larq as lq Download and prepare the MNIST dataset \u00b6 ( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . mnist . load_data () train_images = train_images . reshape (( 60000 , 28 , 28 , 1 )) test_images = test_images . reshape (( 10000 , 28 , 28 , 1 )) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1 Create the model \u00b6 The following will create a simple binarized CNN. The quantization function $$ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} $$ is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning. To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE) (the binarization is essentially replaced by a clipped identity on the backward pass): $$ \\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} $$ In Larq this can be done by using input_quantizer=\"ste_sign\" and kernel_quantizer=\"ste_sign\" . Additionally, the latent full precision weights are clipped to -1 and 1 using kernel_constraint=\"weight_clip\" . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) model = tf . keras . models . Sequential () # In the first layer we only quantize the weights and not the input model . add ( lq . layers . QuantConv2D ( 32 , ( 3 , 3 ), kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 28 , 28 , 1 ))) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Flatten ()) model . add ( lq . layers . QuantDense ( 64 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantDense ( 10 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Activation ( \"softmax\" )) Almost all parameters in the network are binarized, so either -1 or 1. This makes the network extremely fast if it would be deployed on custom BNN hardware. Here is the complete architecture of our model: lq . models . summary ( model ) + sequential_1 stats ---------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + ---------------------------------------------------------------------------------+ | quant_conv2d_3 - ( - 1 , 26 , 26 , 32 ) 288 0 0 . 04 | | max_pooling2d_2 - ( - 1 , 13 , 13 , 32 ) 0 0 0 . 00 | | batch_normalization_5 - ( - 1 , 13 , 13 , 32 ) 0 96 0 . 38 | | quant_conv2d_4 1 ( - 1 , 11 , 11 , 64 ) 18432 0 2 . 25 | | max_pooling2d_3 - ( - 1 , 5 , 5 , 64 ) 0 0 0 . 00 | | batch_normalization_6 - ( - 1 , 5 , 5 , 64 ) 0 192 0 . 75 | | quant_conv2d_5 1 ( - 1 , 3 , 3 , 64 ) 36864 0 4 . 50 | | batch_normalization_7 - ( - 1 , 3 , 3 , 64 ) 0 192 0 . 75 | | flatten_1 - ( - 1 , 576 ) 0 0 0 . 00 | | quant_dense_2 1 ( - 1 , 64 ) 36864 0 4 . 50 | | batch_normalization_8 - ( - 1 , 64 ) 0 192 0 . 75 | | quant_dense_3 1 ( - 1 , 10 ) 640 0 0 . 08 | | batch_normalization_9 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + ---------------------------------------------------------------------------------+ | Total 93088 702 14 . 11 | + ---------------------------------------------------------------------------------+ + sequential_1 summary ------------+ | Total params 93790 | | Trainable params 93322 | | Non - trainable params 468 | | Float - 32 Equivalent 0 . 36 MB | | Compression of Memory 25 . 97 | + --------------------------------+ Compile and train the model \u00b6 Note: This may take a few minutes depending on your system. model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( train_images , train_labels , batch_size = 64 , epochs = 6 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) Epoch 1 / 6 60000 / 60000 [ ============================== ] - 72 s 1 ms / sample - loss : 0 . 6494 - acc : 0 . 9070 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 2 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4760 - acc : 0 . 9606 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 3 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4480 - acc : 0 . 9691 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 4 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4365 - acc : 0 . 9718 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 5 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4329 - acc : 0 . 9739 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 6 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4287 - acc : 0 . 9758 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10000 / 10000 [ ============================== ] - 6 s 576 us / sample - loss : 0 . 4283 - acc : 0 . 9751 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Evaluate the model \u00b6 print ( f \"Test accuracy {test_acc * 100:.2f} %\" ) Test accuracy 97 . 51 % As you can see, our simple binarized CNN has achieved a test accuracy of over 97.5 %. Not bad for a few lines of code!","title":"Introduction to BNNs with Larq"},{"location":"examples/mnist/#introduction-to-bnns-with-larq","text":"Run on Binder View on GitHub This tutorial demonstrates how to train a simple binarized Convolutional Neural Network (CNN) to classify MNIST digits. This simple network will achieve approximately 98% accuracy on the MNIST test set. This tutorial uses Larq and the Keras Sequential API , so creating and training our model will require only a few lines of code. import tensorflow as tf import larq as lq","title":"Introduction to BNNs with Larq"},{"location":"examples/mnist/#download-and-prepare-the-mnist-dataset","text":"( train_images , train_labels ), ( test_images , test_labels ) = tf . keras . datasets . mnist . load_data () train_images = train_images . reshape (( 60000 , 28 , 28 , 1 )) test_images = test_images . reshape (( 10000 , 28 , 28 , 1 )) # Normalize pixel values to be between -1 and 1 train_images , test_images = train_images / 127.5 - 1 , test_images / 127.5 - 1","title":"Download and prepare the MNIST dataset"},{"location":"examples/mnist/#create-the-model","text":"The following will create a simple binarized CNN. The quantization function $$ q(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases} $$ is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning. To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE) (the binarization is essentially replaced by a clipped identity on the backward pass): $$ \\frac{\\partial q(x)}{\\partial x} = \\begin{cases} 1 & \\left|x\\right| \\leq 1 \\\\ 0 & \\left|x\\right| > 1 \\end{cases} $$ In Larq this can be done by using input_quantizer=\"ste_sign\" and kernel_quantizer=\"ste_sign\" . Additionally, the latent full precision weights are clipped to -1 and 1 using kernel_constraint=\"weight_clip\" . # All quantized layers except the first will use the same options kwargs = dict ( input_quantizer = \"ste_sign\" , kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" ) model = tf . keras . models . Sequential () # In the first layer we only quantize the weights and not the input model . add ( lq . layers . QuantConv2D ( 32 , ( 3 , 3 ), kernel_quantizer = \"ste_sign\" , kernel_constraint = \"weight_clip\" , use_bias = False , input_shape = ( 28 , 28 , 1 ))) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . MaxPooling2D (( 2 , 2 ))) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantConv2D ( 64 , ( 3 , 3 ), use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Flatten ()) model . add ( lq . layers . QuantDense ( 64 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( lq . layers . QuantDense ( 10 , use_bias = False , ** kwargs )) model . add ( tf . keras . layers . BatchNormalization ( scale = False )) model . add ( tf . keras . layers . Activation ( \"softmax\" )) Almost all parameters in the network are binarized, so either -1 or 1. This makes the network extremely fast if it would be deployed on custom BNN hardware. Here is the complete architecture of our model: lq . models . summary ( model ) + sequential_1 stats ---------------------------------------------------------------+ | Layer Input prec . Outputs # 1 - bit # 32 - bit Memory | | ( bit ) ( kB ) | + ---------------------------------------------------------------------------------+ | quant_conv2d_3 - ( - 1 , 26 , 26 , 32 ) 288 0 0 . 04 | | max_pooling2d_2 - ( - 1 , 13 , 13 , 32 ) 0 0 0 . 00 | | batch_normalization_5 - ( - 1 , 13 , 13 , 32 ) 0 96 0 . 38 | | quant_conv2d_4 1 ( - 1 , 11 , 11 , 64 ) 18432 0 2 . 25 | | max_pooling2d_3 - ( - 1 , 5 , 5 , 64 ) 0 0 0 . 00 | | batch_normalization_6 - ( - 1 , 5 , 5 , 64 ) 0 192 0 . 75 | | quant_conv2d_5 1 ( - 1 , 3 , 3 , 64 ) 36864 0 4 . 50 | | batch_normalization_7 - ( - 1 , 3 , 3 , 64 ) 0 192 0 . 75 | | flatten_1 - ( - 1 , 576 ) 0 0 0 . 00 | | quant_dense_2 1 ( - 1 , 64 ) 36864 0 4 . 50 | | batch_normalization_8 - ( - 1 , 64 ) 0 192 0 . 75 | | quant_dense_3 1 ( - 1 , 10 ) 640 0 0 . 08 | | batch_normalization_9 - ( - 1 , 10 ) 0 30 0 . 12 | | activation_1 - ( - 1 , 10 ) 0 0 0 . 00 | + ---------------------------------------------------------------------------------+ | Total 93088 702 14 . 11 | + ---------------------------------------------------------------------------------+ + sequential_1 summary ------------+ | Total params 93790 | | Trainable params 93322 | | Non - trainable params 468 | | Float - 32 Equivalent 0 . 36 MB | | Compression of Memory 25 . 97 | + --------------------------------+","title":"Create the model"},{"location":"examples/mnist/#compile-and-train-the-model","text":"Note: This may take a few minutes depending on your system. model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( train_images , train_labels , batch_size = 64 , epochs = 6 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) Epoch 1 / 6 60000 / 60000 [ ============================== ] - 72 s 1 ms / sample - loss : 0 . 6494 - acc : 0 . 9070 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 2 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4760 - acc : 0 . 9606 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 3 / 6 60000 / 60000 [ ============================== ] - 67 s 1 ms / sample - loss : 0 . 4480 - acc : 0 . 9691 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 4 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4365 - acc : 0 . 9718 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 5 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4329 - acc : 0 . 9739 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Epoch 6 / 6 60000 / 60000 [ ============================== ] - 68 s 1 ms / sample - loss : 0 . 4287 - acc : 0 . 9758 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10000 / 10000 [ ============================== ] - 6 s 576 us / sample - loss : 0 . 4283 - acc : 0 . 9751 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b","title":"Compile and train the model"},{"location":"examples/mnist/#evaluate-the-model","text":"print ( f \"Test accuracy {test_acc * 100:.2f} %\" ) Test accuracy 97 . 51 % As you can see, our simple binarized CNN has achieved a test accuracy of over 97.5 %. Not bad for a few lines of code!","title":"Evaluate the model"}]}